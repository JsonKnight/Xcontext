{
  "aiReadme": "AI Readme: This JSON object provides context about a software project. Use this information to understand the project's structure, code, documentation, and guidelines.\\n\nKey Sections Explained:\\n- 'project_name': Identifies the project.\\n- 'project_root': The base directory path.\\n- 'system_info': Details about the environment where this context was generated.\\n- 'meta': Contains user-defined key-value pairs relevant to the project.\\n- 'docs': An array of documentation files. Each object has 'path' (relative to project root) and 'content'.\\n- 'tree': Represents the project's directory structure hierarchically. Each node has 'name', 'type' ('file' or 'directory'), and optional 'children'.\\n- 'source': (Not included or empty) No source files matched the configuration filters.\\n- 'rules': Contains directives (e.g., coding standards, user instructions) to be followed. Keys are rule set names (e.g., 'instructions', 'project_rules'). Values are arrays of rule strings. **Follow these directives strictly.**\\n- 'generation_timestamp': Indicates when this context was created (UTC).",
  "projectName": "xcontext",
  "projectRoot": "/drive/work/xtools/xcontext",
  "systemInfo": {
    "osName": "Fedora Linux",
    "osVersion": "42",
    "kernelVersion": "6.14.5-300.fc42.x86_64",
    "hostname": "fedora",
    "shell": "/usr/bin/fish",
    "term": "xterm-256color"
  },
  "meta": {
    "author": "json"
  },
  "docs": [
    {
      "path": ".dir-locals.el",
      "content": ";;; .dir-locals.el --- Directory Local Variables for Project -*- lexical-binding: t; -*-\n\n;;; Commentary:\n;; This file sets directory-specific variables for Emacs.\n;; It applies settings based on the project structure.\n\n;;; Code:\n\n;;; .dir-locals.el ends here\n"
    },
    {
      "path": ".gitignore",
      "content": "# -*- mode: gitignore; -*-\n\n## ─────────────────────────────────────────────\n## Editor & OS-Specific Files\n## ─────────────────────────────────────────────\n\n# Emacs backup files\n*~\n\\#*\\#\n.\\#*\n\n# Keep project-specific Emacs files\n!**/.dir-locals.el\n!**/.envrc\n\n# Editor/IDE files\n*.sublime-project\n*.sublime-workspace\n.vscode/*\n!.vscode/extensions.json\n.idea/\n.DS_Store\n*.suo\n*.ntvs*\n*.njsproj\n*.sln\n*.sw?\n.idea\n\n# OS-generated files\nThumbs.db\n\n## ─────────────────────────────────────────────\n## Build Directories & Artifacts\n## ─────────────────────────────────────────────\n\n# Ignore build directories\n/.cache/\n/.build/\n/.dist/\n/.ccls-cache/\n\n# Ignore compiled binaries & object files\n*.o\n*.so\n*.dylib\n*.dll\n*.exe\n\n## ─────────────────────────────────────────────\n## Rust Build Artifacts\n## ─────────────────────────────────────────────\n\n/target/\n/debug/\n/Cargo.lock\n.cargo/\nclippy.toml\nrustfmt.toml\n/**/*.rs.bk\n/.rust-analyzer\n\n# Ignore profiling files\n*.profdata\n*.gcda\n\n## ─────────────────────────────────────────────\n## Ruby-Specific Files\n## ─────────────────────────────────────────────\n\n/.ruby-lsp/\n/.ruby-version\n/.bundle/\n/.yardoc\n/_yardoc/\n/coverage/\n/doc/\n/pkg/\n/spec/reports/\n/tmp/\n/vendor/bundle/\nGemfile.lock\n/.gem\n\n## ─────────────────────────────────────────────\n## Logs & Temporary Files\n## ─────────────────────────────────────────────\n\nlogs/\n*.log\n*.logs\n*.log.*\n*.logs.*\n\n# Miscellaneous temp files\n*.swp\n*.tmp\n*.old\n*.orig\n\n## ─────────────────────────────────────────────\n## Archive & Compression Files\n## ─────────────────────────────────────────────\n\n/*.zip\n/*.rar\n/*.tar\n/*.tar.gz\n/*.tar.bz2\n/*.tar.xz\n/*.7z\n/*.gz\n/*.bz2\n/*.xz\n\n## ─────────────────────────────────────────────\n## Project-Specific Ignored Directories\n## ─────────────────────────────────────────────\n\n/__*/\n/.xtools/xcontext/cache/\n\n# Ignore local executable builds\nbuild/\nout/\n\n"
    },
    {
      "path": ".org/CLI.org",
      "content": "#+TITLE: xcontext CLI Specification (v1.5.0) # Version Bump\n#+AUTHOR: json & Gemini\n#+OPTIONS: toc:t\n#+DATE: <2025-04-24 Thu> # Updated Date\n\n* Introduction / Overview\nThis document details the Command Line Interface (CLI) for the ~xcontext~ tool, refactored to use a subcommand structure for better organization and clarity. It aims for consistency and user convenience, incorporating features like configuration management (including common filters, rule/prompt imports), flexible output (including pretty XML, dynamic AI Readme), nested source code representation, structured rule/prompt output (with origin prefixes), sectioned built-in ignores, enhanced save logic, formatted utility command output, watch mode, documentation handling, quick content extraction, shell completions, verbose/quiet modes, and various utility modes.\n\n* Top-Level Usage\n  #+BEGIN_SRC shell\n    xcontext [GLOBAL_OPTIONS] [COMMAND]\n  #+END_SRC\n  - If no COMMAND is specified, help is displayed. Use ~generate~ or its alias ~g~ explicitly for the default action.\n  - Use `-h` or `--help` for help. Use `-V` or `--version` for version info.\n\n* Global Options (Available for most commands)\n  These options can typically be placed before or after the command name.\n\n** Project Setup Options\n*** ~--project-root <PATH>~\n    - *Purpose:* Specify the target project directory.\n    - *Arguments:* ~<PATH>~ (string path).\n    - *Behavior:* Sets the root directory for all operations. Determined via: Flag -> $PROJECT_ROOT -> CWD. Tilde expansion is performed. Path is canonicalized.\n*** ~--context-file <CONTEXT_FILE>~\n    - *Purpose:* Specify path to the TOML config file.\n    - *Arguments:* ~<CONTEXT_FILE>~ (string path or filename).\n    - *Behavior:* Overrides default search (~.xtools/xcontext/xcontext.toml~ relative to project root). Tilde expansion performed. See SPEC.org for full path resolution logic.\n    - *Conflicts:* ~--disable-context-file~.\n*** ~--disable-context-file~\n    - *Purpose:* Disable loading any TOML config file.\n    - *Behavior:* Uses built-in defaults and CLI arguments only.\n    - *Conflicts:* ~--context-file~.\n*** ~--project-name <NAME>~\n    - *Purpose:* Specify the project name.\n    - *Arguments:* ~<NAME>~ (string).\n    - *Behavior:* Overrides the project name determined from the config file or the project root directory's basename. Included in output if not excluded.\n\n** Output Formatting Options\n*** ~-f, --format <FORMAT>~\n    - *Alias:* `-f`\n    - *Purpose:* Set the output format for context or utility data requiring structured output.\n    - *Arguments:* ~<FORMAT>~ (string: `json`, `yaml`, `xml`).\n    - *Behavior:* Sets the output format. Determines serialization and default save extension. Applies to `generate`, `watch` (saved output), `quick`, `metrics` (if specified), `debug` (if specified), and the *plural* variants of `show` (if specified). Does *not* affect the default human-readable output of `metrics`, `debug`, or `show` plural commands.\n    - *Default:* `json` (when format is needed, e.g., for saving or for `quick` default).\n*** ~--enable-json-minify~\n    - *Purpose:* Ensure JSON output is compact (minified) [default: true unless -f specifies yaml/xml or --disable-json-minify].\n    - *Behavior:* Ensures JSON is compact (no extra whitespace). Ignored if format is not JSON.\n    - *Conflicts:* ~--disable-json-minify~.\n*** ~--disable-json-minify~\n    - *Purpose:* Ensure JSON output is pretty-printed (readable).\n    - *Behavior:* Ensures JSON is pretty-printed with indentation. Overrides default minification for JSON. Ignored if format is not JSON.\n    - *Conflicts:* ~--enable-json-minify~.\n*** ~--enable-xml-pretty~ (NEW)\n    - *Purpose:* Ensure XML output is pretty-printed (readable).\n    - *Behavior:* Ensures XML is pretty-printed with indentation. Ignored if format is not XML.\n    - *Conflicts:* ~--disable-xml-pretty~.\n*** ~--disable-xml-pretty~ (NEW)\n    - *Purpose:* Ensure XML output is compact [default].\n    - *Behavior:* Ensures XML has no extra whitespace/indentation. Ignored if format is not XML.\n    - *Conflicts:* ~--enable-xml-pretty~.\n\n\n** Verbosity Options\n*** ~-v, --verbose~\n    - *Purpose:* Increase message verbosity. Shows informational messages (e.g., file watching status, glob interpretations, import search paths) in addition to warnings and errors. Repeat for more detail (currently unused).\n    - *Behavior:* Increments verbosity level (counter).\n    - *Conflicts:* ~--quiet~.\n*** ~-q, --quiet~\n    - *Purpose:* Silence informational messages and warnings. Errors will still be shown on stderr.\n    - *Behavior:* Suppresses non-error output to stderr.\n    - *Conflicts:* ~--verbose~ (Quiet takes precedence).\n    - *Default:* Not quiet (warnings are shown).\n\n* Commands\n\n** ~generate~\n   - *Alias:* ~g~, ~gen~\n   - *Purpose:* Generate the full project context based on configuration and CLI overrides.\n   - *Usage:* ~xcontext [GLOBAL_OPTIONS] generate [GENERATE_OPTIONS]~\n   - *Options (GENERATE_OPTIONS):*\n   *** Output Control\n       - ~--stdout~: Force output of the main context to standard output. Conflicts with ~--save~ (if chunks aren't used and save dir is specified). Default if not saving/chunking.\n       - ~-s, --save [SAVE_DIR]~: (Updated) Save generated context (and chunks if applicable). If `SAVE_DIR` is provided, saves there. If flag is used without `SAVE_DIR`, saves to path defined in `[save].output_dir` config, falling back to the current directory if config is unset.\n       - ~-c, --chunks <SIZE_STRING>~: Split source content into chunks (e.g., \"5MB\", \"512kb\"). Requires JSON format. Implicitly enables saving (uses default save logic if path not given with -s).\n   *** Core Exclusions\n       - ~--exclude-project-name~: Omit 'project_name' field from output.\n       - ~--exclude-project-root~: Omit 'project_root' field from output.\n       - ~--exclude-timestamp~: Omit 'generation_timestamp' field from output.\n       - ~--exclude-system-info~: Omit 'system_info' field from output.\n   *** Section Toggles\n       - ~--enable-tree~ / ~--disable-tree~: Control inclusion of the 'tree' section [default: enabled].\n       - ~--enable-source~ / ~--disable-source~: Control inclusion of the 'source' section [default: enabled].\n       - ~--enable-meta~ / ~--disable-meta~: Control inclusion of the 'meta' section [default: enabled].\n       - ~--enable-rules~ / ~--disable-rules~: Control inclusion of the 'rules' section [default: enabled].\n       - ~--enable-docs~ / ~--disable-docs~: Control inclusion of the 'docs' section [default: enabled].\n   *** Ignore Rules\n       - ~--enable-gitignore~ / ~--disable-gitignore~: Globally control whether ~.gitignore~ files are respected [default: enabled].\n       - ~--enable-builtin-ignore~ / ~--disable-builtin-ignore~: Control whether default built-in ignores are applied [default: enabled].\n   *** Content Filtering\n       - ~--tree-include <PATTERN>~: Add include path/glob pattern for tree view. (Repeatable)\n       - ~--tree-exclude <PATTERN>~: Add exclude path/glob pattern for tree view (trailing `/` implies `/**`). (Repeatable)\n       - ~--source-include <PATTERN>~: Add include path/glob pattern for source files. (Repeatable)\n       - ~--source-exclude <PATTERN>~: Add exclude path/glob pattern for source files (trailing `/` implies `/**`). (Repeatable)\n       - ~--docs-include <PATTERN>~: Add include path/glob pattern for documentation files. (Repeatable)\n       - ~--docs-exclude <PATTERN>~: Add exclude path/glob pattern for documentation files (trailing `/` implies `/**`). (Repeatable)\n   *** Metadata Override\n       - ~--add-meta <key=value>~: Add/override key=value pairs in the 'meta' section. (Repeatable)\n\n** ~watch~\n   - *Alias:* ~w~\n   - *Purpose:* Monitor project files (source, docs, config, imports) and regenerate context automatically on changes.\n   - *Usage:* ~xcontext [GLOBAL_OPTIONS] watch [WATCH_OPTIONS]~\n   - *Options (WATCH_OPTIONS):*\n     - ~--watch-delay <DELAY_STRING>~: Set debounce delay for watch mode (e.g., \"500ms\", \"1s\"). [Default: 300ms]\n     - ~-s, --save [SAVE_DIR]~: (Updated) Save generated context to a specified directory on change. Uses same optional path logic as `generate --save`. If unset, prints context to stdout on change.\n\n** ~show~\n   - *Alias:* ~s~\n   - *Purpose:* Show specific configured items (metadata, prompts, rules).\n   - *Usage:* ~xcontext [GLOBAL_OPTIONS] show <ITEM_TYPE> [ITEM_OPTIONS]~\n   - *Behavior:* Default output is human-readable text. Use global `-f` flag for structured JSON/YAML/XML output when using *plural* subcommands (~metas~, ~prompts~, ~rules~) or when viewing a single item that is structured. Keys may be prefixed (`static:`, `imported:`, `custom:`).\n   - *Subcommands (ITEM_TYPE):*\n   *** ~meta~\n       - *Purpose:* Show specific metadata key or list available keys.\n       - *Usage:* ~xcontext show meta [<KEY>]~\n       - *Behavior:* If ~<KEY>~ given, shows value. If omitted, lists keys.\n   *** ~metas~\n       - *Purpose:* Show content of all metadata keys.\n       - *Usage:* ~xcontext show metas~\n       - *Behavior:* Default: pretty text. Respects `-f`.\n   *** ~prompt~\n       - *Purpose:* Show specific prompt or list available prompt names (including imported).\n       - *Usage:* ~xcontext show prompt [<NAME>]~ (Use prefix like `imported:my_prompt` if needed)\n       - *Behavior:* If ~<NAME>~ given, shows text. If omitted, lists names.\n   *** ~prompts~\n       - *Purpose:* Show content of all prompts (built-in, custom, imported).\n       - *Usage:* ~xcontext show prompts~\n       - *Behavior:* Default: pretty text. Respects `-f`.\n   *** ~rule~\n       - *Purpose:* Show specific rule set/list or list available names (static, imported, custom).\n       - *Usage:* ~xcontext show rule [<NAME>]~ (Use prefix like `static:general` or `imported:my_rules`)\n       - *Behavior:* If ~<NAME>~ given, shows content. If omitted, lists names.\n   *** ~rules~\n       - *Purpose:* Show content of all rule sets/lists (static, imported, custom).\n       - *Usage:* ~xcontext show rules~\n       - *Behavior:* Default: pretty text. Respects `-f`.\n\n** ~metrics~\n   - *Alias:* ~m~\n   - *Purpose:* Calculate and display project statistics (files, lines, bytes, tokens).\n   - *Usage:* ~xcontext [GLOBAL_OPTIONS] metrics~\n   - *Behavior:* Default: pretty table. Respects `-f`.\n\n** ~debug~\n   - *Alias:* ~d~\n   - *Purpose:* Show effective configuration (including common filters) and planned file inclusions for debugging.\n   - *Usage:* ~xcontext [GLOBAL_OPTIONS] debug~\n   - *Behavior:* Default: pretty text. Respects `-f`.\n\n** ~quick~\n   - *Alias:* ~q~\n   - *Purpose:* Quickly extract content of specific files matching a pattern.\n   - *Usage:* ~xcontext [GLOBAL_OPTIONS] quick <PATTERN>~\n   - *Arguments:*\n     - ~<PATTERN>~ (Required): Glob pattern. Directory patterns match files inside recursively.\n   - *Behavior:* Respects `.gitignore` and built-in ignores by default. Output respects global `-f`. Default: minified JSON map { path: content }. Info messages to stderr if verbose (`-v`).\n\n** ~completion~\n   - *Purpose:* Generate shell completion scripts.\n   - *Usage:* ~xcontext completion [OPTIONS]~\n   - *Options:*\n     - ~--shell <SHELL>~: Shell to generate for (e.g., `fish`, `bash`, `zsh`). [Default: `fish`]\n     - ~--save~: Save completion script to default location. If omitted, prints to stdout.\n\n** ~config~\n   - *Purpose:* Show or save the default configuration file structure.\n   - *Usage:* ~xcontext [GLOBAL_OPTIONS] config [--save]~\n   - *Behavior:* Without `--save`, shows default TOML structure to stdout. With `--save`, saves to default path (~.xtools/xcontext/xcontext.toml~), prompting for overwrite unless quiet.\n   - *Options:*\n     - ~--save~: Save default config structure.\n\n** ~cl~\n   - *Alias:* ~c~\n   - *Purpose:* Clear the terminal screen.\n   - *Usage:* ~xcontext cl~\n\n** ~mcp~\n   - *Purpose:* Dummy MCP command (placeholder).\n   - *Usage:* ~xcontext mcp~\n\n* Error Handling / Exit Codes (No changes to codes themselves)\n  - `0`: Success\n  - `1`: Config/TOML Parse/Serialize Error\n  - `2`: Filesystem/IO/WalkDir/Ignore/Glob Error (Includes import file read errors)\n  - `3`: Chunking Error\n  - `4`: System Info Error\n  - `5`: Invalid Argument Error\n  - `6`: Output Serialization Error (JSON/YAML/XML)\n  - `7`: Utility Mode Error (e.g., item not found in show)\n  - `8`: TikToken Error\n  - `9`: Watch Mode Error\n"
    },
    {
      "path": ".org/SPEC.org",
      "content": "#+TITLE: xcontext Specification (v1.5.0) # Version Bump\n#+AUTHOR: json & Gemini\n#+DATE: <2025-04-24 Thu> # Updated Date\n#+VERSION: ~1.5.0~ # Version Bump\n\n* SPECIFICATION\n** Purpose\n   Generate structured project context (metadata, system info, project root, directory tree, documentation files, source files, AI rules) suitable for AI models. The tool utilizes a subcommand structure, aims for clarity, consistency, user convenience, parallel performance, flexible configuration (TOML/CLI), multiple output formats (including pretty XML), optional file saving/chunking, watch mode, quick content extraction, enhanced documentation/metrics handling, shell completion generation, verbosity control, and various utility modes. Supports common filtering patterns, importing rules/prompts from external files, and selecting built-in static rulesets. Includes an enhanced dynamic AI Readme guide within the output.\n\n** Overview\n   ~xcontext~ is a Rust-based command-line interface (CLI) tool designed for creating comprehensive, machine-readable project context. It operates via subcommands (e.g., ~generate~, ~watch~, ~show~ subcommands like ~rule~ vs ~rules~, ~metrics~, ~debug~, ~quick~, ~completion~, ~config~, ~mcp~) with short aliases. It determines a project root (via global ~--project-root~ flag, Env Var, or CWD), loads configuration from a TOML file (found via global ~--context-file~ flag or default path, unless disabled via global ~--disable-context-file~ flag), and overrides settings via command-specific CLI flags. Global ~-v/--verbose~ and ~-q/--quiet~ flags control output verbosity. Global flags (--enable/disable-json-minify, --enable/disable-xml-pretty) control output formatting. TOML configuration keys within sections should use snake_case (e.g., ~use_gitignore~).\n\n   It gathers system information (*internally using `sysinfo` crate*), project metadata, directory structure, documentation files, and source code files. Filtering respects `.gitignore` rules (configurable), *section-aware built-in ignores* (configurable), section-specific include/exclude patterns, and optional *common filters* defined in `[common_filters]`. Directory filtering patterns ending in `/` in TOML/CLI/builtin lists imply recursive matching.\n\n   Rules and prompts can be defined directly in TOML, selected from built-in static sets (via `include_static`), or imported from external user files (via `import`). Output keys for rules/prompts are prefixed (`static:`, `imported:`, `custom:`). Imported files are searched first relative to project root, then relative to the `.xtools/xcontext/` directory.\n\n   It outputs context in JSON (default, minified), YAML, or XML format (via global ~-f/--format~ flag), primarily to ~stdout~. Utility commands (~metrics~, ~debug~, plural ~show~ commands) default to human-readable text output but support ~-f~ for structured data. The ~quick~ command defaults to minified JSON. Singular ~show~ commands list available items by default. The output includes an enhanced ~ai_readme~ field dynamically describing the included content and its purpose. Source code is presented under a ~source~ key, containing either inline ~files~ or references to ~chunks~.\n\n   Optional features include saving output to disk (including a fallback mechanism for `-s` without a path), chunking large source content (JSON only), watching source/docs/config/imported files for changes and regenerating, extracting only specified file content quickly, generating shell completions (Fish default, others optional), utility modes for default config management (~config~ command with ~--save~ flag), information display (~show rule~ vs ~show rules~, ~metrics~, ~debug~), and listing available items (~show rule/prompt/meta~ without args). Includes a simple screen clear utility (~cl~ command) and a dummy ~mcp~ command.\n\n** Goals\n   - *Rich Context Generation*: Produce comprehensive project snapshots via ~generate~ command.\n   - *Enhanced AI Readme*: Automatically include a dynamic guide within the output.\n   - *User Convenience*: Prioritize ease of use with sensible defaults, clear subcommand structure, flexible configuration (including common filters, rule/prompt imports), clear CLI options (with aliases), helpful utility modes, enhanced save logic, verbosity control.\n   - *Flexible Output*: Default ~generate~ output to ~stdout~. Offer explicit saving (with default location logic). Support multiple formats (JSON/YAML/XML via global ~-f~), optional JSON minification, optional XML pretty-printing. Utility commands default to human-readable output (except ~quick~ defaults to minified JSON).\n   - *Structured Source/Rules/Prompts*: Represent source code consistently, group rules/prompts by origin using prefixes.\n   - *Scalable Performance*: Use parallel file processing.\n   - *Size-Based Chunking (JSON Only)*: Reliably split large source code collections.\n   - *Documentation Handling*: Separate identification and inclusion of documentation files.\n   - *Section-Aware Built-in Ignores*: Apply default ignores relevant to context.\n   - *Consistent Directory Filtering*: Treat trailing `/` consistently. Apply common filters as fallback.\n   - *Robustness & Debugging*: Clear error reporting, exit codes, debug mode (~debug~), default config inspection/saving (~config~ command with ~--save~), enhanced metrics (~metrics~), handling for missing import files.\n\n** Use Cases (Examples)\n   - Generate context and view: ~xcontext g | less~\n   - Save context using default location: ~xcontext g -s~\n   - Save context as pretty XML: ~xcontext g -f xml --enable-xml-pretty -s .~\n   - Chunk large JSON context: ~xcontext g -c 10MB -s .~\n   - Show default configuration structure: ~xcontext config~\n   - Save default configuration structure: ~xcontext config --save~\n   - Show content of all rules (pretty text): ~xcontext show rules~\n   - Show specific static rule set (text): ~xcontext show rule static:languages_rust~\n   - Show specific imported rule set (text): ~xcontext show rule imported:my_company_rules~\n   - Show effective configuration and file lists (pretty text): ~xcontext debug~ (or ~d~)\n   - Show debug info as JSON: ~xcontext d -f json~\n   - Get project statistics (pretty table): ~xcontext metrics~ (or ~m~)\n   - Watch for changes and save: ~xcontext watch -s .~ (or ~w~)\n   - Quickly get Markdown file content as YAML: ~xcontext q '**/*.md' -f yaml~\n   - Generate Fish shell completions script to stdout: ~xcontext completion~\n   - Save Bash shell completions script: ~xcontext completion --shell bash --save~\n\n** Configuration Sources & Precedence\n   - Defaults -> TOML File -> CLI Flags\n\n** Configuration (TOML)\n   - Looks for `xcontext.toml`. Loading skipped via global ~--disable-context-file~.\n   - Keys use snake_case.\n   *** Sample Configuration (`xcontext.toml`)\n     #+BEGIN_SRC toml\n       # --- xcontext Configuration ---\n       # This file configures the xcontext tool.\n       # Only include settings here if you want to *change* them from the defaults.\n       # Run 'xcontext config' to see the full default structure.\n       # Run 'xcontext show rule' to list available built-in static rule keys.\n\n       # [general]\n       # project_name = \"MyOverrideName\" # Optional: Defaults to project directory name. Uncomment to override.\n       # use_gitignore = true            # Optional: Default is true.\n       # enable_builtin_ignore = true  # Optional: Default is true.\n\n       # [common_filters]\n       # Optional: Define include/exclude patterns applied as a fallback for docs, tree, source.\n       # Section-specific include/exclude lists override these common ones.\n       # exclude = [\"target/\", \"node_modules/\", \".git/\", \".vscode/\"]\n       # include = [\"src/\", \"docs/\"]\n\n       # [meta]\n       # enabled = true # Optional: Default is true.\n       # version = \"1.0.0\"\n       # team = \"Alpha\"\n\n       [docs]\n       # enabled = true              # Optional: Default is true.\n       # use_gitignore = \"inherit\" # Optional: Default is \"inherit\". Options: true, false, \"inherit\".\n       # If include/exclude are defined here, they OVERRIDE [common_filters] for docs.\n       include = [\"*.org\", \"*.md\", \"README*\", \"LICENSE*\", \"CHANGELOG.md\"] # Overrides common_filters.include\n       # exclude = [] # Overrides common_filters.exclude\n\n       [tree]\n       # enabled = true              # Optional: Default is true.\n       # use_gitignore = \"inherit\" # Optional: Default is \"inherit\".\n       # Exclude here overrides common_filters.exclude for tree.\n       # If exclude omitted, common_filters.exclude applies (if defined).\n       exclude = [\".git/\", \".cache/\"]\n\n       [source]\n       # enabled = true              # Optional: Default is true.\n       # use_gitignore = \"inherit\" # Optional: Default is \"inherit\".\n       # Uses common_filters.exclude because exclude is not defined here.\n       include = [\"src/**/*.rs\", \"Cargo.toml\"] # Overrides common_filters.include\n\n       [rules]\n       # enabled = true # Optional: Default is true.\n\n       # Select built-in static rulesets. Default likely includes [\"general\", \"ai_guidelines\"].\n       include_static = [\"general\", \"languages_rust\"]\n\n       # Import rules from YOUR external files. Searched relative to project root,\n       # then relative to .xtools/xcontext/ if not found in first location.\n       import = [\n         \"docs/project_standards.org\",\n         \"common_team_rules.txt\" # Will search ./common_team_rules.txt then ./.xtools/xcontext/common_team_rules.txt\n       ]\n\n       # Define custom, named rule lists directly here.\n       database_rules = [\n         \"Use migrations for all schema changes.\",\n       ]\n\n       [prompts]\n       # Import prompt text from external files. Uses same search logic as rules import.\n       # Assumes one file = one prompt text body. Output key: 'imported:<file_stem>'.\n       import = [\"prompts/refactor_prompt.txt\"]\n\n       # Define custom named prompts directly. Output key: 'custom:<key>'.\n       explain_code = \"\"\"\n       Explain the code in the provided context...\n       \"\"\"\n\n       [output]\n       # format = \"json\"           # Optional: Default is \"json\". Options: json, yaml, xml.\n       # json_minify = true        # Optional: Default is true (minified).\n       xml_pretty_print = true   # Optional: Default is false. Pretty-print XML output.\n       # include_project_name = true\n       # include_project_root = true\n       # include_timestamp = true\n       # include_system_info = true\n\n       [save]\n       # Optional: Configure default save behavior used when '-s' flag is used without path.\n       # output_dir = \".xcontext_output\" # Optional: Default is \".xtools/xcontext/cache\". Relative to project root.\n       # filename_base = \"my_context\"  # Optional: Default is project name. Base name for saved files.\n       # extension = \"json\"            # Optional: Default determined by format. Override saved file extension.\n\n       [watch]\n       # delay = \"300ms\" # Optional: Default is \"300ms\". Debounce delay.\n\n     #+END_SRC\n   *** Gitignore Handling Logic (Remains the Same)\n       1. *Global Default*: Set in `[general].use_gitignore` (defaults `true`). Overridden by ~generate --enable/disable-gitignore~.\n       2. *Section Override*: Overridden by `use_gitignore` in `[tree]`, `[source]`, or `[docs]` (`true`, `false`, `\\\"inherit\\\"`).\n       3. *Explicit Filter Precedence*: Section `include`/`exclude` always wins over gitignore.\n   *** Built-in Ignore Handling Logic (Remains the Same)\n       1. *Global Toggle*: Controlled by `[general].enable_builtin_ignore` (defaults `true`). Overridden by ~generate --enable/disable-builtin_ignore~.\n       2. *Behavior*: Uses patterns from `data/builtin_ignores.yaml`. `common:` applies everywhere. `tree:`, `source:`, `docs:` apply sectionally. A trailing `/` implies recursive matching.\n       3. *Precedence Order*: Explicit Exclude -> Explicit Include -> `.gitignore` (if enabled) -> Built-in Ignore (if enabled) -> Default.\n   *** Common Filter Logic (NEW)\n       1. *Definition*: Defined in optional `[common_filters]` section with `include` and `exclude` lists.\n       2. *Application*: Applied to `docs`, `tree`, and `source` sections *only if* the respective section does *not* define its own `include` or `exclude` list.\n       3. *Precedence*: Common filters are checked *after* section-specific explicit includes/excludes, but *before* `.gitignore` and built-in ignores. If a section defines its own `exclude = [...]`, the `[common_filters].exclude` is *ignored* for that section.\n   *** Rule Loading Logic (NEW)\n       1. *Static*: Selected via `[rules].include_static = [...]`. Refers to keys identifying rule sets embedded in the binary (e.g., \"general\", \"languages_rust\"). Defaults apply if key omitted. Output prefix: `static:`.\n       2. *Imported*: Loaded via `[rules].import = [...]`. Refers to external user files. Searched relative to project root, then `.xtools/xcontext/`. Missing files generate warnings. Output prefix: `imported:`.\n       3. *Custom*: Defined directly in TOML via `<key> = [...]` under `[rules]`. Output prefix: `custom:`.\n   *** Prompt Loading Logic (NEW)\n       1. *Imported*: Loaded via `[prompts].import = [...]`. Refers to external user files containing prompt text. Uses same 2-step search logic as rule imports. Assumes one file = one prompt. Output prefix: `imported:`.\n       2. *Custom*: Defined directly in TOML via `<key> = \"...\"` under `[prompts]`. Output prefix: `custom:`.\n   *** TOML Structure & Defaults\n       - (See Sample Config above for structure. Defaults detailed in code or `xcontext config` output).\n\n** CLI Options Summary\n   - Refer to updated `CLI.org`. Key changes: `-s/--save` value is optional, added global XML pretty-print flags.\n\n** Output Format & Location\n   - Main context output via ~generate~ or ~watch~ defaults to `stdout`.\n   - Utility commands (~metrics~, ~debug~, plural ~show~ variants) default to human-readable text on `stdout`.\n   - ~quick~ command defaults to minified JSON on `stdout`.\n   - Singular ~show~ variants default to plain text on `stdout` or key listing on stderr.\n   - Explicit format requested via global ~-f/--format~ (json, yaml, xml). Affects commands outputting structured data.\n   - Optional JSON minification (~--enable/disable-json-minify~). Optional XML pretty-printing (~--enable/disable-xml-pretty~).\n   - Saving enabled via ~generate/watch -s [path]~ (path optional) or ~generate -c~. ~config~ uses ~--save~. ~completion~ uses ~--save~. If `-s` used without path, uses config `[save].output_dir` or CWD.\n\n** Implementation Notes\n   - *CLI Parsing*: Update `cli.rs` for optional `save` argument and XML pretty flags.\n   - *Directory Filtering*: Update `context/gather.rs` to implement common filter logic and updated precedence.\n   - *Rule/Prompt Loading*: Update `config.rs` and `context.rs` to handle `include_static`, `import` (with 2-step search), and custom key definitions for rules/prompts. Handle potential file read errors for imports. Use prefixes for output map keys. Embed static rules (likely from YAML source) in binary.\n   - *Output*: Update `output.rs` to support XML pretty-printing via `quick-xml` writer options based on config/CLI flag.\n   - *Save Logic*: Update `config.rs::get_save_details` or core logic to implement the fallback path resolution for `-s` without argument (Config -> CWD).\n   - *Watch Mode*: Update `watch.rs` to potentially watch imported rule/prompt files if feasible. Reloading config should trigger re-evaluation of imports.\n\n** Filter Logic Diagram (Mermaid) - Needs Update\n   #+BEGIN_SRC mermaid\n     graph TD\n       A[Consider Path] --> B{Matches Section Exclude?};\n       B -- Yes --> Z_EX[\\\"OUTCOME: Excluded\\\"];\n       B -- No --> C{Matches Section Include?};\n       C -- Yes --> D{Built-in Ignore Enabled & Matches?}; // Built-in check moved earlier for clarity\n       C -- No --> E{Common Filter Exclude Defined & Matches?};\n\n       D -- Yes --> Z_EX;\n       D -- No --> Y_IN[\\\"OUTCOME: Included (Explicit Section Include Wins)\\\"];\n\n       E -- Yes --> Z_EX;\n       E -- No --> F{Common Filter Include Defined?};\n       F -- Yes --> G{Matches Common Filter Include?};\n       F -- No --> H{Gitignore Enabled & Matches?}; // No common include defined, proceed\n\n       G -- Yes --> H; // Matched common include, check gitignore\n       G -- No --> Z_EX[\\\"OUTCOME: Excluded (Common Includes Defined but Not Matched)\\\"];\n\n       H -- Yes --> Z_EX;\n       H -- No --> I{Built-in Ignore Enabled & Matches?}; // Check built-in again only if passed other checks\n\n       I -- Yes --> Z_EX;\n       I -- No --> J{Explicit Section Includes Defined?}; // Final check: Did we need explicit match?\n\n       J -- Yes --> Z_EX[\\\"OUTCOME: Excluded (Not Matched Section Include)\\\"]; // Should have matched at C\n       J -- No --> Y_IN[\\\"OUTCOME: Included (Default / Common Include)\\\"]; // No section includes defined OR matched common include\n\n       style Z_EX fill:#f99,stroke:#333,stroke-width:2px;\n       style Y_IN fill:#9cf,stroke:#333,stroke-width:2px;\n   #+END_SRC\n   *Note:* This diagram attempts to reflect the new logic (Section Excl -> Section Incl -> Common Excl -> Common Incl -> Gitignore -> Built-in). Needs careful review during implementation.\n"
    },
    {
      "path": "Cargo.toml",
      "content": "[workspace]\nresolver = \"2\"\nmembers = [\n    \"core\",\n    \"cli\",\n]\n\n[workspace.dependencies]\nanyhow = \"1.0.97\"\nlog = \"0.4.27\"\nserde = { version = \"1.0.219\", features = [\"derive\"] }\nserde_json = \"1.0.140\"\nserde_yml = \"0.0.12\"\ntoml = \"0.8.20\"\nthiserror = \"2.0.12\"\nchrono = { version = \"0.4.40\", features = [\"serde\"] }\nindexmap = { version = \"2.9.0\", features = [\"serde\"] }\nclap = { version = \"4.5.35\", features = [\"derive\"] }\ncolored = \"2.0\" # Updated to 2.0, check compatibility if issues arise\nenv_logger = \"0.11.8\"\nignore = \"0.4.23\"\nwalkdir = \"2.5.0\"\nglobset = \"0.4.16\"\nrust-embed = \"8.7.0\"\nquick-xml = { version = \"0.37.4\", features = [\"serde\", \"serialize\"] }\nsysinfo = \"0.34.2\"\nbyte-unit = { version = \"5.1.6\", features = [\"serde\"] }\ntiktoken-rs = \"0.6.0\"\npathdiff = \"0.2.3\"\nshellexpand = \"3.1.1\"\nonce_cell = \"1.21.3\"\ncomfy-table = \"7.1.4\"\ndirs = \"6.0.0\"\nnotify = \"8.0.0\"\nnotify-debouncer-mini = \"0.6.0\"\nclap_complete = \"4.5.47\"\nclearscreen = \"4.0.1\"\nparse_duration = \"2.1.1\"\nrayon = \"1.10.0\" # Added from original src/context/gather.rs\n"
    },
    {
      "path": "Gemfile",
      "content": "# frozen_string_literal: true\n\nsource \"https://rubygems.org\"\n\n# 🛠 Built-in Ruby libraries\n# require 'open3'       # Run shell commands with stdout/stderr capture\n# require 'shellwords'  # Safely escape command arguments\n# require 'yaml'        # Parse YAML files\n# require 'json'        # Parse JSON responses\n# require 'benchmark'   # Measure CLI execution time\n# require 'timecop'     # Freeze/manipulate time in tests (if needed)\n\n# 🔹 Core testing tools\ngem 'aruba'             # CLI testing framework\ngem 'rspec'             # Main testing framework\ngem 'rspec-expectations' # Provides matchers like expect(...).to eq(...)\ngem 'tty-command'       # Run shell commands & capture output\ngem 'toml-rb'          # Parse TOML files (for Cargo/Rust configs)\n\n# 🔹 Additional utilities for CLI testing\ngem 'fakefs'           # Simulates a filesystem for isolated tests\ngem 'webmock'          # Mocks HTTP requests (if CLI interacts with APIs)\ngem 'httparty'         # Lightweight HTTP client for testing API calls\n"
    },
    {
      "path": "LICENSE",
      "content": "                    GNU GENERAL PUBLIC LICENSE\n                       Version 3, 29 June 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The GNU General Public License is a free, copyleft license for\nsoftware and other kinds of works.\n\n  The licenses for most software and other practical works are designed\nto take away your freedom to share and change the works.  By contrast,\nthe GNU General Public License is intended to guarantee your freedom to\nshare and change all versions of a program--to make sure it remains free\nsoftware for all its users.  We, the Free Software Foundation, use the\nGNU General Public License for most of our software; it applies also to\nany other work released this way by its authors.  You can apply it to\nyour programs, too.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthem if you wish), that you receive source code or can get it if you\nwant it, that you can change the software or use pieces of it in new\nfree programs, and that you know you can do these things.\n\n  To protect your rights, we need to prevent others from denying you\nthese rights or asking you to surrender the rights.  Therefore, you have\ncertain responsibilities if you distribute copies of the software, or if\nyou modify it: responsibilities to respect the freedom of others.\n\n  For example, if you distribute copies of such a program, whether\ngratis or for a fee, you must pass on to the recipients the same\nfreedoms that you received.  You must make sure that they, too, receive\nor can get the source code.  And you must show them these terms so they\nknow their rights.\n\n  Developers that use the GNU GPL protect your rights with two steps:\n(1) assert copyright on the software, and (2) offer you this License\ngiving you legal permission to copy, distribute and/or modify it.\n\n  For the developers' and authors' protection, the GPL clearly explains\nthat there is no warranty for this free software.  For both users' and\nauthors' sake, the GPL requires that modified versions be marked as\nchanged, so that their problems will not be attributed erroneously to\nauthors of previous versions.\n\n  Some devices are designed to deny users access to install or run\nmodified versions of the software inside them, although the manufacturer\ncan do so.  This is fundamentally incompatible with the aim of\nprotecting users' freedom to change the software.  The systematic\npattern of such abuse occurs in the area of products for individuals to\nuse, which is precisely where it is most unacceptable.  Therefore, we\nhave designed this version of the GPL to prohibit the practice for those\nproducts.  If such problems arise substantially in other domains, we\nstand ready to extend this provision to those domains in future versions\nof the GPL, as needed to protect the freedom of users.\n\n  Finally, every program is threatened constantly by software patents.\nStates should not allow patents to restrict development and use of\nsoftware on general-purpose computers, but in those that do, we wish to\navoid the special danger that patents applied to a free program could\nmake it effectively proprietary.  To prevent this, the GPL assures that\npatents cannot be used to render the program non-free.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                       TERMS AND CONDITIONS\n\n  0. Definitions.\n\n  \"This License\" refers to version 3 of the GNU General Public License.\n\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\nworks, such as semiconductor masks.\n\n  \"The Program\" refers to any copyrightable work licensed under this\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\n\"recipients\" may be individuals or organizations.\n\n  To \"modify\" a work means to copy from or adapt all or part of the work\nin a fashion requiring copyright permission, other than the making of an\nexact copy.  The resulting work is called a \"modified version\" of the\nearlier work or a work \"based on\" the earlier work.\n\n  A \"covered work\" means either the unmodified Program or a work based\non the Program.\n\n  To \"propagate\" a work means to do anything with it that, without\npermission, would make you directly or secondarily liable for\ninfringement under applicable copyright law, except executing it on a\ncomputer or modifying a private copy.  Propagation includes copying,\ndistribution (with or without modification), making available to the\npublic, and in some countries other activities as well.\n\n  To \"convey\" a work means any kind of propagation that enables other\nparties to make or receive copies.  Mere interaction with a user through\na computer network, with no transfer of a copy, is not conveying.\n\n  An interactive user interface displays \"Appropriate Legal Notices\"\nto the extent that it includes a convenient and prominently visible\nfeature that (1) displays an appropriate copyright notice, and (2)\ntells the user that there is no warranty for the work (except to the\nextent that warranties are provided), that licensees may convey the\nwork under this License, and how to view a copy of this License.  If\nthe interface presents a list of user commands or options, such as a\nmenu, a prominent item in the list meets this criterion.\n\n  1. Source Code.\n\n  The \"source code\" for a work means the preferred form of the work\nfor making modifications to it.  \"Object code\" means any non-source\nform of a work.\n\n  A \"Standard Interface\" means an interface that either is an official\nstandard defined by a recognized standards body, or, in the case of\ninterfaces specified for a particular programming language, one that\nis widely used among developers working in that language.\n\n  The \"System Libraries\" of an executable work include anything, other\nthan the work as a whole, that (a) is included in the normal form of\npackaging a Major Component, but which is not part of that Major\nComponent, and (b) serves only to enable use of the work with that\nMajor Component, or to implement a Standard Interface for which an\nimplementation is available to the public in source code form.  A\n\"Major Component\", in this context, means a major essential component\n(kernel, window system, and so on) of the specific operating system\n(if any) on which the executable work runs, or a compiler used to\nproduce the work, or an object code interpreter used to run it.\n\n  The \"Corresponding Source\" for a work in object code form means all\nthe source code needed to generate, install, and (for an executable\nwork) run the object code and to modify the work, including scripts to\ncontrol those activities.  However, it does not include the work's\nSystem Libraries, or general-purpose tools or generally available free\nprograms which are used unmodified in performing those activities but\nwhich are not part of the work.  For example, Corresponding Source\nincludes interface definition files associated with source files for\nthe work, and the source code for shared libraries and dynamically\nlinked subprograms that the work is specifically designed to require,\nsuch as by intimate data communication or control flow between those\nsubprograms and other parts of the work.\n\n  The Corresponding Source need not include anything that users\ncan regenerate automatically from other parts of the Corresponding\nSource.\n\n  The Corresponding Source for a work in source code form is that\nsame work.\n\n  2. Basic Permissions.\n\n  All rights granted under this License are granted for the term of\ncopyright on the Program, and are irrevocable provided the stated\nconditions are met.  This License explicitly affirms your unlimited\npermission to run the unmodified Program.  The output from running a\ncovered work is covered by this License only if the output, given its\ncontent, constitutes a covered work.  This License acknowledges your\nrights of fair use or other equivalent, as provided by copyright law.\n\n  You may make, run and propagate covered works that you do not\nconvey, without conditions so long as your license otherwise remains\nin force.  You may convey covered works to others for the sole purpose\nof having them make modifications exclusively for you, or provide you\nwith facilities for running those works, provided that you comply with\nthe terms of this License in conveying all material for which you do\nnot control copyright.  Those thus making or running the covered works\nfor you must do so exclusively on your behalf, under your direction\nand control, on terms that prohibit them from making any copies of\nyour copyrighted material outside their relationship with you.\n\n  Conveying under any other circumstances is permitted solely under\nthe conditions stated below.  Sublicensing is not allowed; section 10\nmakes it unnecessary.\n\n  3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n\n  No covered work shall be deemed part of an effective technological\nmeasure under any applicable law fulfilling obligations under article\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\nsimilar laws prohibiting or restricting circumvention of such\nmeasures.\n\n  When you convey a covered work, you waive any legal power to forbid\ncircumvention of technological measures to the extent such circumvention\nis effected by exercising rights under this License with respect to\nthe covered work, and you disclaim any intention to limit operation or\nmodification of the work as a means of enforcing, against the work's\nusers, your or third parties' legal rights to forbid circumvention of\ntechnological measures.\n\n  4. Conveying Verbatim Copies.\n\n  You may convey verbatim copies of the Program's source code as you\nreceive it, in any medium, provided that you conspicuously and\nappropriately publish on each copy an appropriate copyright notice;\nkeep intact all notices stating that this License and any\nnon-permissive terms added in accord with section 7 apply to the code;\nkeep intact all notices of the absence of any warranty; and give all\nrecipients a copy of this License along with the Program.\n\n  You may charge any price or no price for each copy that you convey,\nand you may offer support or warranty protection for a fee.\n\n  5. Conveying Modified Source Versions.\n\n  You may convey a work based on the Program, or the modifications to\nproduce it from the Program, in the form of source code under the\nterms of section 4, provided that you also meet all of these conditions:\n\n    a) The work must carry prominent notices stating that you modified\n    it, and giving a relevant date.\n\n    b) The work must carry prominent notices stating that it is\n    released under this License and any conditions added under section\n    7.  This requirement modifies the requirement in section 4 to\n    \"keep intact all notices\".\n\n    c) You must license the entire work, as a whole, under this\n    License to anyone who comes into possession of a copy.  This\n    License will therefore apply, along with any applicable section 7\n    additional terms, to the whole of the work, and all its parts,\n    regardless of how they are packaged.  This License gives no\n    permission to license the work in any other way, but it does not\n    invalidate such permission if you have separately received it.\n\n    d) If the work has interactive user interfaces, each must display\n    Appropriate Legal Notices; however, if the Program has interactive\n    interfaces that do not display Appropriate Legal Notices, your\n    work need not make them do so.\n\n  A compilation of a covered work with other separate and independent\nworks, which are not by their nature extensions of the covered work,\nand which are not combined with it such as to form a larger program,\nin or on a volume of a storage or distribution medium, is called an\n\"aggregate\" if the compilation and its resulting copyright are not\nused to limit the access or legal rights of the compilation's users\nbeyond what the individual works permit.  Inclusion of a covered work\nin an aggregate does not cause this License to apply to the other\nparts of the aggregate.\n\n  6. Conveying Non-Source Forms.\n\n  You may convey a covered work in object code form under the terms\nof sections 4 and 5, provided that you also convey the\nmachine-readable Corresponding Source under the terms of this License,\nin one of these ways:\n\n    a) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by the\n    Corresponding Source fixed on a durable physical medium\n    customarily used for software interchange.\n\n    b) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by a\n    written offer, valid for at least three years and valid for as\n    long as you offer spare parts or customer support for that product\n    model, to give anyone who possesses the object code either (1) a\n    copy of the Corresponding Source for all the software in the\n    product that is covered by this License, on a durable physical\n    medium customarily used for software interchange, for a price no\n    more than your reasonable cost of physically performing this\n    conveying of source, or (2) access to copy the\n    Corresponding Source from a network server at no charge.\n\n    c) Convey individual copies of the object code with a copy of the\n    written offer to provide the Corresponding Source.  This\n    alternative is allowed only occasionally and noncommercially, and\n    only if you received the object code with such an offer, in accord\n    with subsection 6b.\n\n    d) Convey the object code by offering access from a designated\n    place (gratis or for a charge), and offer equivalent access to the\n    Corresponding Source in the same way through the same place at no\n    further charge.  You need not require recipients to copy the\n    Corresponding Source along with the object code.  If the place to\n    copy the object code is a network server, the Corresponding Source\n    may be on a different server (operated by you or a third party)\n    that supports equivalent copying facilities, provided you maintain\n    clear directions next to the object code saying where to find the\n    Corresponding Source.  Regardless of what server hosts the\n    Corresponding Source, you remain obligated to ensure that it is\n    available for as long as needed to satisfy these requirements.\n\n    e) Convey the object code using peer-to-peer transmission, provided\n    you inform other peers where the object code and Corresponding\n    Source of the work are being offered to the general public at no\n    charge under subsection 6d.\n\n  A separable portion of the object code, whose source code is excluded\nfrom the Corresponding Source as a System Library, need not be\nincluded in conveying the object code work.\n\n  A \"User Product\" is either (1) a \"consumer product\", which means any\ntangible personal property which is normally used for personal, family,\nor household purposes, or (2) anything designed or sold for incorporation\ninto a dwelling.  In determining whether a product is a consumer product,\ndoubtful cases shall be resolved in favor of coverage.  For a particular\nproduct received by a particular user, \"normally used\" refers to a\ntypical or common use of that class of product, regardless of the status\nof the particular user or of the way in which the particular user\nactually uses, or expects or is expected to use, the product.  A product\nis a consumer product regardless of whether the product has substantial\ncommercial, industrial or non-consumer uses, unless such uses represent\nthe only significant mode of use of the product.\n\n  \"Installation Information\" for a User Product means any methods,\nprocedures, authorization keys, or other information required to install\nand execute modified versions of a covered work in that User Product from\na modified version of its Corresponding Source.  The information must\nsuffice to ensure that the continued functioning of the modified object\ncode is in no case prevented or interfered with solely because\nmodification has been made.\n\n  If you convey an object code work under this section in, or with, or\nspecifically for use in, a User Product, and the conveying occurs as\npart of a transaction in which the right of possession and use of the\nUser Product is transferred to the recipient in perpetuity or for a\nfixed term (regardless of how the transaction is characterized), the\nCorresponding Source conveyed under this section must be accompanied\nby the Installation Information.  But this requirement does not apply\nif neither you nor any third party retains the ability to install\nmodified object code on the User Product (for example, the work has\nbeen installed in ROM).\n\n  The requirement to provide Installation Information does not include a\nrequirement to continue to provide support service, warranty, or updates\nfor a work that has been modified or installed by the recipient, or for\nthe User Product in which it has been modified or installed.  Access to a\nnetwork may be denied when the modification itself materially and\nadversely affects the operation of the network or violates the rules and\nprotocols for communication across the network.\n\n  Corresponding Source conveyed, and Installation Information provided,\nin accord with this section must be in a format that is publicly\ndocumented (and with an implementation available to the public in\nsource code form), and must require no special password or key for\nunpacking, reading or copying.\n\n  7. Additional Terms.\n\n  \"Additional permissions\" are terms that supplement the terms of this\nLicense by making exceptions from one or more of its conditions.\nAdditional permissions that are applicable to the entire Program shall\nbe treated as though they were included in this License, to the extent\nthat they are valid under applicable law.  If additional permissions\napply only to part of the Program, that part may be used separately\nunder those permissions, but the entire Program remains governed by\nthis License without regard to the additional permissions.\n\n  When you convey a copy of a covered work, you may at your option\nremove any additional permissions from that copy, or from any part of\nit.  (Additional permissions may be written to require their own\nremoval in certain cases when you modify the work.)  You may place\nadditional permissions on material, added by you to a covered work,\nfor which you have or can give appropriate copyright permission.\n\n  Notwithstanding any other provision of this License, for material you\nadd to a covered work, you may (if authorized by the copyright holders of\nthat material) supplement the terms of this License with terms:\n\n    a) Disclaiming warranty or limiting liability differently from the\n    terms of sections 15 and 16 of this License; or\n\n    b) Requiring preservation of specified reasonable legal notices or\n    author attributions in that material or in the Appropriate Legal\n    Notices displayed by works containing it; or\n\n    c) Prohibiting misrepresentation of the origin of that material, or\n    requiring that modified versions of such material be marked in\n    reasonable ways as different from the original version; or\n\n    d) Limiting the use for publicity purposes of names of licensors or\n    authors of the material; or\n\n    e) Declining to grant rights under trademark law for use of some\n    trade names, trademarks, or service marks; or\n\n    f) Requiring indemnification of licensors and authors of that\n    material by anyone who conveys the material (or modified versions of\n    it) with contractual assumptions of liability to the recipient, for\n    any liability that these contractual assumptions directly impose on\n    those licensors and authors.\n\n  All other non-permissive additional terms are considered \"further\nrestrictions\" within the meaning of section 10.  If the Program as you\nreceived it, or any part of it, contains a notice stating that it is\ngoverned by this License along with a term that is a further\nrestriction, you may remove that term.  If a license document contains\na further restriction but permits relicensing or conveying under this\nLicense, you may add to a covered work material governed by the terms\nof that license document, provided that the further restriction does\nnot survive such relicensing or conveying.\n\n  If you add terms to a covered work in accord with this section, you\nmust place, in the relevant source files, a statement of the\nadditional terms that apply to those files, or a notice indicating\nwhere to find the applicable terms.\n\n  Additional terms, permissive or non-permissive, may be stated in the\nform of a separately written license, or stated as exceptions;\nthe above requirements apply either way.\n\n  8. Termination.\n\n  You may not propagate or modify a covered work except as expressly\nprovided under this License.  Any attempt otherwise to propagate or\nmodify it is void, and will automatically terminate your rights under\nthis License (including any patent licenses granted under the third\nparagraph of section 11).\n\n  However, if you cease all violation of this License, then your\nlicense from a particular copyright holder is reinstated (a)\nprovisionally, unless and until the copyright holder explicitly and\nfinally terminates your license, and (b) permanently, if the copyright\nholder fails to notify you of the violation by some reasonable means\nprior to 60 days after the cessation.\n\n  Moreover, your license from a particular copyright holder is\nreinstated permanently if the copyright holder notifies you of the\nviolation by some reasonable means, this is the first time you have\nreceived notice of violation of this License (for any work) from that\ncopyright holder, and you cure the violation prior to 30 days after\nyour receipt of the notice.\n\n  Termination of your rights under this section does not terminate the\nlicenses of parties who have received copies or rights from you under\nthis License.  If your rights have been terminated and not permanently\nreinstated, you do not qualify to receive new licenses for the same\nmaterial under section 10.\n\n  9. Acceptance Not Required for Having Copies.\n\n  You are not required to accept this License in order to receive or\nrun a copy of the Program.  Ancillary propagation of a covered work\noccurring solely as a consequence of using peer-to-peer transmission\nto receive a copy likewise does not require acceptance.  However,\nnothing other than this License grants you permission to propagate or\nmodify any covered work.  These actions infringe copyright if you do\nnot accept this License.  Therefore, by modifying or propagating a\ncovered work, you indicate your acceptance of this License to do so.\n\n  10. Automatic Licensing of Downstream Recipients.\n\n  Each time you convey a covered work, the recipient automatically\nreceives a license from the original licensors, to run, modify and\npropagate that work, subject to this License.  You are not responsible\nfor enforcing compliance by third parties with this License.\n\n  An \"entity transaction\" is a transaction transferring control of an\norganization, or substantially all assets of one, or subdividing an\norganization, or merging organizations.  If propagation of a covered\nwork results from an entity transaction, each party to that\ntransaction who receives a copy of the work also receives whatever\nlicenses to the work the party's predecessor in interest had or could\ngive under the previous paragraph, plus a right to possession of the\nCorresponding Source of the work from the predecessor in interest, if\nthe predecessor has it or can get it with reasonable efforts.\n\n  You may not impose any further restrictions on the exercise of the\nrights granted or affirmed under this License.  For example, you may\nnot impose a license fee, royalty, or other charge for exercise of\nrights granted under this License, and you may not initiate litigation\n(including a cross-claim or counterclaim in a lawsuit) alleging that\nany patent claim is infringed by making, using, selling, offering for\nsale, or importing the Program or any portion of it.\n\n  11. Patents.\n\n  A \"contributor\" is a copyright holder who authorizes use under this\nLicense of the Program or a work on which the Program is based.  The\nwork thus licensed is called the contributor's \"contributor version\".\n\n  A contributor's \"essential patent claims\" are all patent claims\nowned or controlled by the contributor, whether already acquired or\nhereafter acquired, that would be infringed by some manner, permitted\nby this License, of making, using, or selling its contributor version,\nbut do not include claims that would be infringed only as a\nconsequence of further modification of the contributor version.  For\npurposes of this definition, \"control\" includes the right to grant\npatent sublicenses in a manner consistent with the requirements of\nthis License.\n\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\npatent license under the contributor's essential patent claims, to\nmake, use, sell, offer for sale, import and otherwise run, modify and\npropagate the contents of its contributor version.\n\n  In the following three paragraphs, a \"patent license\" is any express\nagreement or commitment, however denominated, not to enforce a patent\n(such as an express permission to practice a patent or covenant not to\nsue for patent infringement).  To \"grant\" such a patent license to a\nparty means to make such an agreement or commitment not to enforce a\npatent against the party.\n\n  If you convey a covered work, knowingly relying on a patent license,\nand the Corresponding Source of the work is not available for anyone\nto copy, free of charge and under the terms of this License, through a\npublicly available network server or other readily accessible means,\nthen you must either (1) cause the Corresponding Source to be so\navailable, or (2) arrange to deprive yourself of the benefit of the\npatent license for this particular work, or (3) arrange, in a manner\nconsistent with the requirements of this License, to extend the patent\nlicense to downstream recipients.  \"Knowingly relying\" means you have\nactual knowledge that, but for the patent license, your conveying the\ncovered work in a country, or your recipient's use of the covered work\nin a country, would infringe one or more identifiable patents in that\ncountry that you have reason to believe are valid.\n\n  If, pursuant to or in connection with a single transaction or\narrangement, you convey, or propagate by procuring conveyance of, a\ncovered work, and grant a patent license to some of the parties\nreceiving the covered work authorizing them to use, propagate, modify\nor convey a specific copy of the covered work, then the patent license\nyou grant is automatically extended to all recipients of the covered\nwork and works based on it.\n\n  A patent license is \"discriminatory\" if it does not include within\nthe scope of its coverage, prohibits the exercise of, or is\nconditioned on the non-exercise of one or more of the rights that are\nspecifically granted under this License.  You may not convey a covered\nwork if you are a party to an arrangement with a third party that is\nin the business of distributing software, under which you make payment\nto the third party based on the extent of your activity of conveying\nthe work, and under which the third party grants, to any of the\nparties who would receive the covered work from you, a discriminatory\npatent license (a) in connection with copies of the covered work\nconveyed by you (or copies made from those copies), or (b) primarily\nfor and in connection with specific products or compilations that\ncontain the covered work, unless you entered into that arrangement,\nor that patent license was granted, prior to 28 March 2007.\n\n  Nothing in this License shall be construed as excluding or limiting\nany implied license or other defenses to infringement that may\notherwise be available to you under applicable patent law.\n\n  12. No Surrender of Others' Freedom.\n\n  If conditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot convey a\ncovered work so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you may\nnot convey it at all.  For example, if you agree to terms that obligate you\nto collect a royalty for further conveying from those to whom you convey\nthe Program, the only way you could satisfy both those terms and this\nLicense would be to refrain entirely from conveying the Program.\n\n  13. Use with the GNU Affero General Public License.\n\n  Notwithstanding any other provision of this License, you have\npermission to link or combine any covered work with a work licensed\nunder version 3 of the GNU Affero General Public License into a single\ncombined work, and to convey the resulting work.  The terms of this\nLicense will continue to apply to the part which is the covered work,\nbut the special requirements of the GNU Affero General Public License,\nsection 13, concerning interaction through a network will apply to the\ncombination as such.\n\n  14. Revised Versions of this License.\n\n  The Free Software Foundation may publish revised and/or new versions of\nthe GNU General Public License from time to time.  Such new versions will\nbe similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\n  Each version is given a distinguishing version number.  If the\nProgram specifies that a certain numbered version of the GNU General\nPublic License \"or any later version\" applies to it, you have the\noption of following the terms and conditions either of that numbered\nversion or of any later version published by the Free Software\nFoundation.  If the Program does not specify a version number of the\nGNU General Public License, you may choose any version ever published\nby the Free Software Foundation.\n\n  If the Program specifies that a proxy can decide which future\nversions of the GNU General Public License can be used, that proxy's\npublic statement of acceptance of a version permanently authorizes you\nto choose that version for the Program.\n\n  Later license versions may give you additional or different\npermissions.  However, no additional obligations are imposed on any\nauthor or copyright holder as a result of your choosing to follow a\nlater version.\n\n  15. Disclaimer of Warranty.\n\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n  16. Limitation of Liability.\n\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGES.\n\n  17. Interpretation of Sections 15 and 16.\n\n  If the disclaimer of warranty and limitation of liability provided\nabove cannot be given local legal effect according to their terms,\nreviewing courts shall apply local law that most closely approximates\nan absolute waiver of all civil liability in connection with the\nProgram, unless a warranty or assumption of liability accompanies a\ncopy of the Program in return for a fee.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nstate the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    <one line to give the program's name and a brief idea of what it does.>\n    Copyright (C) <year>  <name of author>\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\nAlso add information on how to contact you by electronic and paper mail.\n\n  If the program does terminal interaction, make it output a short\nnotice like this when it starts in an interactive mode:\n\n    <program>  Copyright (C) <year>  <name of author>\n    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n\nThe hypothetical commands `show w' and `show c' should show the appropriate\nparts of the General Public License.  Of course, your program's commands\nmight be different; for a GUI interface, you would use an \"about box\".\n\n  You should also get your employer (if you work as a programmer) or school,\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\nFor more information on this, and how to apply and follow the GNU GPL, see\n<https://www.gnu.org/licenses/>.\n\n  The GNU General Public License does not permit incorporating your program\ninto proprietary programs.  If your program is a subroutine library, you\nmay consider it more useful to permit linking proprietary applications with\nthe library.  If this is what you want to do, use the GNU Lesser General\nPublic License instead of this License.  But first, please read\n<https://www.gnu.org/licenses/why-not-lgpl.html>.\n"
    },
    {
      "path": "README.org",
      "content": "#+TITLE: xcontext\n#+DATE: <2025-04-24 Thu>\n#+AUTHOR: json\n#+STARTUP: showeverything\n#+OPTIONS: toc:t\n\n* Overview\n~xcontext~ is a command-line tool designed to generate comprehensive, structured context about software projects. It operates via subcommands like ~generate~, ~watch~, ~show~ subcommands (~meta/metas~, ~prompt/prompts~, ~rule/rules~), ~metrics~, etc., with short aliases available (e.g. ~g~, ~w~, ~s~). This context, typically output as JSON, YAML, or XML (controlled by the global ~-f/--format~ option), is ideal for feeding into AI models like LLMs to enable tasks such as code analysis, refactoring, documentation generation, and more. Global flags ~-v~ (verbose) and ~-q~ (quiet) control informational/warning message output.\n\nThe tool scans your project based on flexible configuration, gathers information about the project structure (tree), documentation files, source code files, system environment (*internally*), and user-defined metadata/rules. It includes a dynamic \"AI Readme\" within the output to help explain the context structure and purpose to an AI. It prioritizes user convenience and performance through parallel processing.\n\nKey features include:\n- Subcommand-based interface with short aliases (~generate/g~, ~watch/w~, ~show/s~, ~metrics/m~, ~debug/d~, ~quick/q~, ~completion~, ~config~, ~cl/c~, ~mcp~).\n- Layered configuration (Defaults -> TOML File -> CLI Flags) using snake_case keys in TOML. Global flags control project root and config file loading (~--project-root~, ~--context-file~, ~--disable-context-file~).\n- Automatic project root detection (CLI -> Env Var -> CWD).\n- Detailed context generation via ~generate~ command. Output field order is consistent.\n- *Enhanced AI Readme*: Includes an ~ai_readme~ field explaining the included sections.\n- Optional exclusion of core fields (via flags under ~generate~).\n- Selective inclusion/exclusion of context sections via flags or config (under ~generate~).\n- Flexible filtering of files/folders using paths and glob patterns (respects `.gitignore` by default, configurable globally via ~generate --enable/disable-gitignore~). Trailing `/` in filters implies recursive directory matching, like `.gitignore`.\n- *Common Filters*: Define include/exclude patterns in `[common_filters]` to apply them to docs, tree, and source sections unless overridden.\n- *Section-Aware Built-in Ignores*: Ignores common files by default, configurable via ~generate --enable/disable-builtin-ignore~ and sectionally via ~data/builtin_ignores.yaml~ (trailing `/` implies recursion).\n- Separate handling for documentation files.\n- *Nested Source Representation*: Source code included under ~source~ key (either `files` array or `chunks` array).\n- *Refined Rules Handling*:\n  - Select built-in static rules (common & language-specific) via `[rules].include_static = [...]`.\n  - Import *additional* user rules from external files (`.org`, `.txt`) via `[rules].import = [...]`.\n  - Define custom rules directly in TOML via `<your_rule_name> = [...]`.\n  - Output keys clearly prefixed: `static:`, `imported:`, `custom:`.\n- *Prompt Imports*: Import prompt text from external files via `[prompts].import = [...]`.\n- Multiple output formats (~json~, ~yaml~, ~xml~, use global ~-f~) with optional JSON minification (global ~--enable/disable-json-minify~) and *XML pretty-printing* (global ~--enable/disable-xml-pretty~). Applies to structured output needs.\n- Default human-readable output for ~metrics~, ~debug~, and plural ~show~ commands (~metas~, ~prompts~, ~rules~). ~quick~ defaults to minified JSON. Singular ~show~ defaults to plain text or key listing.\n- *Enhanced Save Logic*: Use `-s` / `--save` without a path argument to save to the configured `[save].output_dir` or the current directory as a fallback. Optional saving of context/config/completions (~generate -s [path]~, ~watch -s [path]~, ~config --save~, ~completion --save~).\n- Chunking of large source content (~generate -c~, JSON only).\n- Watch mode (~watch~) to automatically regenerate context on file changes (reloads config/rewatches files if config changes).\n- Quick mode (~quick~) to extract specific file contents. Handles directory patterns (`data/` implies `data/**`).\n- Utility commands for:\n  - Default config inspection/saving (~config~ command with ~--save~ flag).\n  - Showing specific items or listing keys (~show meta/prompt/rule~), or showing all content (~show metas/prompts/rules~).\n  - Debugging file inclusions and effective config (~debug~ command).\n  - Viewing project metrics (~metrics~ command, pretty table default).\n- Shell completion generation (~completion~ command, Fish default, others optional, default to stdout).\n- Simple screen clear utility (~cl~ command).\n- Verbosity control (~-q~ silences info/warnings, ~-v~ shows info).\n\n* Requirements\n - Operating System: Linux (tested on Fedora, likely compatible with others).\n - Rust Toolchain: Cargo and Rust compiler (check `Cargo.toml` for version).\n - *For Fish Completions:* Fish shell installed.\n\n* Installation\n** From Source\n   1. Clone the repository.\n   2. Navigate to the project directory.\n   3. Build and install using Cargo:\n      #+BEGIN_SRC shell\n        cargo install --path .\n      #+END_SRC\n      This will typically place the `xcontext` binary in `~/.cargo/bin/`. Alternatively, build a release binary:\n      #+BEGIN_SRC shell\n        cargo build --release\n      #+END_SRC\n      The binary will be located at `target/release/xcontext`. You can copy this to a directory in your system's ~PATH~.\n\n* Setup (Configuration)\n** Configuration File\n   - Uses `xcontext.toml`, typically in `.xtools/xcontext/` relative to project root (path configurable via global ~--context-file~).\n   - Keys use snake_case (e.g., ~use_gitignore~, ~enable_builtin_ignore~). Use trailing `/` for directory excludes/includes to imply recursive contents (e.g., `exclude = [\"target/\", \"node_modules/\"]`).\n   - Loading disabled via global ~--disable-context-file~.\n   - See [[file:SPEC.org::Sample Configuration (xcontext.toml)][Sample Configuration]] in SPEC.org for the structure and new features like `[common_filters]`, `[rules].include_static`, `[rules].import`, `[prompts].import`.\n** Generating a Starting Configuration\n   - Use ~xcontext config > path/to/xcontext.toml~ to view the default config structure.\n   - Use ~xcontext config --save~ to save the default config structure to the default location (~./.xtools/xcontext/xcontext.toml~), prompting for overwrite.\n** Configuration Loading\n   - Layers: Defaults -> TOML File -> CLI Flags.\n   - Use ~xcontext debug~ (or ~d~) to see the final *effective* configuration and included file lists.\n** Shell Completions\n   - Use ~xcontext completion~ to view Fish script or ~xcontext completion --save~ to save it. Use ~--shell~ for others.\n\n* Usage\n  #+BEGIN_SRC shell\n    xcontext [GLOBAL_OPTIONS] <COMMAND> [COMMAND_OPTIONS]\n  #+END_SRC\n  - Run ~xcontext --help~ or ~xcontext h~ for top-level help.\n  - Run ~xcontext <COMMAND> --help~ for help on a specific command (e.g., ~xcontext generate --help~).\n  - If no command is given, help is displayed. Use ~generate~ or ~g~ explicitly for default action.\n\n* Examples\n** Basic Usage\n   #+BEGIN_SRC shell\n     # Generate context explicitly and view it\n     xcontext generate | less\n     xcontext g | less # Alias\n\n     # Generate context for a different project\n     xcontext --project-root /path/to/project g\n\n     # Generate context in YAML format\n     xcontext g -f yaml\n\n     # Generate pretty-printed JSON\n     xcontext g --disable-json-minify\n\n     # Generate pretty-printed XML\n     xcontext g -f xml --enable-xml-pretty\n\n     # Generate context without loading any TOML config file\n     xcontext g --disable-context-file\n\n     # Generate context excluding project name and timestamp\n     xcontext g --exclude-project-name --exclude-timestamp\n\n     # Show version info\n     xcontext -v\n     xcontext --version\n\n     # Run generate verbosely (shows info messages like file read errors)\n     xcontext g -v\n\n     # Run generate quietly (suppresses warnings/info)\n     xcontext g -q\n\n     # Clear the screen\n     xcontext cl\n     xcontext c # Alias\n   #+END_SRC\n\n** Saving & Chunking (Generate Command)\n   #+BEGIN_SRC shell\n     # Save context using default path logic (config or CWD)\n     xcontext g -s\n\n     # Save context as JSON explicitly to ./output\n     xcontext g -s ./output\n\n     # Save context as pretty XML\n     xcontext g -f xml --enable-xml-pretty -s ./output_files\n\n     # Chunk source files into ~5MB JSON files and save to default location\n     xcontext g -c 5MB -s\n   #+END_SRC\n\n** Filtering Content (Generate Command)\n   #+BEGIN_SRC shell\n     # Only include Rust source files and Cargo.toml\n     xcontext g --source-include 'src/**/*.rs' --source-include 'Cargo.toml'\n\n     # Exclude test directories from source (use trailing / or /**)\n     # This assumes no common_filters exclude it already\n     xcontext g --source-exclude 'src/tests/'\n\n     # Generate context but ignore .gitignore files globally\n     xcontext g --disable-gitignore\n\n     # Generate context without using the default built-in ignores\n     xcontext g --disable-builtin-ignore\n   #+END_SRC\n\n** Watch Mode\n   #+BEGIN_SRC shell\n     # Watch for changes and print context to stdout\n     xcontext watch\n     xcontext w # Alias\n\n     # Watch for changes and save to disk (default location) with a 1-second delay\n     xcontext w --watch-delay 1s -s\n   #+END_SRC\n\n** Quick Mode\n   #+BEGIN_SRC shell\n     # Quickly output content of all TOML files (minified JSON default)\n     xcontext quick '**/*.toml'\n     xcontext q '**/*.toml' # Alias\n\n     # Quickly output content of files in data dir as YAML\n     xcontext q data/ -f yaml # Handles directory path\n   #+END_SRC\n\n** Utility Commands\n   #+BEGIN_SRC shell\n     # Show the *default* configuration structure (TOML to stdout)\n     xcontext config\n\n     # Save the *default* configuration structure (prompts for overwrite)\n     xcontext config --save\n\n     # Show content of all available prompts (human-readable default)\n     # Includes built-in, custom, and imported prompts.\n     xcontext show prompts\n     xcontext s prompts # Alias\n\n     # Show all available prompts as YAML\n     xcontext s prompts -f yaml\n\n     # Show a specific prompt (plain text default)\n     xcontext show prompt custom:code_review # Use prefix if needed\n\n     # List available rule definition names (static, imported, custom)\n     xcontext show rule # Use singular to list keys\n\n     # Show content of all available rule definitions (human-readable default)\n     xcontext show rules # Use plural to show all content\n\n     # Show content of all rules as JSON\n     xcontext s rules -f json\n\n     # Show content of a specific imported rule file\n     xcontext show rule imported:my_company_standards\n\n     # Show all custom metadata key-value pairs (human-readable default)\n     xcontext show metas\n\n     # Show the value for a specific metadata key (plain text default)\n     xcontext show meta version # Use singular to show specific\n\n     # Show overall project metrics (human-readable default table)\n     xcontext metrics\n     xcontext m # Alias\n\n     # Show overall project metrics as YAML\n     xcontext m -f yaml\n\n     # Show detailed debug information (human-readable default)\n     # Includes effective config, common filters, included files\n     xcontext debug\n     xcontext d # Alias\n\n     # Show Fish shell completion script to stdout (fish is default)\n     xcontext completion\n\n     # Save Bash shell completion script\n     xcontext completion --shell bash --save\n   #+END_SRC\n\n* Configuration Details\n  - See the TOML structure, sample configuration, and detailed explanations in [[file:SPEC.org]].\n  - Key new sections/keys: `[common_filters]`, `[rules].include_static`, `[rules].import`, `[prompts].import`, `[output].xml_pretty_print`.\n  - Built-in ignore patterns are in ~data/builtin_ignores.yaml~ and can be section-specific (`common:`, `tree:`, `source:`, `docs:`). Trailing `/` in exclude/include patterns implies recursive directory matching.\n\n* Output Structure Summary\n    The generated output (JSON/YAML/XML) via the `generate` command contains the following top-level fields (order roughly as shown, optional fields depend on config/flags):\n    - `ai_readme`: (String) An enhanced guide for AI explaining the included fields.\n    - `project_name`: (String, Optional) The determined project name.\n    - `project_root`: (String, Optional) The absolute path to the project root.\n    - `system_info`: (Object, Optional) Information about the generation environment.\n    - `meta`: (Object, Optional) Key-value pairs from config and `--add-meta`.\n    - `docs`: (Array, Optional) List of documentation files: `{ \"path\": \"...\", \"content\": \"...\" }`.\n    - `tree`: (Array, Optional) Hierarchical representation of the directory structure.\n    - `source`: (Object, Optional) Contains *either* `files` or `chunks`:\n      - `files`: (Array) List of source files: `{ \"path\": \"...\", \"content\": \"...\" }`.\n      - `chunks`: (Array) List of relative paths to chunk files (String).\n    - `rules`: (Object, Optional) Map where keys are rule set names (prefixed `static:`, `imported:`, `custom:`) and values are lists of rule strings.\n    - `generation_timestamp`: (String, Optional) ISO 8601 timestamp of generation.\n\n\n* References\n  - [[file:CLI.org][CLI Options Specification]]\n  - [[file:SPEC.org][Detailed Specification & TOML Structure]]\n\n* License\n  [[file:LICENSE][GPLv3 License]]\n"
    },
    {
      "path": "Rakefile",
      "content": "# frozen_string_literal: true\n\nrequire 'date'\nrequire 'fileutils'\n\nPROJECT_ROOT = ENV.fetch('PROJECT_ROOT', Dir.pwd)\nPROJECT_NAME_STR = ENV.fetch('PROJECT_NAME', File.basename(PROJECT_ROOT))\nPROJECT_NAME_SYM = PROJECT_NAME_STR.to_sym\nPROJECT_CLI_CRATE_NAME = \"#{PROJECT_NAME_STR}-cli\"\n\nNEXUS_DIR = ENV.fetch('JSWM_NEXUS_DIR')\nINSTALL_DIR = ENV.fetch('PROJECT_INSTALL_DIR', \"#{NEXUS_DIR}/xtools\")\nINSTALL_PATH = File.join(INSTALL_DIR, PROJECT_NAME_STR)\n\nCARGO_TARGET_DIR = ENV.fetch('CARGO_TARGET_DIR', File.join(PROJECT_ROOT, 'target'))\nRELEASE_BINARY_PATH = File.join(CARGO_TARGET_DIR, 'release', PROJECT_NAME_STR)\n\nFISH_COMPLETION_DIR = File.expand_path('~/.config/fish/completions')\nFISH_COMPLETION_FILE = File.join(FISH_COMPLETION_DIR, \"#{PROJECT_NAME_STR}.fish\")\n\nDir.glob(File.join(PROJECT_ROOT, 'tasks/**/*.rake')).sort.each { |r| import r }\n\nnamespace PROJECT_NAME_SYM do\n  desc 'Check for errors'\n  task :check do\n    Dir.chdir(PROJECT_ROOT) { sh 'cargo check --all --all-features' }\n  end\n\n  desc 'Build all crates (debug)'\n  task :build do\n    Dir.chdir(PROJECT_ROOT) { sh 'cargo build --all --all-features' }\n  end\n\n  desc 'Build release version'\n  task :build_release do\n    puts \"Building release version of #{PROJECT_NAME_STR}...\"\n    Dir.chdir(PROJECT_ROOT) do\n      sh \"cargo build --release --all-features --target-dir #{CARGO_TARGET_DIR}\"\n    end\n    raise \"Release build failed: Main binary #{RELEASE_BINARY_PATH} not found.\" unless File.exist?(RELEASE_BINARY_PATH)\n\n    puts \"Release build successful: #{RELEASE_BINARY_PATH}\"\n  end\n\n  desc 'Run tests'\n  task :test do\n    Dir.chdir(PROJECT_ROOT) { sh 'cargo test --all --all-features' }\n  end\n\n  desc 'Run CLI with default project config to sync all projects'\n  task :run do\n    Dir.chdir(PROJECT_ROOT) { sh \"cargo run -p #{PROJECT_CLI_CRATE_NAME} --all-features -- sync\" }\n  end\n\n  desc \"Run CLI to sync a specific project (e.g., rake #{PROJECT_NAME_SYM}:run_project[MyProject1])\"\n  task :run_project, [:project_name] do |_t, args|\n    project_name_arg = args[:project_name]\n    raise \"Project name is required. Usage: rake #{PROJECT_NAME_SYM}:run_project[ProjectName]\" unless project_name_arg\n\n    Dir.chdir(PROJECT_ROOT) do\n      sh \"cargo run -p #{PROJECT_CLI_CRATE_NAME} --all-features -- sync --project #{project_name_arg}\"\n    end\n  end\n\n  desc 'Format code'\n  task :fmt do\n    Dir.chdir(PROJECT_ROOT) { sh 'cargo fmt --all' }\n  end\n\n  desc 'Lint with clippy'\n  task :lint do\n    Dir.chdir(PROJECT_ROOT) do\n      sh 'cargo clippy --all --all-features -- -D warnings -A clippy::style -A clippy::complexity -A clippy::perf -A clippy::pedantic -A clippy::restriction -A clippy::nursery -A clippy::cargo'\n    end\n  end\n\n  desc 'Generate documentation'\n  task :doc do\n    Dir.chdir(PROJECT_ROOT) { sh 'cargo doc --open --all-features --no-deps' }\n  end\n\n  desc 'Generate Fish shell completion'\n  task :gen_fish_completion do\n    Rake::Task[\"#{PROJECT_NAME_SYM}:build_release\"].invoke unless File.exist?(RELEASE_BINARY_PATH)\n    unless File.exist?(RELEASE_BINARY_PATH)\n      raise \"Cannot generate completions: Release binary #{RELEASE_BINARY_PATH} not found.\"\n    end\n\n    puts \"Generating Fish completion script using #{RELEASE_BINARY_PATH}...\"\n    completion_script = `\"#{RELEASE_BINARY_PATH}\" completion`\n    unless $?.success? && !completion_script.empty?\n      raise \"Failed to generate fish completion script using #{RELEASE_BINARY_PATH}. Output: #{completion_script}\"\n    end\n\n    FileUtils.mkdir_p(FISH_COMPLETION_DIR) unless Dir.exist?(FISH_COMPLETION_DIR)\n    File.write(FISH_COMPLETION_FILE, completion_script)\n    puts \"Fish completion script written to #{FISH_COMPLETION_FILE}\"\n  end\n\n  desc 'Convert Mermaid .mmd files to versioned SVG'\n  task :mermaid do\n    Dir.chdir(PROJECT_ROOT) do\n      timestamp = Date.today.strftime('%Y%m%d')\n      Dir.glob('.mermaid/mmd/*.mmd').each do |mmd|\n        base_name = File.basename(mmd, '.mmd')\n        svg_dir = File.join(PROJECT_ROOT, '.mermaid', 'diagrams')\n        FileUtils.mkdir_p(svg_dir) unless Dir.exist?(svg_dir)\n        svg = File.join(svg_dir, \"#{base_name}_#{timestamp}.svg\")\n        sh \"mmdc -i \\\"#{mmd}\\\" -o \\\"#{svg}\\\"\"\n      end\n    end\n  end\n\n  desc 'Tangle Org-mode files'\n  task :org do\n    Dir.chdir(PROJECT_ROOT) do\n      Dir.glob(File.join(PROJECT_ROOT, '.org', '*.org')).each do |org_file|\n        sh \"emacs --batch --no-init-file --no-site-file --eval \\\"(require 'org)\\\" --eval \\\"(org-babel-tangle-file \\\\\\\"#{org_file}\\\\\\\")\\\"\"\n      end\n      mermaid_org_file = File.join(PROJECT_ROOT, '.mermaid', 'MERMAID.org')\n      if File.exist?(mermaid_org_file)\n        sh \"emacs --batch --no-init-file --no-site-file --eval \\\"(require 'org)\\\" --eval \\\"(org-babel-tangle-file \\\\\\\"#{mermaid_org_file}\\\\\\\")\\\"\"\n      end\n    end\n  end\n\n  desc 'Format, Lint, Check, Build Release, Install CLI, Generate Completion'\n  task deploy: %i[fmt lint check build_release] do\n    puts \"Deploying #{PROJECT_NAME_STR}...\"\n\n    FileUtils.mkdir_p(INSTALL_DIR) unless Dir.exist?(INSTALL_DIR)\n    puts \"Copying CLI binary #{RELEASE_BINARY_PATH} to #{INSTALL_PATH}...\"\n    FileUtils.cp(RELEASE_BINARY_PATH, INSTALL_PATH, verbose: true)\n    FileUtils.chmod(0o755, INSTALL_PATH)\n\n    Rake::Task[\"#{PROJECT_NAME_SYM}:gen_fish_completion\"].invoke\n\n    puts \"#{PROJECT_NAME_STR} deployed successfully!\"\n    puts \"- CLI installed to: #{INSTALL_PATH}\"\n    puts \"- Fish completions installed to: #{FISH_COMPLETION_FILE}\"\n    puts \"Ensure #{INSTALL_DIR} is in your $PATH.\"\n  end\n\n  default_action_tasks = %i[check build run]\n  desc \"Default task: #{default_action_tasks.join(', ')}\"\n  task default_action: default_action_tasks\nend\n\ndesc 'Setup project (install Rust, update, build dependencies, create default config)'\ntask :setup do\n  sh 'rustup update stable' if system('which rustup > /dev/null 2>&1')\n  Dir.chdir(PROJECT_ROOT) { sh 'cargo build --all --all-features' }\n  puts \"Project setup complete. Dependencies built. Default config is at #{DEFAULT_CONFIG_FILE}\"\nrescue StandardError => e\n  puts \"Setup failed: #{e.message}\"\nend\n\ndesc \"Default task: Runs #{PROJECT_NAME_SYM}:default_action\"\ntask default: [\"#{PROJECT_NAME_SYM}:default_action\"]\n\ntask r: [\"#{PROJECT_NAME_SYM}:run\"]\ntask c: [\"#{PROJECT_NAME_SYM}:check\"]\ntask t: [\"#{PROJECT_NAME_SYM}:test\"]\ntask b: [\"#{PROJECT_NAME_SYM}:build\"]\ntask br: [\"#{PROJECT_NAME_SYM}:build_release\"]\ntask f: [\"#{PROJECT_NAME_SYM}:fmt\"]\ntask l: [\"#{PROJECT_NAME_SYM}:lint\"]\ntask d: [\"#{PROJECT_NAME_SYM}:deploy\"]\ntask fish: [\"#{PROJECT_NAME_SYM}:gen_fish_completion\"]\ntask syncall: [\"#{PROJECT_NAME_SYM}:run\"]\ntask m: [\"#{PROJECT_NAME_SYM}:mermaid\"]\ntask o: [\"#{PROJECT_NAME_SYM}:org\"]\n"
    },
    {
      "path": "VISION.org",
      "content": "#+TITLE: Xcontext Vision\n#+AUTHOR: json\n#+DATE: May 07, 2025\n\n* Vision\n\nXcontext is an simple tool for generating project context for feeding to a AI chat apps.\n\nIt is specifially created for generating neat project context in an structure json format.\n\nTo see what the output of this cli might look like see: [[file:samples/xcontext_project.json][samples/xcontext_project.json]]\n\nMoving Forward:\n\nSee: [[file:handling-large-codebases.org][Handling Large Codebases]]\n\n> To be continued...\n"
    },
    {
      "path": "cli/Cargo.toml",
      "content": "[package]\nname = \"xcontext-cli\"\nversion = \"0.1.0\"\nedition = \"2024\"                           # Assuming 2024 edition, adjust if needed\ndescription = \"CLI interface for xcontext\"\nlicense = \"GPL-3.0-or-later\"\npublish = false\n\n[[bin]]\nname = \"xcontext\"\npath = \"cli.rs\"\n\n[dependencies]\n# Workspace dependencies\nanyhow = { workspace = true }\nlog = { workspace = true }\nclap = { workspace = true }\ncolored = { workspace = true }\nenv_logger = { workspace = true }\ntoml = { workspace = true }       # Added\npathdiff = { workspace = true }   # Added\nrayon = { workspace = true }      # Added\nignore = { workspace = true }     # Added\n\n# Local Core Crate\nxcontext-core = { path = \"../core\" }\n\n# CLI specific dependencies (from workspace or direct)\nclap_complete = { workspace = true }\nclearscreen = { workspace = true }\ncomfy-table = { workspace = true }\ndirs = { workspace = true }\nnotify = { workspace = true }\nnotify-debouncer-mini = { workspace = true }\nserde = { workspace = true }\nserde_json = { workspace = true }\nserde_yml = { workspace = true }\nquick-xml = { workspace = true }\ntiktoken-rs = { workspace = true }\nbyte-unit = { workspace = true }\n\n# Direct dependencies (if not in workspace)\nglob = \"0.3\" # Added: needed for quick command (specify version)\n"
    },
    {
      "path": "cli/cli.rs",
      "content": "mod cli_args;\nmod commands;\nmod output;\nmod watch;\n\nuse anyhow::{Context, Result};\nuse clap::{CommandFactory, Parser};\nuse colored::*;\nuse log;\nuse std::process;\n// Removed unused Arc import\n\n// Corrected import: Added GenerateArgs\nuse cli_args::{Cli, Commands, FormatOutputOpts, GenerateArgs, ProjectConfigOpts};\nuse xcontext_core::{AppError, Config}; // Use Config from core crate\n\nfn main() {\n    let cli_args = Cli::parse();\n\n    setup_logging(cli_args.quiet, cli_args.verbose);\n\n    let quiet = cli_args.quiet;\n    let verbose = cli_args.verbose;\n\n    log::debug!(\"CLI args parsed: {:?}\", cli_args);\n\n    let exit_code = match run_app(cli_args, quiet, verbose) {\n        Ok(_) => {\n            log::info!(\"Application finished successfully.\");\n            0\n        }\n        Err(e) => {\n            let core_err = e.downcast_ref::<xcontext_core::AppError>();\n            let exit_code = match core_err {\n                // Define exit codes based on core errors if needed\n                Some(AppError::Config(_)) => 1,\n                Some(AppError::TomlParse(_)) => 1,\n                Some(AppError::TomlSerialize(_)) => 1,\n                Some(AppError::Io(_)) => 2,\n                Some(AppError::FileRead { .. }) => 2,\n                Some(AppError::FileWrite { .. }) => 2,\n                Some(AppError::DirCreation { .. }) => 2,\n                Some(AppError::WalkDir(_)) => 2,\n                Some(AppError::Ignore(_)) => 2,\n                Some(AppError::RuleLoading(_)) => 2,\n                Some(AppError::Glob(_)) => 2,\n                Some(AppError::Chunking(_)) => 3,\n                Some(AppError::SystemInfo(_)) => 4,\n                Some(AppError::InvalidArgument(_)) => 5,\n                // Add mapping for AppError::ClapUsage if you move clap errors to core\n                Some(AppError::JsonSerialize(_)) => 6,\n                Some(AppError::YamlError(_)) => 6,\n                Some(AppError::XmlSerialize(_)) => 6,\n                Some(AppError::TikToken(_)) => 8,\n                // Add mapping for AppError::WatchError if moved back to core\n                Some(AppError::DataLoading(_)) => 1, // Treat data loading like config error\n                Some(AppError::DurationParse(_)) => 5, // Treat like invalid arg\n                // Corrected: Added wildcard arm for non-exhaustive AppError\n                Some(_) => 1, // Default exit code for other *core* AppErrors\n                None => 1,    // Default exit code for other *anyhow* errors\n            };\n\n            // Only print error if not quiet, or if it's a critical config/clap error\n            // This prevents noisy error messages for things handled by core logging\n            if !quiet || exit_code == 1 || exit_code == 5 {\n                eprintln!(\"{} {:#}\\\\n\", \"Error:\".red().bold(), e);\n            } else {\n                // Log even if quiet for critical errors\n                log::error!(\"Application failed: {:#}\", e);\n            }\n\n            exit_code\n        }\n    };\n    log::debug!(\"Exiting with code {}\", exit_code);\n    process::exit(exit_code);\n}\n\nfn setup_logging(quiet: bool, verbose: u8) {\n    let log_level = if quiet {\n        log::LevelFilter::Off // Turn off logging completely if quiet\n    } else {\n        match verbose {\n            0 => log::LevelFilter::Warn,  // Default: Show warnings and errors\n            1 => log::LevelFilter::Info,  // -v: Show info, warnings, errors\n            2 => log::LevelFilter::Debug, // -vv: Show debug, info, warnings, errors\n            _ => log::LevelFilter::Trace, // -vvv+: Show all levels\n        }\n    };\n    env_logger::Builder::new()\n        .filter_level(log_level)\n        .format_timestamp(None) // Keep logs clean\n        .init();\n    log::trace!(\"Logger initialized with level: {:?}\", log_level);\n}\n\nfn run_app(cli: Cli, quiet: bool, verbose: u8) -> Result<()> {\n    match cli.command {\n        None => {\n            Cli::command().print_help()?;\n        }\n        Some(command) => {\n            match command {\n                Commands::Cl => {\n                    log::debug!(\"Executing 'cl' command...\");\n                    clearscreen::clear().context(\"Failed to clear screen\")?;\n                    log::debug!(\"Screen cleared.\");\n                }\n                Commands::Completion(args) => {\n                    log::debug!(\"Executing 'completion' command...\");\n                    commands::completion::handle_completion_command(&args, quiet)?;\n                }\n                Commands::Config(args) => {\n                    log::debug!(\"Executing 'config' command...\");\n                    let temp_opts = ProjectConfigOpts::default();\n                    let project_root =\n                        Config::determine_project_root(temp_opts.project_root.as_ref())\n                            .context(\"Failed to determine project root for config command\")?;\n                    commands::config::handle_config_command(&args, &project_root, quiet)?;\n                }\n                Commands::Mcp(_args) => {\n                    log::warn!(\"Executing dummy 'mcp' command...\");\n                    eprintln!(\"MCP command not implemented yet.\");\n                }\n                Commands::Generate(args) => {\n                    log::debug!(\"Executing 'generate' command...\");\n                    commands::generate::handle_generate_command(args, quiet, verbose)?;\n                }\n                Commands::Watch(args) => {\n                    log::debug!(\"Executing 'watch' command...\");\n                    // run_watch_mode now takes args directly\n                    watch::run_watch_mode(args, quiet, verbose)?;\n                }\n                Commands::Show(args) => {\n                    log::debug!(\"Executing 'show' command...\");\n                    commands::show::handle_show_command(args, quiet, verbose)?;\n                }\n                Commands::Metrics(args) => {\n                    log::debug!(\"Executing 'metrics' command...\");\n                    commands::metrics::handle_metrics_command(args, quiet)?;\n                }\n                Commands::Debug(args) => {\n                    log::debug!(\"Executing 'debug' command...\");\n                    commands::debug::handle_debug_command(args, quiet, verbose)?;\n                }\n                Commands::Quick(args) => {\n                    log::debug!(\"Executing 'quick' command...\");\n                    commands::quick::handle_quick_command(args, quiet, verbose)?;\n                }\n            }\n        }\n    }\n    Ok(())\n}\n\n// Kept this function as it seems used by load_config_for_command\nfn merge_config_with_cli_overrides(mut config: Config, args: &GenerateArgs) -> Config {\n    log::trace!(\"Applying generate command CLI overrides to config...\");\n\n    if let Some(name) = &args.project_config.project_name {\n        config.general.project_name = Some(name.clone());\n    }\n\n    // Output Format Overrides\n    if let Some(format) = &args.format_output.format {\n        config.output.format = format.clone();\n    }\n    // Apply JSON minify logic based on flags and format\n    config.output.json_minify = if config.output.format == \"json\" {\n        !args.format_output.disable_json_minify // default to true (minify) unless disable flag is set\n    } else {\n        true // Default irrelevant for non-JSON, keep consistent default\n    };\n    // Apply XML pretty print logic based on flags and format\n    config.output.xml_pretty_print = if config.output.format == \"xml\" {\n        args.format_output.enable_xml_pretty // default to false (compact) unless enable flag is set\n    } else {\n        false // Default irrelevant for non-XML\n    };\n\n    // Exclusion Overrides\n    if args.exclusion.exclude_project_name {\n        config.output.include_project_name = false;\n    }\n    if args.exclusion.exclude_project_root {\n        config.output.include_project_root = false;\n    }\n    if args.exclusion.exclude_timestamp {\n        config.output.include_timestamp = false;\n    }\n    if args.exclusion.exclude_system_info {\n        config.output.include_system_info = false;\n    }\n\n    // Section Toggle Overrides\n    if args.section_toggles.disable_tree {\n        config.tree.enabled = false;\n    }\n    if args.section_toggles.enable_tree {\n        config.tree.enabled = true;\n    }\n    if args.section_toggles.disable_source {\n        config.source.enabled = false;\n    }\n    if args.section_toggles.enable_source {\n        config.source.enabled = true;\n    }\n    if args.section_toggles.disable_meta {\n        config.meta.enabled = false;\n    }\n    if args.section_toggles.enable_meta {\n        config.meta.enabled = true;\n    }\n    if args.section_toggles.disable_rules {\n        config.rules.enabled = false;\n    }\n    if args.section_toggles.enable_rules {\n        config.rules.enabled = true;\n    }\n    if args.section_toggles.disable_docs {\n        config.docs.enabled = false;\n    }\n    if args.section_toggles.enable_docs {\n        config.docs.enabled = true;\n    }\n\n    // Ignore Toggle Overrides\n    if args.ignore_toggles.disable_gitignore {\n        config.general.use_gitignore = false;\n    }\n    if args.ignore_toggles.enable_gitignore {\n        config.general.use_gitignore = true;\n    }\n    if args.ignore_toggles.disable_builtin_ignore {\n        config.general.enable_builtin_ignore = false;\n    }\n    if args.ignore_toggles.enable_builtin_ignore {\n        config.general.enable_builtin_ignore = true;\n    }\n\n    // Filter Overrides\n    if !args.filters.tree_include.is_empty() {\n        config.tree.include = Some(args.filters.tree_include.clone());\n    }\n    if !args.filters.tree_exclude.is_empty() {\n        config.tree.exclude = Some(args.filters.tree_exclude.clone());\n    }\n    if !args.filters.source_include.is_empty() {\n        config.source.include = Some(args.filters.source_include.clone());\n    }\n    if !args.filters.source_exclude.is_empty() {\n        config.source.exclude = Some(args.filters.source_exclude.clone());\n    }\n    if !args.filters.docs_include.is_empty() {\n        config.docs.include = Some(args.filters.docs_include.clone());\n    }\n    if !args.filters.docs_exclude.is_empty() {\n        config.docs.exclude = Some(args.filters.docs_exclude.clone());\n    }\n\n    // Meta Override\n    if !args.meta_override.add_meta.is_empty() {\n        log::trace!(\"Applying meta overrides: {:?}\", args.meta_override.add_meta);\n        config.meta.enabled = true; // Ensure meta section is enabled if adding via CLI\n        for (key, value) in &args.meta_override.add_meta {\n            config.meta.custom_meta.insert(key.clone(), value.clone());\n        }\n    }\n\n    // Watch specific args are handled separately if needed (e.g., in watch command)\n    log::trace!(\"Config after CLI overrides: {:?}\", config);\n    config\n}\n\n// Helper function to load config considering CLI options\n// Kept public as it's used by multiple command modules\npub fn load_config_for_command(\n    project_root: &std::path::Path,\n    project_opts: &ProjectConfigOpts,\n    // Pass specific args structs for commands that can override config parts\n    generate_args: Option<&cli_args::GenerateArgs>,\n    watch_args: Option<&cli_args::WatchArgs>,\n    format_override: Option<&FormatOutputOpts>, // For commands like show, metrics, debug, quick\n) -> Result<Config> {\n    let config_path = Config::resolve_config_path(\n        project_root,\n        project_opts.context_file.as_ref(),\n        project_opts.disable_context_file,\n    )\n    .context(\"Failed to resolve configuration path\")?;\n\n    let mut config = match &config_path {\n        Some(path) => Config::load_from_path(path)\n            .with_context(|| format!(\"Failed to load config from {}\", path.display()))?,\n        None => Config::default(),\n    };\n\n    // Apply overrides from GenerateArgs if provided\n    if let Some(gen_args) = generate_args {\n        config = merge_config_with_cli_overrides(config, gen_args);\n    } else {\n        // Apply overrides common to other commands if needed\n        if let Some(name) = &project_opts.project_name {\n            config.general.project_name = Some(name.clone());\n        }\n        // Apply format overrides if present\n        if let Some(fmt_opts) = format_override {\n            if let Some(format) = &fmt_opts.format {\n                config.output.format = format.clone();\n            }\n            config.output.json_minify = if config.output.format == \"json\" {\n                !fmt_opts.disable_json_minify\n            } else {\n                true\n            };\n            config.output.xml_pretty_print = if config.output.format == \"xml\" {\n                fmt_opts.enable_xml_pretty\n            } else {\n                false\n            };\n        }\n        // Apply watch-specific overrides if present\n        if let Some(w_args) = watch_args {\n            if let Some(delay) = &w_args.watch_delay {\n                config.watch.delay = delay.clone();\n            }\n            // Note: watch also uses format_override logic handled above if needed\n        }\n    }\n\n    // Ensure project name is set (fallback to directory name)\n    config.general.project_name = Some(config.get_effective_project_name(project_root));\n\n    Ok(config)\n}\n"
    },
    {
      "path": "cli/cli_args.rs",
      "content": "use clap::{Args, Parser, Subcommand};\nuse std::path::PathBuf;\n\n#[derive(Args, Debug, Clone, Default)]\npub struct ProjectConfigOpts {\n    #[arg(\n        long,\n        help = \"Specify the target project directory (default: current dir).\",\n        help_heading = \"Project Setup\",\n        value_name = \"PATH\"\n    )]\n    pub project_root: Option<PathBuf>,\n\n    #[arg(\n        long,\n        help = \"Specify path/filename of the TOML config file (default: .xtools/xcontext/xcontext.toml).\",\n        value_name = \"CONTEXT_FILE\",\n        conflicts_with = \"disable_context_file\",\n        help_heading = \"Project Setup\"\n    )]\n    pub context_file: Option<String>,\n\n    #[arg(\n        long,\n        help = \"Disable loading any TOML config file.\",\n        conflicts_with = \"context_file\",\n        help_heading = \"Project Setup\"\n    )]\n    pub disable_context_file: bool,\n\n    #[arg(\n        long,\n        help = \"Specify the project name (overrides config/dir name).\",\n        value_name = \"NAME\",\n        help_heading = \"Project Setup\"\n    )]\n    pub project_name: Option<String>,\n}\n\n#[derive(Args, Debug, Clone, Default)]\npub struct FormatOutputOpts {\n    #[arg(short = 'f', long, help = \"Set the output format.\", value_name = \"FORMAT\", value_parser = [\"json\", \"yaml\", \"xml\"], help_heading = \"Output Formatting\")]\n    pub format: Option<String>,\n\n    #[arg(\n        long,\n        help = \"Ensure JSON output is compact (minified) [default].\",\n        conflicts_with = \"disable_json_minify\",\n        help_heading = \"Output Formatting\"\n    )]\n    pub enable_json_minify: bool,\n\n    #[arg(\n        long,\n        help = \"Ensure JSON output is pretty-printed (readable).\",\n        conflicts_with = \"enable_json_minify\",\n        help_heading = \"Output Formatting\"\n    )]\n    pub disable_json_minify: bool,\n\n    #[arg(\n        long,\n        help = \"Ensure XML output is pretty-printed (readable).\",\n        conflicts_with = \"disable_xml_pretty\",\n        help_heading = \"Output Formatting\"\n    )]\n    pub enable_xml_pretty: bool,\n\n    #[arg(\n        long,\n        help = \"Ensure XML output is compact [default].\",\n        conflicts_with = \"enable_xml_pretty\",\n        help_heading = \"Output Formatting\"\n    )]\n    pub disable_xml_pretty: bool,\n}\n\n#[derive(Parser, Debug)]\n#[command(\n    author,\n    version,\n    about = \"Generate structured project context for AI models.\",\n    long_about = \"xcontext scans project files based on configuration and generates context \\n(metadata, system info, structure, code, docs, rules) suitable for AI processing. \\nSupports multiple output formats, filtering, and utility modes.\",\n    help_template = \"{about-section}\\nUsage: {usage}\\n\\n{all-args}{after-help}\",\n    after_help = \"EXAMPLES:\\n  xcontext generate -f yaml --save ./output\\n  xcontext show rules -f json\\n  xcontext metrics\\n  xcontext watch -s\",\n    arg_required_else_help = true\n)]\npub struct Cli {\n    #[command(subcommand)]\n    pub command: Option<Commands>,\n\n    #[arg(short, long, action = clap::ArgAction::Count, global = true, help = \"Increase message verbosity (-v, -vv).\")]\n    pub verbose: u8,\n\n    #[arg(\n        short,\n        long,\n        global = true,\n        help = \"Silence informational messages and warnings.\"\n    )]\n    pub quiet: bool,\n}\n\n#[derive(Subcommand, Debug, Clone)]\npub enum Commands {\n    #[command(\n        visible_alias = \"g\",\n        visible_alias = \"gen\",\n        about = \"Generate the full project context.\"\n    )]\n    Generate(GenerateArgs),\n\n    #[command(\n        visible_alias = \"w\",\n        about = \"Monitor project files and regenerate context automatically.\"\n    )]\n    Watch(WatchArgs),\n\n    #[command(\n        visible_alias = \"s\",\n        about = \"Show specific configured items (metadata, prompts, rules).\"\n    )]\n    Show(ShowArgs),\n\n    #[command(\n        visible_alias = \"m\",\n        about = \"Calculate and display project statistics.\"\n    )]\n    Metrics(MetricsArgs),\n\n    #[command(\n        visible_alias = \"d\",\n        about = \"Show effective configuration and planned file inclusions.\"\n    )]\n    Debug(DebugArgs),\n\n    #[command(\n        visible_alias = \"q\",\n        about = \"Quickly extract content of files matching a pattern.\"\n    )]\n    Quick(QuickArgs),\n\n    #[command(about = \"Generate or save shell completion scripts.\")]\n    Completion(CompletionArgs),\n\n    #[command(about = \"Show or save the default configuration file structure.\")]\n    Config(ConfigArgs),\n\n    #[command(visible_alias = \"c\", about = \"Clear the terminal screen.\")]\n    Cl,\n\n    #[command(about = \"Dummy MCP command (placeholder).\")]\n    Mcp(McpArgs),\n}\n\n#[derive(Args, Debug, Clone)]\npub struct GenerateArgs {\n    #[clap(flatten)]\n    pub project_config: ProjectConfigOpts,\n    #[clap(flatten)]\n    pub format_output: FormatOutputOpts,\n\n    #[arg(\n        long,\n        help = \"Force output of the main context to standard output.\",\n        help_heading = \"Output Control\",\n        conflicts_with = \"save\"\n    )]\n    pub stdout: bool,\n\n    #[arg(\n        short = 's', long, value_name = \"SAVE_DIR\",\n        num_args = 0..=1,\n        help_heading = \"Output Control\",\n        help = \"Save context. Optional SAVE_DIR overrides config/default logic.\",\n    )]\n    pub save: Option<Option<PathBuf>>,\n\n    #[arg(\n        short = 'c',\n        long,\n        help = \"Split source content into chunks (e.g., '5MB', '1024kb'). Requires JSON format.\",\n        value_name = \"SIZE_STRING\",\n        help_heading = \"Output Control\"\n    )]\n    pub chunks: Option<String>,\n\n    #[clap(flatten)]\n    pub exclusion: ExclusionGroup,\n    #[clap(flatten)]\n    pub section_toggles: SectionTogglesGroup,\n    #[clap(flatten)]\n    pub ignore_toggles: IgnoreTogglesGroup,\n    #[clap(flatten)]\n    pub filters: FilterGroup,\n    #[clap(flatten)]\n    pub meta_override: MetaOverrideGroup,\n}\n\n#[derive(Args, Debug, Clone)]\npub struct WatchArgs {\n    #[clap(flatten)]\n    pub project_config: ProjectConfigOpts,\n    #[clap(flatten)]\n    pub format_output: FormatOutputOpts,\n\n    #[arg(\n        long,\n        value_name = \"DELAY_STRING\",\n        help = \"Set debounce delay for watch mode [default: 300ms]\"\n    )]\n    pub watch_delay: Option<String>,\n\n    #[arg( short = 's', long, value_name = \"SAVE_DIR\", num_args = 0..=1, help = \"Save context on change. Optional SAVE_DIR overrides config/default logic.\", )]\n    pub save: Option<Option<PathBuf>>,\n}\n\n#[derive(Args, Debug, Clone)]\npub struct ShowArgs {\n    #[clap(flatten)]\n    pub project_config: ProjectConfigOpts,\n    #[clap(flatten)]\n    pub format_output: FormatOutputOpts,\n    #[command(subcommand)]\n    pub item: ShowItem,\n}\n\n#[derive(Subcommand, Debug, Clone)]\npub enum ShowItem {\n    #[command(about = \"Show specific metadata key or list available keys.\")]\n    Meta { key: Option<String> },\n    #[command(about = \"Show content of all metadata keys (default: pretty text).\")]\n    Metas {},\n    #[command(about = \"Show specific prompt or list available prompt names.\")]\n    Prompt { name: Option<String> },\n    #[command(about = \"Show content of all prompts (default: pretty text).\")]\n    Prompts {},\n    #[command(about = \"Show specific rule set/list or list available names.\")]\n    Rule { name: Option<String> },\n    #[command(about = \"Show content of all rule sets/lists (default: pretty text).\")]\n    Rules {},\n}\n\n#[derive(Args, Debug, Clone)]\npub struct MetricsArgs {\n    #[clap(flatten)]\n    pub project_config: ProjectConfigOpts,\n    #[clap(flatten)]\n    pub format_output: FormatOutputOpts,\n}\n\n#[derive(Args, Debug, Clone)]\npub struct DebugArgs {\n    #[clap(flatten)]\n    pub project_config: ProjectConfigOpts,\n    #[clap(flatten)]\n    pub format_output: FormatOutputOpts,\n}\n\n#[derive(Args, Debug, Clone)]\npub struct QuickArgs {\n    #[clap(flatten)]\n    pub project_config: ProjectConfigOpts,\n    #[clap(flatten)]\n    pub format_output: FormatOutputOpts,\n    #[arg(\n        required = true,\n        help = \"Glob pattern (e.g., 'src/**/*.rs', 'data/', 'file.txt')\"\n    )]\n    pub pattern: String,\n}\n\n#[derive(Args, Debug, Clone)]\npub struct CompletionArgs {\n    #[arg(\n        long,\n        value_name = \"SHELL\",\n        help = \"Shell to generate completions for (fish, bash, zsh) [default: fish]\"\n    )]\n    pub shell: Option<String>,\n    #[arg(\n        long,\n        help = \"Save completion script to default location (prompts overwrite).\"\n    )]\n    pub save: bool,\n}\n\n#[derive(Args, Debug, Clone)]\npub struct ConfigArgs {\n    #[arg(\n        long,\n        help = \"Save default config structure to default path (prompts overwrite).\"\n    )]\n    pub save: bool,\n}\n\n#[derive(Args, Debug, Clone)]\npub struct McpArgs {}\n\n#[derive(Args, Debug, Clone, Default)]\npub struct ExclusionGroup {\n    #[arg(\n        long,\n        help = \"Omit 'project_name' field from output.\",\n        help_heading = \"Core Exclusions\"\n    )]\n    pub exclude_project_name: bool,\n    #[arg(\n        long,\n        help = \"Omit 'project_root' field from output.\",\n        help_heading = \"Core Exclusions\"\n    )]\n    pub exclude_project_root: bool,\n    #[arg(\n        long,\n        help = \"Omit 'generation_timestamp' field from output.\",\n        help_heading = \"Core Exclusions\"\n    )]\n    pub exclude_timestamp: bool,\n    #[arg(\n        long,\n        help = \"Omit 'system_info' field from output.\",\n        help_heading = \"Core Exclusions\"\n    )]\n    pub exclude_system_info: bool,\n}\n\n#[derive(Args, Debug, Clone, Default)]\npub struct SectionTogglesGroup {\n    #[arg(\n        long,\n        help = \"Force inclusion of the 'tree' section [default: enabled].\",\n        overrides_with = \"disable_tree\",\n        help_heading = \"Section Toggles\"\n    )]\n    pub enable_tree: bool,\n    #[arg(\n        long,\n        help = \"Disable the 'tree' section.\",\n        overrides_with = \"enable_tree\",\n        help_heading = \"Section Toggles\"\n    )]\n    pub disable_tree: bool,\n\n    #[arg(\n        long,\n        help = \"Force inclusion of the 'source' section [default: enabled].\",\n        overrides_with = \"disable_source\",\n        help_heading = \"Section Toggles\"\n    )]\n    pub enable_source: bool,\n    #[arg(\n        long,\n        help = \"Disable the 'source' section.\",\n        overrides_with = \"enable_source\",\n        help_heading = \"Section Toggles\"\n    )]\n    pub disable_source: bool,\n\n    #[arg(\n        long,\n        help = \"Force inclusion of the 'meta' section [default: enabled].\",\n        overrides_with = \"disable_meta\",\n        help_heading = \"Section Toggles\"\n    )]\n    pub enable_meta: bool,\n    #[arg(\n        long,\n        help = \"Disable the 'meta' section.\",\n        overrides_with = \"enable_meta\",\n        help_heading = \"Section Toggles\"\n    )]\n    pub disable_meta: bool,\n\n    #[arg(\n        long,\n        help = \"Force inclusion of the 'rules' section [default: enabled].\",\n        overrides_with = \"disable_rules\",\n        help_heading = \"Section Toggles\"\n    )]\n    pub enable_rules: bool,\n    #[arg(\n        long,\n        help = \"Disable the 'rules' section.\",\n        overrides_with = \"enable_rules\",\n        help_heading = \"Section Toggles\"\n    )]\n    pub disable_rules: bool,\n\n    #[arg(\n        long,\n        help = \"Force inclusion of the 'docs' section [default: enabled].\",\n        overrides_with = \"disable_docs\",\n        help_heading = \"Section Toggles\"\n    )]\n    pub enable_docs: bool,\n    #[arg(\n        long,\n        help = \"Disable the 'docs' section.\",\n        overrides_with = \"enable_docs\",\n        help_heading = \"Section Toggles\"\n    )]\n    pub disable_docs: bool,\n}\n\n#[derive(Args, Debug, Clone, Default)]\npub struct IgnoreTogglesGroup {\n    #[arg(\n        long,\n        help = \"Globally enable respecting .gitignore files [default: enabled].\",\n        overrides_with = \"disable_gitignore\",\n        help_heading = \"Ignore Rules\"\n    )]\n    pub enable_gitignore: bool,\n    #[arg(\n        long,\n        help = \"Globally disable respecting .gitignore files.\",\n        overrides_with = \"enable_gitignore\",\n        help_heading = \"Ignore Rules\"\n    )]\n    pub disable_gitignore: bool,\n\n    #[arg(\n        long,\n        help = \"Enable default built-in ignores (e.g., *.lock, target/) [default: enabled].\",\n        overrides_with = \"disable_builtin_ignore\",\n        help_heading = \"Ignore Rules\"\n    )]\n    pub enable_builtin_ignore: bool,\n    #[arg(\n        long,\n        help = \"Disable default built-in ignores.\",\n        overrides_with = \"enable_builtin_ignore\",\n        help_heading = \"Ignore Rules\"\n    )]\n    pub disable_builtin_ignore: bool,\n}\n\n#[derive(Args, Debug, Clone, Default)]\npub struct FilterGroup {\n    #[arg(long = \"tree-include\", value_name = \"PATTERN\", action = clap::ArgAction::Append, help = \"Add include path/glob pattern for tree view.\", help_heading = \"Content Filtering\")]\n    pub tree_include: Vec<String>,\n    #[arg(long = \"tree-exclude\", value_name = \"PATTERN\", action = clap::ArgAction::Append, help = \"Add exclude path/glob pattern for tree view.\", help_heading = \"Content Filtering\")]\n    pub tree_exclude: Vec<String>,\n\n    #[arg(long = \"source-include\", value_name = \"PATTERN\", action = clap::ArgAction::Append, help = \"Add include path/glob pattern for source files.\", help_heading = \"Content Filtering\")]\n    pub source_include: Vec<String>,\n    #[arg(long = \"source-exclude\", value_name = \"PATTERN\", action = clap::ArgAction::Append, help = \"Add exclude path/glob pattern for source files.\", help_heading = \"Content Filtering\")]\n    pub source_exclude: Vec<String>,\n\n    #[arg(long = \"docs-include\", value_name = \"PATTERN\", action = clap::ArgAction::Append, help = \"Add include path/glob pattern for documentation files.\", help_heading = \"Content Filtering\")]\n    pub docs_include: Vec<String>,\n    #[arg(long = \"docs-exclude\", value_name = \"PATTERN\", action = clap::ArgAction::Append, help = \"Add exclude path/glob pattern for documentation files.\", help_heading = \"Content Filtering\")]\n    pub docs_exclude: Vec<String>,\n}\n\n#[derive(Args, Debug, Clone, Default)]\npub struct MetaOverrideGroup {\n    #[arg(long = \"add-meta\", value_name = \"key=value\", value_parser = parse_key_val, action = clap::ArgAction::Append, help = \"Add/override key=value pairs in the 'meta' section.\", help_heading = \"Metadata Override\")]\n    pub add_meta: Vec<(String, String)>,\n}\n\nfn parse_key_val(s: &str) -> std::result::Result<(String, String), String> {\n    s.find('=')\n        .map(|idx| {\n            let key = s[..idx].trim().to_string();\n            let value = s[idx + 1..].trim().to_string();\n            if key.is_empty() {\n                Err(\"Metadata key cannot be empty\".to_string())\n            } else {\n                Ok((key, value))\n            }\n        })\n        .ok_or_else(|| \"Invalid KEY=VALUE format for --add-meta\".to_string())?\n}\n"
    },
    {
      "path": "cli/commands/completion.rs",
      "content": "use anyhow::{Context, Result};\nuse clap::CommandFactory;\nuse clap_complete::{Shell, generate};\nuse colored::*;\nuse std::fs::{self, File};\nuse std::io::{self, Write};\nuse xcontext_core::AppError; // Use core error for specific cases if needed\n\nuse crate::cli_args::{Cli, CompletionArgs};\n\npub fn handle_completion_command(args: &CompletionArgs, quiet: bool) -> Result<()> {\n    let shell_str = args.shell.as_deref().unwrap_or(\"fish\");\n    let save_output = args.save;\n\n    let shell_enum: Shell = match shell_str.to_lowercase().as_str() {\n        \"fish\" => Shell::Fish,\n        \"bash\" => Shell::Bash,\n        \"zsh\" => Shell::Zsh,\n        _ => {\n            anyhow::bail!(AppError::InvalidArgument(format!(\n                // Use anyhow::bail! for CLI errors\n                \"Unsupported shell for completion: {}\",\n                shell_str\n            )));\n        }\n    };\n\n    let mut command = Cli::command();\n    let bin_name = command.get_name().to_string();\n\n    if !save_output {\n        generate(shell_enum, &mut command, bin_name, &mut io::stdout());\n    } else {\n        let save_dir_res = match shell_enum {\n            Shell::Fish => dirs::config_dir().map(|p| p.join(\"fish\").join(\"completions\")),\n            Shell::Bash => dirs::config_dir().map(|p| p.join(\"bash_completion.d\")), // Common location\n            Shell::Zsh => dirs::data_local_dir().map(|p| p.join(\"zsh\").join(\"site-functions\")),\n            _ => anyhow::bail!(AppError::InvalidArgument(format!(\n                \"Default save location not known for shell: {}\",\n                shell_str\n            ))),\n        };\n\n        let save_dir = save_dir_res\n            .ok_or_else(|| anyhow::anyhow!(\"Could not determine standard completion directory.\"))?;\n\n        let filename = match shell_enum {\n            Shell::Fish => format!(\"{}.fish\", bin_name),\n            Shell::Bash => format!(\"{}.bash\", bin_name), // Or just bin_name\n            Shell::Zsh => format!(\"_{}\", bin_name),\n            _ => unreachable!(),\n        };\n        let save_path = save_dir.join(&filename); // Use reference\n\n        if save_path.exists() {\n            if !quiet {\n                print!(\n                    \"{} Completion file already exists at '{}'. Overwrite? [{}/{}] \",\n                    \"⚠️\".yellow(),\n                    save_path.display().to_string().cyan(),\n                    \"y\".green(),\n                    \"N\".red()\n                );\n                io::stdout().flush().context(\"Failed to flush stdout\")?;\n                let mut response = String::new();\n                io::stdin()\n                    .read_line(&mut response)\n                    .context(\"Failed to read user input\")?;\n                if !response.trim().eq_ignore_ascii_case(\"y\") {\n                    println!(\"Save cancelled.\");\n                    return Ok(());\n                }\n            } else {\n                anyhow::bail!(\n                    \"Target file '{}' exists. Overwrite prevented in quiet mode.\",\n                    save_path.display()\n                );\n            }\n        }\n\n        fs::create_dir_all(&save_dir)\n            .with_context(|| format!(\"Failed to create directory {}\", save_dir.display()))?;\n        let mut file = File::create(&save_path)\n            .with_context(|| format!(\"Failed to create file {}\", save_path.display()))?;\n        generate(shell_enum, &mut command, bin_name, &mut file);\n\n        if !quiet {\n            println!(\n                \"{} {} completions saved to: {}\",\n                \"✅\".green(),\n                shell_str.cyan(),\n                save_path.display().to_string().blue()\n            );\n        }\n    }\n    Ok(())\n}\n"
    },
    {
      "path": "cli/commands/config.rs",
      "content": "use crate::cli_args::ConfigArgs;\nuse crate::output::print_data_or_text; // Use unified output helper\nuse anyhow::{Context, Result};\nuse colored::*;\nuse std::fs;\nuse std::io::{self, Write};\nuse std::path::Path;\nuse toml; // Added use\nuse xcontext_core::{\n    Config,\n    config::{DEFAULT_CONFIG_DIR, DEFAULT_CONFIG_FILENAME}, // Use core constants\n};\n\npub fn handle_config_command(args: &ConfigArgs, project_root: &Path, quiet: bool) -> Result<()> {\n    let determined_name = project_root\n        .file_name()\n        .map(|n| n.to_string_lossy().to_string())\n        .unwrap_or_else(|| \"UnknownProject\".to_string());\n\n    let mut config_to_show_or_save = Config::default();\n    config_to_show_or_save.general.project_name = Some(determined_name);\n\n    if args.save {\n        let default_config_path = project_root\n            .join(DEFAULT_CONFIG_DIR)\n            .join(DEFAULT_CONFIG_FILENAME);\n        save_config_to_path(&config_to_show_or_save, &default_config_path, quiet)\n    } else {\n        // Serialize default config to TOML string for printing\n        let toml_string = toml::to_string_pretty(&config_to_show_or_save) // Added use toml\n            .context(\"Failed to serialize default config to TOML\")?;\n        // Use print_data_or_text (though it defaults to JSON/YAML/XML, text fallback works)\n        print_data_or_text(\n            &config_to_show_or_save,                       // Pass data even if using text\n            Some(toml_string),                             // Provide the TOML string as plain text\n            &crate::cli_args::FormatOutputOpts::default(), // Use default format options\n            \"text\",                                        // Default to text for this command\n            \"Config\",                                      // XML root name if XML were used\n        )\n    }\n}\n\nfn save_config_to_path(config: &Config, path: &Path, quiet: bool) -> Result<()> {\n    if path.exists() {\n        if quiet {\n            anyhow::bail!(\n                \"Target file '{}' exists. Overwrite prevented in quiet mode.\",\n                path.display()\n            );\n        }\n        print!(\n            \"{} Config file exists at '{}'. Overwrite? [{}/{}] \",\n            \"⚠️\".yellow(),\n            path.display().to_string().cyan(),\n            \"y\".green(),\n            \"N\".red()\n        );\n        io::stdout().flush().context(\"Failed to flush stdout\")?;\n        let mut response = String::new();\n        io::stdin()\n            .read_line(&mut response)\n            .context(\"Failed to read user input\")?;\n        if !response.trim().eq_ignore_ascii_case(\"y\") {\n            println!(\"Save cancelled.\");\n            return Ok(());\n        }\n    }\n\n    if let Some(parent) = path.parent() {\n        fs::create_dir_all(parent)\n            .with_context(|| format!(\"Failed to create directory {}\", parent.display()))?;\n    }\n\n    let header = format!(\n        \"{}\\\\n{}\\\\n\\\\n\", // Add extra newline\n        \"# Default xcontext configuration\", \"# Generated by 'xcontext config --save'\"\n    );\n    let toml_string =\n        toml::to_string_pretty(config).context(\"Failed to serialize default config to TOML\")?; // Added use toml\n    let final_content = format!(\"{}{}\", header, toml_string);\n\n    fs::write(path, final_content)\n        .with_context(|| format!(\"Failed to write config file {}\", path.display()))?;\n\n    if !quiet {\n        println!(\n            \"{} Configuration saved successfully to: {}\",\n            \"✅\".green(),\n            path.display().to_string().blue()\n        );\n    }\n    Ok(())\n}\n"
    },
    {
      "path": "cli/commands/debug.rs",
      "content": "use crate::cli_args::DebugArgs; // Removed unused FormatOutputOpts\nuse crate::load_config_for_command;\nuse crate::output::print_data_or_text;\nuse anyhow::{Context, Result};\nuse colored::*;\nuse log;\nuse pathdiff; // Added use\nuse serde::Serialize;\nuse std::collections::BTreeMap;\nuse std::path::Path;\nuse toml; // Added use\nuse xcontext_core::{self as core, Config, FileInfo, ResolvedRules}; // Removed unused 'config' import alias\n\n#[derive(Debug, Serialize)]\nstruct DebugInfo<'a> {\n    effective_config: &'a Config,\n    source_files_to_include: Vec<String>,\n    docs_files_to_include: Vec<String>,\n    tree_elements_to_include: &'a [(String, bool)], // path, is_dir\n    resolved_rules: &'a ResolvedRules,\n}\n\npub fn handle_debug_command(args: DebugArgs, quiet: bool, _verbose: u8) -> Result<()> {\n    // Marked verbose as unused\n    let project_root = Config::determine_project_root(args.project_config.project_root.as_ref())\n        .context(\"Failed to determine project root\")?;\n    log::info!(\"Project root determined: {}\", project_root.display());\n\n    let config = load_config_for_command(\n        &project_root,\n        &args.project_config,\n        None,\n        None,\n        Some(&args.format_output), // Pass format override options\n    )\n    .context(\"Failed to load configuration for debug command\")?;\n\n    log::debug!(\"Debug: Gathering file lists...\");\n    let (source_files, docs_files, tree_path_types) =\n        core::gather_files_and_tree(&project_root, &config, quiet)\n            .context(\"Failed to gather file lists for debug\")?;\n    log::debug!(\"Debug: File lists gathered.\");\n\n    log::debug!(\"Debug: Detecting project characteristics...\");\n    let project_characteristics = core::detect_project_characteristics(&project_root)\n        .context(\"Failed to detect project characteristics for debug\")?;\n    log::debug!(\"Debug: Characteristics detected.\");\n\n    log::debug!(\"Debug: Resolving rules...\");\n    let resolved_rules =\n        core::config::resolve_rules(&config.rules, &project_root, &project_characteristics)\n            .context(\"Failed to resolve rules for debug\")?;\n    log::debug!(\"Debug: Rules resolved.\");\n\n    let debug_data = DebugInfo {\n        effective_config: &config,\n        source_files_to_include: get_relative_paths(&source_files, &project_root),\n        docs_files_to_include: get_relative_paths(&docs_files, &project_root),\n        tree_elements_to_include: &tree_path_types,\n        resolved_rules: &resolved_rules,\n    };\n\n    if args.format_output.format.is_none() {\n        log::debug!(\"Debug: Printing pretty output...\");\n        print_debug_info_pretty(&debug_data, &project_root)?;\n        log::debug!(\"Debug: Pretty output complete.\");\n    } else {\n        log::debug!(\n            \"Debug: Printing structured output (format: {:?})...\",\n            args.format_output.format\n        );\n        // Pass None for plain_text, rely on structured output\n        print_data_or_text(&debug_data, None, &args.format_output, \"json\", \"DebugInfo\")?;\n        log::debug!(\"Debug: Structured output complete.\");\n    }\n    Ok(())\n}\n\nfn get_relative_paths(files: &[FileInfo], project_root: &Path) -> Vec<String> {\n    files\n        .iter()\n        .map(|f| {\n            pathdiff::diff_paths(&f.path, project_root) // Added use pathdiff\n                .map(|p| p.to_string_lossy().to_string())\n                .unwrap_or_else(|| f.path.to_string_lossy().to_string()) // Fallback\n        })\n        .collect()\n}\n\nfn print_debug_info_pretty(debug_info: &DebugInfo, _project_root: &Path) -> Result<()> {\n    println!(\n        \"{}\",\n        \"\\n--- Effective Configuration ---\"\n            .green()\n            .bold()\n            .underline()\n    );\n    let config_toml = toml::to_string_pretty(debug_info.effective_config) // Added use toml\n        .context(\"Failed to serialize effective config to TOML\")?;\n    println!(\"{}\", config_toml);\n\n    print_path_list(\"Source Files Included\", &debug_info.source_files_to_include);\n    print_path_list(\"Docs Files Included\", &debug_info.docs_files_to_include);\n\n    println!(\n        \"{}\",\n        \"\\n--- Tree Elements Included ---\"\n            .green()\n            .bold()\n            .underline()\n    );\n    if debug_info.tree_elements_to_include.is_empty() {\n        println!(\"{}\", \"(None)\".dimmed());\n    } else {\n        // Sort tree elements for consistent output\n        let mut sorted_tree = debug_info.tree_elements_to_include.to_vec();\n        sorted_tree.sort();\n        for (p, is_dir) in sorted_tree {\n            let suffix = if is_dir {\n                \" (dir)\".dimmed()\n            } else {\n                \"\".normal()\n            };\n            println!(\"- {}{}\", p.cyan(), suffix);\n        }\n    }\n\n    display_debug_rules(debug_info.resolved_rules);\n\n    println!(\"{}\", \"\\n--- End Debug Info ---\".green().bold());\n    Ok(())\n}\n\nfn print_path_list(title: &str, paths: &[String]) {\n    println!(\n        \"{}\",\n        format!(\"\\n--- {} ---\", title).green().bold().underline()\n    );\n    if paths.is_empty() {\n        println!(\"{}\", \"(None)\".dimmed());\n    } else {\n        // Assume paths are already sorted from gather step if needed, or sort here\n        let mut sorted_paths = paths.to_vec();\n        sorted_paths.sort();\n        sorted_paths.iter().for_each(|p| println!(\"- {}\", p.cyan()));\n    }\n}\n\nfn display_debug_rules(resolved_rules: &ResolvedRules) {\n    println!(\"{}\", \"\\n--- Resolved Rules ---\".green().bold().underline());\n    if resolved_rules.rulesets.is_empty() {\n        println!(\"{}\", \"(No rules enabled or resolved)\".dimmed());\n        return;\n    }\n    println!(\n        \"{:<35} {:<18} {:<10}\",\n        \"Ruleset Key\".bold(),\n        \"Origin\".bold(),\n        \"Rule Count\".bold()\n    );\n    println!(\"{:-<65}\", \"\"); // Separator line\n\n    // Sort rules by key for consistent output\n    let sorted_rules: BTreeMap<_, _> = resolved_rules.rulesets.iter().collect();\n\n    for (key, rules_list) in sorted_rules {\n        let origin_str = resolved_rules\n            .origins\n            .get(key)\n            .map_or(\"unknown\", |s| s.as_str());\n        let origin_colored = match origin_str {\n            \"default\" | \"default+include\" => origin_str.cyan(),\n            \"dynamic\" => origin_str.magenta(),\n            \"include\" => origin_str.green(),\n            \"import\" => origin_str.yellow(),\n            \"custom\" => origin_str.blue(),\n            _ => origin_str.dimmed(),\n        };\n        println!(\n            \"{:<35} {:<24} {:<10}\", // Adjust spacing if needed\n            key.blue(),\n            origin_colored,\n            rules_list.len()\n        );\n    }\n}\n"
    },
    {
      "path": "cli/commands/generate.rs",
      "content": "use crate::cli_args::GenerateArgs; // Removed unused WatchArgs\nuse crate::load_config_for_command;\nuse crate::output;\nuse anyhow::{Context, Result};\nuse colored::Colorize;\nuse log;\nuse std::fs; // Added use std::fs\nuse std::path::{Path, PathBuf};\nuse std::sync::Arc;\nuse xcontext_core::{self as core, Config, ProjectContext}; // Use core types\n\npub fn handle_generate_command(args: GenerateArgs, quiet: bool, verbose: u8) -> Result<()> {\n    let project_root = Config::determine_project_root(args.project_config.project_root.as_ref())\n        .context(\"Failed to determine project root\")?;\n    log::info!(\"Project root determined: {}\", project_root.display());\n\n    let config = Arc::new(\n        load_config_for_command(\n            &project_root,\n            &args.project_config,\n            Some(&args), // Pass generate args for overrides\n            None,\n            None, // Format handled within load_config_for_command via generate_args\n        )\n        .context(\"Failed to load configuration\")?,\n    );\n\n    // Create OutputTargetArgs from GenerateArgs\n    let output_target_args = OutputTargetArgs {\n        save: &args.save,\n        chunks: &args.chunks,\n        stdout: args.stdout,\n        format_output: &args.format_output,\n    };\n\n    // Use trigger_generation which handles the core logic + output\n    trigger_generation(&project_root, &config, &output_target_args, quiet, verbose) // Pass correct type\n}\n\n// This function now encapsulates the core generation logic\n// It's called by both `generate` and `watch` commands\n// Made public so watch.rs can use it\npub fn trigger_generation(\n    project_root: &Path,\n    config: &Arc<Config>,\n    output_target_args: &OutputTargetArgs, // Now expects this type\n    quiet: bool,\n    verbose: u8,\n) -> Result<()> {\n    log::info!(\n        \"Starting context generation for: {}\",\n        project_root.display()\n    );\n\n    validate_args_for_generation(config, output_target_args)?;\n\n    log::debug!(\"Gathering files and tree elements...\");\n    let (source_files, docs_files, tree_path_types) =\n        core::gather_files_and_tree(project_root, config, quiet)\n            .context(\"Failed to gather project files and directory structure\")?;\n    log::debug!(\n        \"Gathering complete. Found {} source, {} docs, {} tree elements.\",\n        source_files.len(),\n        docs_files.len(),\n        tree_path_types.len()\n    );\n\n    let tree_for_context: Option<Vec<core::TreeNode>> = if config.tree.enabled {\n        log::debug!(\"Building tree structure...\");\n        let tree = core::gather::build_tree_from_paths(&tree_path_types)\n            .context(\"Failed to build directory tree structure\")?;\n        log::debug!(\"Tree structure built.\");\n        Some(tree)\n    } else {\n        log::debug!(\"Tree structure disabled in config.\");\n        None\n    };\n\n    log::debug!(\"Detecting project characteristics...\");\n    let project_characteristics = core::detect_project_characteristics(project_root)\n        .context(\"Failed to detect project characteristics\")?;\n    log::debug!(\"Characteristics detected: {:?}\", project_characteristics);\n\n    log::debug!(\"Building initial project context (including rule resolution)...\");\n    let mut main_context = ProjectContext::build(\n        project_root,\n        config,\n        tree_for_context,\n        &project_characteristics,\n    )\n    .context(\"Failed to build initial project context\")?;\n    log::debug!(\"Initial context built.\");\n\n    // Add docs if enabled\n    main_context = main_context.add_docs(docs_files, project_root, config);\n\n    // Handle source files (inline or chunking)\n    if config.source.enabled {\n        log::debug!(\"Processing source files...\");\n        if let Some(chunk_size_str) = output_target_args.chunks.as_deref() {\n            log::info!(\"Chunking source files with size: {}\", chunk_size_str);\n\n            let (save_dir, filename_base, _) =\n                get_save_details_from_args(config, output_target_args.save.as_ref(), project_root);\n\n            let chunk_files_data =\n                core::chunking::split_files_into_chunks(source_files, chunk_size_str, project_root)\n                    .context(\"Failed to split files into chunks\")?;\n\n            let mut chunk_file_paths = Vec::<PathBuf>::new();\n            if !chunk_files_data.is_empty() {\n                fs::create_dir_all(&save_dir).with_context(|| {\n                    // Added std::fs import\n                    format!(\n                        \"Failed to create chunk output directory {}\",\n                        save_dir.display()\n                    )\n                })?;\n            }\n\n            for (i, chunk_data) in chunk_files_data.iter().enumerate() {\n                let chunk_num = i + 1;\n                let chunk_filename = format!(\"{}_chunk_{}.json\", filename_base, chunk_num); // Chunks always JSON\n                let chunk_path = save_dir.join(&chunk_filename);\n                // Use output::save_chunk_file, passing format_opts from output_target_args\n                output::save_chunk_file(\n                    chunk_data,\n                    &chunk_path,\n                    &output_target_args.format_output,\n                    quiet,\n                )?;\n                chunk_file_paths.push(chunk_path);\n            }\n\n            main_context = main_context.add_chunk_paths(chunk_file_paths, &save_dir, config);\n            log::info!(\"Chunking processing complete.\");\n\n            // Output the main context file (without sources, just chunk refs) if saving is requested\n            if output_target_args.save.is_some() {\n                let (main_save_dir, main_filename_base, main_extension) =\n                    get_save_details_from_args(\n                        config,\n                        output_target_args.save.as_ref(),\n                        project_root,\n                    );\n                let main_filename = format!(\"{}.{}\", main_filename_base, main_extension);\n                let main_output_path = main_save_dir.join(main_filename);\n                log::info!(\n                    \"Saving main context (with chunk references) to file: {}\",\n                    main_output_path.display()\n                );\n                output::print_context_or_save(\n                    &main_context,\n                    config,\n                    Some(&main_output_path),\n                    &output_target_args.format_output,\n                    quiet,\n                )?;\n            } else if output_target_args.stdout {\n                // If stdout is forced even with chunking, print the main context (with chunk refs)\n                log::info!(\"Outputting main context (with chunk references) to stdout...\");\n                output::print_context_or_save(\n                    &main_context,\n                    config,\n                    None,\n                    &output_target_args.format_output,\n                    quiet,\n                )?;\n            } else if !quiet {\n                // Print confirmation about chunks being saved if not outputting main context elsewhere\n                println!(\n                    \"{} Source content chunked and saved in: {}\",\n                    \"✅\".green(),                          // Added Colorize import\n                    save_dir.display().to_string().blue()  // Added Colorize import\n                );\n            }\n        } else {\n            log::debug!(\"Adding source files inline...\");\n            main_context = main_context.add_files(source_files, project_root, config);\n            // Output main context (with inline sources)\n            handle_final_output(\n                &main_context,\n                config,\n                output_target_args,\n                project_root,\n                quiet,\n            )?;\n        }\n    } else {\n        log::debug!(\"Source section disabled.\");\n        if !source_files.is_empty() && !quiet && verbose > 0 {\n            eprintln!(\n                \"{}\",\n                \"Warning: Source section disabled, but source files were found and ignored.\"\n                    .yellow() // Added Colorize import\n            );\n        }\n        // Output main context (without any source section)\n        handle_final_output(\n            &main_context,\n            config,\n            output_target_args,\n            project_root,\n            quiet,\n        )?;\n    }\n\n    Ok(())\n}\n\n// Define a helper struct to pass output-related args cleanly\n// Made public so watch.rs can use it\npub struct OutputTargetArgs<'a> {\n    pub save: &'a Option<Option<PathBuf>>,\n    pub chunks: &'a Option<String>,\n    pub stdout: bool,\n    pub format_output: &'a crate::cli_args::FormatOutputOpts,\n}\n\n// Helper to get save details from OutputTargetArgs\nfn get_save_details_from_args(\n    config: &Config,\n    cli_save_opt: Option<&Option<PathBuf>>,\n    project_root: &Path,\n) -> (PathBuf, String, String) {\n    let save_dir_base = match cli_save_opt {\n        Some(Some(cli_path)) => {\n            log::trace!(\n                \"Save directory explicitly provided via CLI: {}\",\n                cli_path.display()\n            );\n            cli_path.clone()\n        }\n        Some(None) => {\n            log::trace!(\n                \"Save flag used without path, using configured/default save directory: {}\",\n                config.save.output_dir.display()\n            );\n            config.save.output_dir.clone()\n        }\n        None => {\n            log::trace!(\n                \"Save flag not used, using configured/default save directory for potential chunks: {}\",\n                config.save.output_dir.display()\n            );\n            config.save.output_dir.clone() // Default needed if chunking without -s\n        }\n    };\n\n    let save_dir = if save_dir_base.is_absolute() {\n        save_dir_base\n    } else {\n        project_root.join(save_dir_base)\n    };\n    log::trace!(\"Resolved absolute save directory: {}\", save_dir.display());\n\n    let name_options: [Option<&str>; 3] = [\n        config.save.filename_base.as_deref(),\n        config.general.project_name.as_deref(),\n        project_root.file_name().and_then(|n| n.to_str()),\n    ];\n    let filename_base_str = name_options\n        .into_iter()\n        .flatten()\n        .next()\n        .unwrap_or(\"context\"); // Fallback base name\n    let filename_base = filename_base_str.to_string();\n    log::trace!(\"Using filename base: {}\", filename_base);\n\n    let extension = config.save.extension.as_deref().unwrap_or_else(|| {\n        match config.output.format.to_lowercase().as_str() {\n            \"yaml\" | \"yml\" => \"yaml\",\n            \"xml\" => \"xml\",\n            _ => \"json\",\n        }\n    });\n    log::trace!(\"Using save extension: {}\", extension);\n\n    (save_dir, filename_base, extension.to_string())\n}\n\nfn handle_final_output(\n    main_context: &ProjectContext,\n    config: &Config,\n    output_target_args: &OutputTargetArgs,\n    project_root: &Path,\n    quiet: bool,\n) -> Result<()> {\n    log::debug!(\"Determining final output target...\");\n    let mut output_target_path: Option<PathBuf> = None;\n    let needs_saving_to_disk = output_target_args.save.is_some();\n\n    if needs_saving_to_disk {\n        let (save_dir, filename_base, extension) =\n            get_save_details_from_args(config, output_target_args.save.as_ref(), project_root);\n        let main_filename = format!(\"{}.{}\", filename_base, extension);\n        output_target_path = Some(save_dir.join(main_filename));\n        log::debug!(\n            \"Output target path set to file: {}\",\n            output_target_path.as_ref().unwrap().display()\n        );\n    } else if output_target_args.stdout {\n        log::debug!(\"Output target set to stdout (forced).\");\n    } else {\n        log::debug!(\"Output target set to stdout (default).\");\n    }\n\n    output::print_context_or_save(\n        main_context,\n        config,\n        output_target_path.as_deref(),\n        &output_target_args.format_output,\n        quiet,\n    )\n}\n\nfn validate_args_for_generation(config: &Config, args: &OutputTargetArgs) -> Result<()> {\n    if args.chunks.is_some() {\n        if !config.source.enabled {\n            anyhow::bail!(core::AppError::InvalidArgument(\n                \"Chunking (-c) cannot be used when source file inclusion ([source].enabled=false) is disabled\".to_string()\n            ));\n        }\n        let format = args\n            .format_output\n            .format\n            .as_deref()\n            .unwrap_or(&config.output.format);\n        if format.to_lowercase() != \"json\" {\n            anyhow::bail!(core::AppError::Chunking(\n                \"Chunking requires the output format to be 'json'. Use '-f json'.\".to_string()\n            ));\n        }\n        // Chunking implies saving, so stdout without save doesn't make sense unless explicitly handled\n        if args.stdout && args.save.is_none() {\n            anyhow::bail!(core::AppError::InvalidArgument(\n                 \"--stdout cannot be used with --chunks unless --save is also specified to define the main context output location.\".to_string()\n             ));\n        }\n    }\n    Ok(())\n}\n"
    },
    {
      "path": "cli/commands/metrics.rs",
      "content": "use crate::cli_args::MetricsArgs;\nuse crate::load_config_for_command;\nuse crate::output::{print_data_or_text, print_metrics_pretty_table};\nuse anyhow::{Context, Result};\nuse byte_unit::{Byte, UnitType};\nuse log;\nuse pathdiff; // Added use\nuse serde::Serialize;\nuse std::path::Path;\nuse tiktoken_rs::cl100k_base;\nuse xcontext_core::{self as core, Config, FileInfo}; // Use core types\n\n#[derive(Debug, Serialize)]\npub struct ProjectMetrics {\n    pub total_files: usize,\n    pub total_lines: usize,\n    pub total_bytes: u128,\n    pub total_bytes_readable: String,\n    pub estimated_tokens: usize,\n    pub files_details: Vec<FileMetrics>,\n}\n\n#[derive(Debug, Serialize)]\npub struct FileMetrics {\n    pub path: String,\n    pub lines: usize,\n    pub bytes: usize,\n    pub bytes_readable: String,\n    pub estimated_tokens: usize,\n}\n\npub fn handle_metrics_command(args: MetricsArgs, quiet: bool) -> Result<()> {\n    let project_root = Config::determine_project_root(args.project_config.project_root.as_ref())\n        .context(\"Failed to determine project root\")?;\n    log::info!(\"Project root determined: {}\", project_root.display());\n\n    let config = load_config_for_command(\n        &project_root,\n        &args.project_config,\n        None,\n        None,\n        Some(&args.format_output), // Pass format override options\n    )\n    .context(\"Failed to load configuration for metrics command\")?;\n\n    log::debug!(\"Gathering files for metrics...\");\n    let (source_files, docs_files, _) = core::gather_files_and_tree(&project_root, &config, quiet)\n        .context(\"Failed to gather files for metrics calculation\")?;\n    log::debug!(\"Files gathered.\");\n\n    let combined_files: Vec<&FileInfo> = source_files.iter().chain(docs_files.iter()).collect();\n\n    if combined_files.is_empty() && !quiet {\n        println!(\"No source or documentation files found to calculate metrics.\");\n        return Ok(()); // Exit gracefully if no files\n    }\n\n    log::debug!(\"Calculating metrics...\");\n    let metrics = calculate_metrics(&combined_files, &project_root)?;\n    log::debug!(\"Metrics calculation complete.\");\n\n    if args.format_output.format.is_none() {\n        print_metrics_pretty_table(&metrics)\n    } else {\n        // Pass None for plain_text, rely on structured output\n        print_data_or_text(\n            &metrics,\n            None,\n            &args.format_output,\n            \"json\",\n            \"ProjectMetrics\",\n        )\n    }\n}\n\nfn calculate_metrics(files: &[&FileInfo], project_root: &Path) -> Result<ProjectMetrics> {\n    let bpe =\n        cl100k_base().map_err(|e| anyhow::anyhow!(core::AppError::TikToken(e.to_string())))?;\n    let mut total_files = 0;\n    let mut total_lines = 0;\n    let mut total_bytes: u128 = 0;\n    let mut total_tokens = 0;\n    let mut files_details = Vec::new();\n\n    for file_info in files {\n        if file_info.size == 0 {\n            continue;\n        } // Skip empty files\n\n        let lines = file_info.content.lines().count();\n        let bytes = file_info.size;\n        // Estimate tokens in parallel? Might be overkill unless content is huge\n        let tokens = bpe.encode_ordinary(&file_info.content).len();\n\n        let relative_path = pathdiff::diff_paths(&file_info.path, project_root) // Added use pathdiff\n            .unwrap_or_else(|| file_info.path.clone())\n            .to_string_lossy()\n            .to_string();\n\n        total_files += 1;\n        total_lines += lines;\n        total_bytes = total_bytes.saturating_add(bytes as u128);\n        total_tokens += tokens;\n\n        let file_byte = Byte::from_u128(bytes as u128).unwrap_or_default();\n        let file_size_readable = file_byte.get_appropriate_unit(UnitType::Binary).to_string();\n\n        files_details.push(FileMetrics {\n            path: relative_path,\n            lines,\n            bytes,\n            bytes_readable: file_size_readable,\n            estimated_tokens: tokens,\n        });\n    }\n\n    // Sort by path for consistent output\n    files_details.sort_by(|a, b| a.path.cmp(&b.path));\n\n    let total_byte = Byte::from_u128(total_bytes).unwrap_or_default();\n    let total_size_readable = total_byte\n        .get_appropriate_unit(UnitType::Binary)\n        .to_string();\n\n    Ok(ProjectMetrics {\n        total_files,\n        total_lines,\n        total_bytes,\n        total_bytes_readable: total_size_readable,\n        estimated_tokens: total_tokens,\n        files_details,\n    })\n}\n"
    },
    {
      "path": "cli/commands/quick.rs",
      "content": "use crate::cli_args::QuickArgs; // Removed unused FormatOutputOpts\nuse crate::load_config_for_command;\nuse crate::output::print_data_or_text;\nuse anyhow::{Context, Result};\nuse colored::*;\nuse glob::Pattern;\nuse ignore::{WalkBuilder, WalkState};\nuse log;\nuse rayon::iter::{IntoParallelRefIterator, ParallelIterator};\n// Removed: use rayon::prelude::*;\nuse serde::Serialize;\nuse std::collections::HashMap;\nuse std::fs;\nuse std::path::PathBuf; // Removed unused Path import\nuse std::sync::mpsc;\nuse xcontext_core::Config;\n\n#[derive(Debug, Serialize)]\nstruct QuickOutput {\n    files: HashMap<String, String>,\n}\n\npub fn handle_quick_command(args: QuickArgs, quiet: bool, verbose: u8) -> Result<()> {\n    let project_root = Config::determine_project_root(args.project_config.project_root.as_ref())\n        .context(\"Failed to determine project root\")?;\n    log::info!(\"Project root determined: {}\", project_root.display());\n\n    // Load config primarily to respect ignore rules (.gitignore, built-in)\n    let config = load_config_for_command(\n        &project_root,\n        &args.project_config,\n        None,\n        None,\n        Some(&args.format_output), // Pass format override options\n    )\n    .context(\"Failed to load configuration for quick command\")?;\n\n    let mut pattern_to_use = args.pattern.clone();\n    let potential_path = project_root.join(&args.pattern);\n    let mut info_msg = None;\n\n    // Check if the pattern looks like a directory and adjust glob\n    if potential_path.is_dir() {\n        pattern_to_use = format!(\n            \"{}**/*\",\n            args.pattern.trim_end_matches(&['/', '\\\\'] as &[char])\n        );\n        info_msg = Some(format!(\n            \"{} Interpreting directory input '{}' as glob '{}'\",\n            \"ℹ️\".blue(),\n            args.pattern,\n            pattern_to_use\n        ));\n    } else if args.pattern.ends_with(&['/', '\\\\'] as &[char]) {\n        // If it ends with slash but isn't a dir, warn and use modified pattern\n        if !quiet {\n            eprintln!(\n                \"{} Directory pattern '{}' matches no existing directory, using pattern without trailing slash.\",\n                \"⚠️\".yellow(),\n                args.pattern\n            );\n        }\n        pattern_to_use = args\n            .pattern\n            .trim_end_matches(&['/', '\\\\'] as &[char])\n            .to_string();\n    }\n\n    if let Some(msg) = info_msg {\n        if !quiet && verbose > 0 {\n            eprintln!(\"{}\", msg);\n        }\n    }\n\n    let glob_pattern = Pattern::new(&pattern_to_use).with_context(|| {\n        format!(\n            \"Invalid glob pattern for quick: '{}' (processed as '{}')\",\n            args.pattern, pattern_to_use\n        )\n    })?;\n\n    let use_gitignore = config.general.use_gitignore;\n    let _enable_builtin_ignore = config.general.enable_builtin_ignore; // TODO: Apply built-in ignores too?\n\n    let mut builder = WalkBuilder::new(&project_root);\n    builder.hidden(false); // Include hidden files unless filtered by ignores\n    builder.ignore(use_gitignore);\n    builder.git_ignore(use_gitignore);\n    builder.git_exclude(use_gitignore);\n    builder.require_git(false);\n    // TODO: Add logic to apply built-in ignores here if desired for `quick`\n\n    let walker = builder.build_parallel();\n    let (tx_path, rx_path) = mpsc::channel::<PathBuf>();\n    let glob_pattern_outer_clone = glob_pattern.clone(); // Clone for closure\n    let proj_root_clone = project_root.clone(); // Clone for closure\n\n    log::debug!(\n        \"Starting parallel walk for pattern: {}\",\n        glob_pattern.as_str()\n    );\n    walker.run(move || {\n        // tx_path is MOVED here\n        let tx = tx_path.clone(); // Clone the moved sender for the inner closure\n        let proj_root_inner = proj_root_clone.clone();\n        let glob_pattern_inner_clone = glob_pattern_outer_clone.clone();\n\n        Box::new(move |entry_result| {\n            if let Ok(entry) = entry_result {\n                if entry.file_type().map_or(false, |ft| ft.is_file()) {\n                    if let Some(relative_path) =\n                        pathdiff::diff_paths(entry.path(), &proj_root_inner)\n                    {\n                        if glob_pattern_inner_clone.matches_path(&relative_path) {\n                            log::trace!(\"Matched file: {}\", relative_path.display());\n                            // Send using the cloned sender for this thread\n                            let _ = tx.send(entry.path().to_path_buf());\n                        }\n                    } else if glob_pattern_inner_clone.matches_path(entry.path()) {\n                        log::trace!(\"Matched absolute path: {}\", entry.path().display());\n                        let _ = tx.send(entry.path().to_path_buf());\n                    }\n                }\n            }\n            WalkState::Continue\n        })\n    }); // The original tx_path (owned by the closure) goes out of scope here\n\n    // Removed: drop(tx_path); // No longer needed, and tx_path was moved\n\n    let paths_to_read: Vec<_> = rx_path.into_iter().collect(); // This will finish when all senders (clones) are dropped\n    log::info!(\n        \"Found {} files matching pattern. Reading content...\",\n        paths_to_read.len()\n    );\n\n    let results: Vec<Result<(String, String)>> = paths_to_read\n        .par_iter()\n        .map(|path| {\n            let content = fs::read_to_string(path)\n                .with_context(|| format!(\"Failed to read file {}\", path.display()))?;\n            let relative_path = pathdiff::diff_paths(path, &project_root)\n                .unwrap_or_else(|| path.clone())\n                .to_string_lossy()\n                .to_string();\n            Ok((relative_path, content))\n        })\n        .collect();\n\n    let mut files_map = HashMap::new();\n    let mut read_errors = Vec::new();\n\n    for result in results {\n        match result {\n            Ok((path_str, content)) => {\n                files_map.insert(path_str, content);\n            }\n            Err(e) => {\n                read_errors.push(format!(\"{:#}\", e));\n            }\n        }\n    }\n\n    if !read_errors.is_empty() && !quiet {\n        eprintln!(\n            \"{}\",\n            \"Warning: Errors encountered during file reading:\".yellow()\n        );\n        for err_msg in read_errors {\n            eprintln!(\" - {}\", err_msg);\n        }\n        eprintln!(\"---\");\n    }\n\n    if files_map.is_empty() && !quiet {\n        println!(\"No files matched the pattern '{}'.\", args.pattern);\n        return Ok(());\n    }\n\n    let output_data = QuickOutput { files: files_map };\n\n    let default_format = \"json\";\n    print_data_or_text(\n        &output_data,\n        None,\n        &args.format_output,\n        default_format,\n        \"QuickOutput\",\n    )\n}\n"
    },
    {
      "path": "cli/commands/show.rs",
      "content": "use crate::cli_args::{FormatOutputOpts, ShowArgs}; // Removed unused Cli, ProjectConfigOpts, ShowItem\nuse crate::load_config_for_command;\nuse crate::output::print_data_or_text; // Use CLI output helpers\nuse anyhow::{Context, Result};\nuse colored::*;\nuse log; // Corrected: On its own line\nuse serde::Serialize; // Needed for ShowOutputWrapper\nuse std::collections::{BTreeMap, HashMap};\nuse std::path::Path;\nuse xcontext_core::{self as core, Config, ResolvedRules}; // Removed unused config import alias\n\n#[derive(Serialize)]\nstruct ShowOutputWrapper<T: Serialize> {\n    value: T,\n}\n\npub fn handle_show_command(args: ShowArgs, quiet: bool, verbose: u8) -> Result<()> {\n    let project_root = Config::determine_project_root(args.project_config.project_root.as_ref())\n        .context(\"Failed to determine project root\")?;\n    log::info!(\"Project root determined: {}\", project_root.display());\n\n    let config = load_config_for_command(\n        &project_root,\n        &args.project_config,\n        None,\n        None,\n        Some(&args.format_output), // Pass format override options\n    )\n    .context(\"Failed to load configuration for show command\")?;\n\n    // Use the full path from cli_args here\n    match &args.item {\n        crate::cli_args::ShowItem::Meta { key } => {\n            handle_show_meta_singular(&config, key.as_deref(), &args.format_output, quiet, verbose)\n        }\n        crate::cli_args::ShowItem::Metas {} => {\n            handle_show_meta_plural(&config, &args.format_output, quiet, verbose)\n        }\n        crate::cli_args::ShowItem::Prompt { name } => handle_show_prompt_singular(\n            &config,\n            name.as_deref(),\n            &args.format_output,\n            quiet,\n            verbose,\n        ),\n        crate::cli_args::ShowItem::Prompts {} => {\n            handle_show_prompt_plural(&config, &args.format_output, quiet, verbose)\n        }\n        crate::cli_args::ShowItem::Rule { name } => handle_show_rule_singular(\n            &config,\n            name.as_deref(),\n            &project_root, // Pass project root for rule resolution\n            &args.format_output,\n            quiet,\n            verbose,\n        ),\n        crate::cli_args::ShowItem::Rules {} => handle_show_rule_plural(\n            &config,\n            &project_root, // Pass project root for rule resolution\n            &args.format_output,\n            quiet,\n            verbose,\n        ),\n    }\n}\n\nfn list_available_keys(map: &HashMap<String, String>, item_type: &str, _quiet: bool) {\n    // Always print key listing to stderr\n    eprintln!(\"\\nAvailable {} keys:\", item_type.bold());\n    if map.is_empty() {\n        eprintln!(\"  {}\", \"(None available)\".dimmed());\n        return;\n    }\n    let mut sorted_keys: Vec<_> = map.keys().collect();\n    sorted_keys.sort();\n    for key in sorted_keys {\n        eprintln!(\"  - {}\", key.blue());\n    }\n}\n\nfn list_available_rule_keys(resolved_rules: &ResolvedRules, _quiet: bool) {\n    // Always print key listing to stderr\n    eprintln!(\"\\nAvailable {} keys:\", \"Rule Set\".yellow().bold());\n    if resolved_rules.rulesets.is_empty() {\n        eprintln!(\n            \"  {}\",\n            \"(None available based on current config and project content)\".dimmed()\n        );\n        return;\n    }\n    let mut sorted_keys: Vec<_> = resolved_rules.rulesets.keys().cloned().collect();\n    sorted_keys.sort();\n    for key in sorted_keys {\n        let origin = resolved_rules.origins.get(&key).map_or(\"?\", |s| s.as_str());\n        let origin_colored = match origin {\n            \"default\" | \"default+include\" => origin.cyan(),\n            \"dynamic\" => origin.magenta(),\n            \"include\" => origin.green(),\n            \"import\" => origin.yellow(),\n            \"custom\" => origin.blue(),\n            _ => origin.dimmed(),\n        };\n        eprintln!(\"  - {} {}\", key.blue(), format!(\"({})\", origin_colored));\n    }\n}\n\nfn handle_show_meta_singular(\n    config: &Config,\n    key: Option<&str>,\n    format_opts: &FormatOutputOpts,\n    quiet: bool,\n    _verbose: u8,\n) -> Result<()> {\n    if !config.meta.enabled {\n        if !quiet {\n            eprintln!(\n                \"{}\",\n                \"Warning: Meta section is disabled in config.\".yellow()\n            );\n        }\n        return Ok(());\n    }\n    let meta_data = &config.meta.custom_meta;\n    if let Some(k) = key {\n        if let Some(value) = meta_data.get(k) {\n            let wrapper = ShowOutputWrapper { value };\n            print_data_or_text(\n                &wrapper,\n                Some(value.clone()),\n                format_opts,\n                \"text\",\n                \"MetaValue\",\n            )\n        } else {\n            eprintln!(\n                \"{} Metadata key \\\"{}\\\" not found.\",\n                \"Error:\".red(),\n                k.blue()\n            );\n            list_available_keys(meta_data, \"metadata\", quiet);\n            anyhow::bail!(\"Metadata key not found\") // Use anyhow::bail!\n        }\n    } else {\n        if !quiet {\n            eprintln!(\n                \"{}\",\n                \"Please specify a metadata key to show, or use 'show metas' to list all.\".yellow()\n            );\n        }\n        list_available_keys(meta_data, \"metadata\", quiet);\n        Ok(())\n    }\n}\n\nfn handle_show_meta_plural(\n    config: &Config,\n    format_opts: &FormatOutputOpts,\n    quiet: bool,\n    _verbose: u8,\n) -> Result<()> {\n    if !config.meta.enabled {\n        if !quiet {\n            eprintln!(\n                \"{}\",\n                \"Warning: Meta section is disabled in config.\".yellow()\n            );\n        }\n        return Ok(());\n    }\n    let meta_data = &config.meta.custom_meta;\n    if meta_data.is_empty() {\n        if !quiet {\n            println!(\"No custom metadata defined.\");\n        }\n        return Ok(());\n    }\n\n    let sorted_meta: BTreeMap<_, _> = meta_data.iter().collect();\n\n    let pretty_text = if format_opts.format.is_none() {\n        let mut output = String::new();\n        output.push_str(&format!(\n            \"{}\\n\",\n            \"\\n--- Custom Metadata ---\".green().bold().underline()\n        ));\n        for (k, v) in &sorted_meta {\n            output.push_str(&format!(\"  {:<25} : {}\\n\", k.blue(), v));\n        }\n        Some(output)\n    } else {\n        None\n    };\n\n    print_data_or_text(&sorted_meta, pretty_text, format_opts, \"text\", \"Metadata\")\n}\n\nfn handle_show_prompt_singular(\n    config: &Config,\n    name: Option<&str>,\n    format_opts: &FormatOutputOpts,\n    quiet: bool,\n    _verbose: u8,\n) -> Result<()> {\n    let merged_prompts = core::config::resolve_prompts(&config.prompts, Path::new(\".\")) // Pass dummy path, not needed for static/custom\n        .context(\"Failed to resolve prompts\")?;\n\n    if let Some(n) = name {\n        let key_to_find = if n.contains(':') {\n            n.to_string()\n        } else {\n            let static_key = format!(\"static:{}\", n);\n            let custom_key = format!(\"custom:{}\", n);\n            let imported_key = format!(\"imported:{}\", n); // Check imported too\n            if merged_prompts.contains_key(&static_key) {\n                static_key\n            } else if merged_prompts.contains_key(&custom_key) {\n                custom_key\n            } else if merged_prompts.contains_key(&imported_key) {\n                imported_key\n            } else {\n                n.to_string()\n            } // Fallback to original name if prefix missing\n        };\n\n        if let Some(text) = merged_prompts.get(&key_to_find) {\n            let wrapper = ShowOutputWrapper { value: text };\n            print_data_or_text(\n                &wrapper,\n                Some(text.trim().to_string()),\n                format_opts,\n                \"text\",\n                \"PromptText\",\n            )\n        } else {\n            eprintln!(\"{} Prompt name \\\"{}\\\" not found.\", \"Error:\".red(), n.blue());\n            list_available_keys(&merged_prompts, \"prompt\", quiet);\n            anyhow::bail!(\"Prompt name not found\")\n        }\n    } else {\n        if !quiet {\n            eprintln!(\n                \"{}\",\n                \"Please specify a prompt name to show, or use 'show prompts' to show all.\".yellow()\n            );\n        }\n        list_available_keys(&merged_prompts, \"prompt\", quiet);\n        Ok(())\n    }\n}\n\nfn handle_show_prompt_plural(\n    config: &Config,\n    format_opts: &FormatOutputOpts,\n    quiet: bool,\n    _verbose: u8,\n) -> Result<()> {\n    let merged_prompts = core::config::resolve_prompts(&config.prompts, Path::new(\".\")) // Pass dummy path\n        .context(\"Failed to resolve prompts\")?;\n\n    if merged_prompts.is_empty() {\n        if !quiet {\n            println!(\"No prompts available (static, custom, or imported).\");\n        }\n        return Ok(());\n    }\n\n    let sorted_prompts: BTreeMap<_, _> = merged_prompts.iter().collect();\n\n    let pretty_text = if format_opts.format.is_none() {\n        let mut output = String::new();\n        output.push_str(&format!(\n            \"{}\\n\",\n            \"\\n--- Available Prompts ---\".green().bold().underline()\n        ));\n        for (key, text) in &sorted_prompts {\n            output.push_str(&format!(\"\\n▶ {}:\\n\", key.blue().bold()));\n            for line in text.trim().lines() {\n                output.push_str(&format!(\"    {}\\n\", line));\n            }\n        }\n        Some(output)\n    } else {\n        None\n    };\n\n    print_data_or_text(&sorted_prompts, pretty_text, format_opts, \"text\", \"Prompts\")\n}\n\nfn handle_show_rule_singular(\n    config: &Config,\n    name_with_optional_prefix: Option<&str>,\n    project_root: &Path, // Need project root to resolve rules\n    format_opts: &FormatOutputOpts,\n    quiet: bool,\n    _verbose: u8,\n) -> Result<()> {\n    if !config.rules.enabled {\n        if !quiet {\n            eprintln!(\n                \"{}\",\n                \"Warning: Rules section is disabled in config.\".yellow()\n            );\n        }\n        return Ok(());\n    }\n\n    let project_characteristics = core::detect_project_characteristics(project_root)\n        .context(\"Failed to detect project characteristics for rule resolution\")?;\n    let resolved =\n        core::config::resolve_rules(&config.rules, project_root, &project_characteristics)\n            .context(\"Failed to resolve rules\")?;\n\n    if let Some(name) = name_with_optional_prefix {\n        let stem_name = name.split(':').last().unwrap_or(name);\n        let potential_keys_to_check = [\n            name.to_string(), // Exact match first\n            format!(\"static:{}\", stem_name),\n            format!(\"imported:{}\", stem_name),\n            format!(\"custom:{}\", stem_name),\n        ];\n\n        let mut found_key: Option<String> = None;\n        let mut found_rules: Option<&Vec<String>> = None;\n\n        for key_candidate in potential_keys_to_check.iter() {\n            if let Some(rules_list) = resolved.rulesets.get(key_candidate) {\n                found_key = Some(key_candidate.clone());\n                found_rules = Some(rules_list);\n                break;\n            }\n        }\n\n        if let (Some(_key), Some(rules_list)) = (found_key, found_rules) {\n            let plain_text = rules_list.join(\"\\n\");\n            let wrapper = ShowOutputWrapper { value: rules_list }; // Wrap the list\n            print_data_or_text(&wrapper, Some(plain_text), format_opts, \"text\", \"RuleSet\")\n        } else {\n            eprintln!(\n                \"{} Rule set name \\\"{}\\\" not found in resolved rules.\",\n                \"Error:\".red(),\n                name.blue()\n            );\n            list_available_rule_keys(&resolved, quiet);\n            anyhow::bail!(\"Rule set name not found\")\n        }\n    } else {\n        if !quiet {\n            eprintln!(\n                \"{}\",\n                \"Please specify a rule set name to show, or use 'show rules' to show all.\".yellow()\n            );\n        }\n        list_available_rule_keys(&resolved, quiet);\n        Ok(())\n    }\n}\n\nfn handle_show_rule_plural(\n    config: &Config,\n    project_root: &Path, // Need project root to resolve rules\n    format_opts: &FormatOutputOpts,\n    quiet: bool,\n    _verbose: u8,\n) -> Result<()> {\n    if !config.rules.enabled {\n        if !quiet {\n            eprintln!(\n                \"{}\",\n                \"Warning: Rules section is disabled in config.\".yellow()\n            );\n        }\n        return Ok(());\n    }\n\n    let project_characteristics = core::detect_project_characteristics(project_root)\n        .context(\"Failed to detect project characteristics for rule resolution\")?;\n    let resolved =\n        core::config::resolve_rules(&config.rules, project_root, &project_characteristics)\n            .context(\"Failed to resolve rules\")?;\n\n    if resolved.rulesets.is_empty() {\n        if !quiet {\n            println!(\n                \"No rules available or resolved based on current configuration and project content.\"\n            );\n        }\n        return Ok(());\n    }\n\n    let rules_to_output = &resolved.rulesets;\n    let sorted_rules: BTreeMap<_, _> = rules_to_output.iter().collect();\n\n    let pretty_text = if format_opts.format.is_none() {\n        let mut output = String::new();\n        output.push_str(&format!(\n            \"{}\\n\",\n            \"\\n--- Resolved Rule Sets ---\".green().bold().underline()\n        ));\n        for (key, rules) in &sorted_rules {\n            let origin = resolved.origins.get(*key).map_or(\"?\", |s| s.as_str());\n            let origin_colored = match origin {\n                \"default\" | \"default+include\" => origin.cyan(),\n                \"dynamic\" => origin.magenta(),\n                \"include\" => origin.green(),\n                \"import\" => origin.yellow(),\n                \"custom\" => origin.blue(),\n                _ => origin.dimmed(),\n            };\n            output.push_str(&format!(\n                \"\\n▶ {} {}:\\n\",\n                key.blue().bold(),\n                format!(\"({})\", origin_colored)\n            ));\n            if rules.is_empty() {\n                output.push_str(&format!(\"  {}\\n\", \"(empty)\".dimmed()));\n            } else {\n                for rule in *rules {\n                    output.push_str(&format!(\"  {}\\n\", rule));\n                }\n            }\n        }\n        Some(output)\n    } else {\n        None\n    };\n\n    print_data_or_text(&sorted_rules, pretty_text, format_opts, \"text\", \"RuleSets\")\n}\n"
    },
    {
      "path": "cli/commands.rs",
      "content": "pub mod completion;\npub mod config;\npub mod debug;\npub mod generate;\npub mod metrics;\npub mod quick;\npub mod show;\n// Add other command modules here if created (e.g., mcp)\n"
    },
    {
      "path": "cli/output.rs",
      "content": "use anyhow::{Context, Result};\nuse colored::*;\n// Corrected: Separate use statements onto different lines\nuse comfy_table::{Cell, Color, ContentArrangement, Table, presets::UTF8_FULL};\nuse serde::Serialize;\nuse std::fs::{self, File};\nuse std::io::{self, Write};\nuse std::path::Path;\nuse xcontext_core::{ChunkFile, ProjectContext, output_formats}; // Use core types\n\nuse crate::cli_args::FormatOutputOpts; // Use CLI format options\n\n// --- Public Output Functions ---\n\npub fn print_context_or_save(\n    context: &ProjectContext,\n    config: &xcontext_core::Config, // Use core config\n    output_path: Option<&Path>,\n    format_opts: &FormatOutputOpts,\n    quiet: bool,\n) -> Result<()> {\n    let final_format = format_opts\n        .format\n        .as_deref()\n        .unwrap_or(&config.output.format);\n    let pretty_json = !config.output.json_minify; // Use config value after overrides\n    let pretty_xml = config.output.xml_pretty_print; // Use config value after overrides\n\n    let content = serialize_output(\n        context,\n        final_format,\n        pretty_json,\n        pretty_xml,\n        \"ProjectContext\",\n    )?;\n\n    match output_path {\n        Some(path) => {\n            write_to_file(path, &content)?;\n            let is_chunked = context.source.as_ref().is_some_and(|s| s.chunks.is_some());\n            if !is_chunked && !quiet {\n                println!(\n                    \"{} Context saved to: {}\",\n                    \"✅\".green(),\n                    path.display().to_string().blue()\n                );\n            }\n        }\n        None => {\n            write_to_stdout(&content)?;\n        }\n    }\n    Ok(())\n}\n\npub fn save_chunk_file(\n    chunk_data: &ChunkFile,\n    path: &Path,\n    format_opts: &FormatOutputOpts, // Use CLI format opts for chunk format\n    quiet: bool,\n) -> Result<()> {\n    // Chunks are always JSON for now, respect pretty/minify from CLI args\n    let pretty = !format_opts.disable_json_minify || format_opts.enable_json_minify;\n    let content = output_formats::serialize_context_to_json(chunk_data, pretty)?;\n\n    write_to_file(path, &content)?;\n    if !quiet {\n        println!(\n            \"{} Chunk saved to: {}\",\n            \"📦\".blue(), // Use different emoji for chunks\n            path.display().to_string().dimmed()\n        );\n    }\n    Ok(())\n}\n\n// Helper for commands that might output structured data or plain text\npub fn print_data_or_text<T: Serialize>(\n    data: &T,\n    plain_text: Option<String>,\n    format_opts: &FormatOutputOpts,\n    default_format: &str, // e.g., \"json\" or \"text\"\n    root_name: &str,      // For XML root element\n) -> Result<()> {\n    let format = format_opts\n        .format\n        .as_deref()\n        .unwrap_or(default_format)\n        .to_lowercase();\n\n    if format == \"text\" {\n        match plain_text {\n            Some(text) => write_to_stdout(&text),\n            None => {\n                // Fallback to JSON pretty print if text is not available but format is text\n                let pretty = true;\n                let content = output_formats::serialize_context_to_json(data, pretty)?;\n                write_to_stdout(&content)\n            }\n        }\n    } else {\n        let pretty_json = !format_opts.disable_json_minify;\n        let pretty_xml = format_opts.enable_xml_pretty;\n        let content = serialize_output(data, &format, pretty_json, pretty_xml, root_name)?;\n        write_to_stdout(&content)\n    }\n}\n\n// --- Internal Helpers ---\n\nfn serialize_output<T: Serialize>(\n    data: &T,\n    format: &str,\n    pretty_json: bool,\n    pretty_xml: bool,\n    xml_root: &str,\n) -> Result<String> {\n    match format.to_lowercase().as_str() {\n        \"yaml\" | \"yml\" => {\n            output_formats::serialize_context_to_yaml(data).map_err(anyhow::Error::from)\n        }\n        \"xml\" => output_formats::serialize_context_to_xml(data, xml_root, pretty_xml)\n            .map_err(anyhow::Error::from),\n        \"json\" | _ => {\n            // Default to JSON\n            output_formats::serialize_context_to_json(data, pretty_json)\n                .map_err(anyhow::Error::from)\n        }\n    }\n}\n\nfn write_to_file(path: &Path, content: &str) -> Result<()> {\n    if let Some(parent) = path.parent() {\n        fs::create_dir_all(parent)\n            .with_context(|| format!(\"Failed to create directory {}\", parent.display()))?;\n        // Added Context\n    }\n    let mut file =\n        File::create(path).with_context(|| format!(\"Failed to create file {}\", path.display()))?; // Added Context\n    file.write_all(content.as_bytes())\n        .with_context(|| format!(\"Failed to write to file {}\", path.display()))?; // Added Context\n    Ok(())\n}\n\nfn write_to_stdout(content: &str) -> Result<()> {\n    let stdout = io::stdout();\n    let mut handle = stdout.lock();\n    handle\n        .write_all(content.as_bytes())\n        .context(\"Failed to write to stdout\")?; // Added Context\n    // Add a newline if the content doesn't end with one, for better terminal behavior\n    // Corrected: Check for actual newline character '\\n'\n    if !content.ends_with('\\n') {\n        handle\n            .write_all(b\"\\\\n\") // Still write literal `\\n` if adding one, common practice\n            .context(\"Failed to write newline to stdout\")?; // Added Context\n    }\n    handle.flush().context(\"Failed to flush stdout\")?; // Added Context\n    Ok(())\n}\n\n// Example of a pretty printer function for a specific command (e.g., metrics)\npub fn print_metrics_pretty_table(\n    metrics: &crate::commands::metrics::ProjectMetrics,\n) -> Result<()> {\n    println!();\n    println!(\"{}\", \" Project Metrics Summary \".green().bold().underline());\n    println!(\n        \"{:<20} {}\",\n        \"Total Files:\".green(),\n        metrics.total_files.to_string().cyan()\n    );\n    println!(\n        \"{:<20} {}\",\n        \"Total Lines:\".green(),\n        metrics.total_lines.to_string().cyan()\n    );\n    println!(\n        \"{:<20} {}\",\n        \"Total Size:\".green(),\n        metrics.total_bytes_readable.cyan()\n    );\n    println!(\n        \"{:<20} {}\",\n        \"Est. Tokens:\".green(),\n        metrics.estimated_tokens.to_string().cyan()\n    );\n\n    if metrics.files_details.is_empty() {\n        println!(\"\\n{}\", \"(No files included in metrics)\".yellow());\n    } else {\n        println!(\"\\n{}\", \" File Details \".green().bold().underline());\n        let mut table = Table::new();\n        table\n            .load_preset(UTF8_FULL)\n            .set_content_arrangement(ContentArrangement::Dynamic);\n        table.set_header(vec![\n            Cell::new(\"Path\").fg(Color::Green),\n            Cell::new(\"Lines\").fg(Color::Green),\n            Cell::new(\"Size\").fg(Color::Green),\n            Cell::new(\"Tokens\").fg(Color::Green),\n        ]);\n        for file in &metrics.files_details {\n            table.add_row(vec![\n                Cell::new(&file.path).fg(Color::Cyan),\n                Cell::new(file.lines).set_alignment(comfy_table::CellAlignment::Right),\n                Cell::new(&file.bytes_readable)\n                    .set_alignment(comfy_table::CellAlignment::Right)\n                    .fg(Color::DarkGrey),\n                Cell::new(file.estimated_tokens).set_alignment(comfy_table::CellAlignment::Right),\n            ]);\n        }\n        println!(\"{table}\");\n    }\n    println!();\n    Ok(())\n}\n"
    },
    {
      "path": "cli/watch.rs",
      "content": "// Use generate command logic AND the OutputTargetArgs struct from it\nuse crate::cli_args::WatchArgs;\nuse crate::commands::generate::{self, OutputTargetArgs};\nuse crate::load_config_for_command; // Use helper from main\nuse anyhow::{Context, Result};\nuse colored::*;\nuse log;\n// CORRECTED: Removed unused NotifyWatcher alias\nuse notify::{ErrorKind, RecommendedWatcher};\nuse notify_debouncer_mini::{Debouncer, new_debouncer};\nuse std::collections::HashSet;\nuse std::path::{Path, PathBuf};\nuse std::sync::{Arc, mpsc};\nuse xcontext_core::{self as core, Config}; // Use core types\n\n// Removed Watcher type alias\n\nfn watch_path(\n    watcher: &mut Debouncer<RecommendedWatcher>, // Use concrete type\n    path: &Path,\n    watched_paths: &mut HashSet<PathBuf>,\n    quiet: bool,\n) -> Result<()> {\n    // Attempt to canonicalize, fallback to original path if it fails (e.g., path doesn't exist yet)\n    let path_to_watch = path.canonicalize().unwrap_or_else(|_| path.to_path_buf());\n\n    if !watched_paths.contains(&path_to_watch) && path_to_watch.exists() {\n        log::trace!(\"Attempting to watch: {}\", path_to_watch.display());\n        match watcher\n            .watcher()\n            .watch(&path_to_watch, notify::RecursiveMode::NonRecursive) // Watch specific file/dir non-recursively\n        {\n            Ok(_) => {\n                log::debug!(\"Watching: {}\", path_to_watch.display());\n                watched_paths.insert(path_to_watch);\n            }\n            Err(e) => {\n                // Don't error out, just log if not quiet\n                if !quiet {\n                    eprintln!(\n                        \"{} Failed to watch {}: {}\",\n                        \"⚠️\".yellow(),\n                        path.display(),\n                        e\n                    );\n                }\n                log::warn!(\"Failed to watch {}: {}\", path.display(), e);\n            }\n        }\n    } else if !path_to_watch.exists() {\n        log::trace!(\"Skipping watch for non-existent path: {}\", path.display());\n    } else {\n        log::trace!(\"Already watching: {}\", path_to_watch.display());\n    }\n    Ok(())\n}\n\nfn setup_watches(\n    project_root: &Path,\n    current_config: &Arc<Config>,\n    watcher: &mut Debouncer<RecommendedWatcher>, // Use concrete type\n    current_watched: &mut HashSet<PathBuf>,\n    quiet: bool,\n    verbose: u8,\n) -> Result<()> {\n    let paths_to_unwatch: Vec<_> = current_watched.iter().cloned().collect();\n    log::debug!(\"Clearing {} previous watches.\", paths_to_unwatch.len());\n    for path in paths_to_unwatch {\n        match watcher.watcher().unwatch(&path) {\n            Ok(_) => {\n                log::trace!(\"Unwatched: {}\", path.display());\n                current_watched.remove(&path);\n            }\n            Err(e) => match e.kind {\n                ErrorKind::WatchNotFound => {\n                    log::trace!(\"Watch not found for {}, removing.\", path.display());\n                    current_watched.remove(&path); // Remove even if not found\n                }\n                _ => {\n                    if !quiet && verbose > 0 {\n                        eprintln!(\n                            \"{} Failed to unwatch {}: {}\",\n                            \"⚠️\".yellow(),\n                            path.display(),\n                            e\n                        );\n                    }\n                    log::warn!(\"Failed to unwatch {}: {}\", path.display(), e);\n                }\n            },\n        }\n    }\n    current_watched.clear(); // Ensure it's empty before adding new ones\n\n    log::debug!(\"Setting up new watches based on current config...\");\n    match core::gather_files_and_tree(project_root, current_config, quiet) {\n        Ok((source_files, docs_files, _)) => {\n            if current_config.source.enabled {\n                for file_info in source_files {\n                    let _ = watch_path(watcher, &file_info.path, current_watched, quiet);\n                }\n            }\n            if current_config.is_docs_section_active() {\n                for file_info in docs_files {\n                    let _ = watch_path(watcher, &file_info.path, current_watched, quiet);\n                }\n            }\n            // Watch imported rule files\n            for rule_import_rel in &current_config.rules.import {\n                let mut path = project_root.join(rule_import_rel);\n                if !path.exists() {\n                    path = project_root\n                        .join(core::config::DEFAULT_CONFIG_DIR)\n                        .join(rule_import_rel);\n                }\n                if path.exists() {\n                    let _ = watch_path(watcher, &path, current_watched, quiet);\n                } else {\n                    log::warn!(\n                        \"Could not find imported rule file to watch: {}\",\n                        rule_import_rel.display()\n                    );\n                }\n            }\n            // Watch imported prompt files\n            for prompt_import_rel in &current_config.prompts.import {\n                let mut path = project_root.join(prompt_import_rel);\n                if !path.exists() {\n                    path = project_root\n                        .join(core::config::DEFAULT_CONFIG_DIR)\n                        .join(prompt_import_rel);\n                }\n                if path.exists() {\n                    let _ = watch_path(watcher, &path, current_watched, quiet);\n                } else {\n                    log::warn!(\n                        \"Could not find imported prompt file to watch: {}\",\n                        prompt_import_rel.display()\n                    );\n                }\n            }\n        }\n        Err(e) => {\n            if !quiet {\n                eprintln!(\n                    \"{} {}\",\n                    \"⚠️ Error gathering files for watch setup:\".yellow(),\n                    e\n                );\n            }\n        }\n    }\n\n    let config_path_to_watch = Config::resolve_config_path(\n        project_root,\n        None,  // Rely on default resolution logic here for watching\n        false, // Don't disable checking for the config file\n    )?;\n    if let Some(ref config_path) = config_path_to_watch {\n        let _ = watch_path(watcher, config_path, current_watched, quiet);\n    }\n\n    if current_watched.is_empty() {\n        if !quiet {\n            println!(\n                \"{}\",\n                \"⚠️ No source, documentation, import, or config files found to watch based on current configuration.\"\n                    .yellow()\n            );\n        }\n    } else if !quiet && verbose > 0 {\n        println!(\"🔍 Watching {} files/paths...\", current_watched.len());\n    }\n    Ok(())\n}\n\npub fn run_watch_mode(watch_args: WatchArgs, quiet: bool, verbose: u8) -> Result<()> {\n    let project_root =\n        Config::determine_project_root(watch_args.project_config.project_root.as_ref())\n            .context(\"Failed to determine project root for watch mode\")?;\n\n    if !quiet {\n        println!(\n            \"👀 Starting watch mode for '{}'. Press Ctrl+C to exit.\",\n            project_root.display()\n        );\n    }\n\n    let mut config = Arc::new(\n        load_config_for_command(\n            &project_root,\n            &watch_args.project_config,\n            None, // No generate args\n            Some(&watch_args),\n            None, // Format handled by generate call\n        )\n        .context(\"Failed to load initial configuration for watch mode\")?,\n    );\n\n    let initial_output_target_args = OutputTargetArgs {\n        save: &watch_args.save,\n        chunks: &None, // Watch mode doesn't support chunking trigger\n        stdout: watch_args.save.is_none(), // Default to stdout if not saving\n        format_output: &watch_args.format_output,\n    };\n\n    if let Err(e) = generate::trigger_generation(\n        &project_root,\n        &config,\n        &initial_output_target_args,\n        quiet,\n        verbose,\n    ) {\n        if !quiet {\n            eprintln!(\"{} {}\\n\", \"⚠️ Error during initial generation:\".yellow(), e);\n        }\n    } else if !quiet && verbose > 0 {\n        println!(\"{}\\n\", \"✅ Initial generation complete.\".green());\n    }\n\n    let (tx, rx) = mpsc::channel();\n    let delay_duration = config\n        .get_watch_delay()\n        .with_context(|| \"Invalid watch delay duration\")?;\n    let mut debouncer = new_debouncer(delay_duration, tx)\n        .map_err(|e| anyhow::anyhow!(\"Failed to create debouncer: {}\", e))?;\n    let mut watched_paths = HashSet::new();\n\n    if let Err(e) = setup_watches(\n        &project_root,\n        &config,\n        &mut debouncer,\n        &mut watched_paths,\n        quiet,\n        verbose,\n    ) {\n        if !quiet {\n            eprintln!(\n                \"{} {}\\n\",\n                \"⚠️ Error setting up initial watches:\".yellow(),\n                e\n            );\n        }\n    }\n\n    loop {\n        match rx.recv() {\n            Ok(event_result) => match event_result {\n                Ok(debounced_events) => {\n                    if !debounced_events.is_empty() {\n                        if !quiet && verbose > 0 {\n                            eprintln!(\n                                \"\\n{} {} event(s) detected.\",\n                                \"🔄\".blue(),\n                                debounced_events.len()\n                            );\n                            for event in &debounced_events {\n                                log::trace!(\"Debounced event: {:?}\", event);\n                            }\n                        }\n\n                        let config_path_being_used_result = Config::resolve_config_path(\n                            &project_root,\n                            watch_args.project_config.context_file.as_ref(),\n                            watch_args.project_config.disable_context_file,\n                        );\n\n                        let config_changed = if let Ok(Some(ref current_config_path)) =\n                            config_path_being_used_result\n                        {\n                            let canonical_config_path = current_config_path.canonicalize().ok();\n                            debounced_events.iter().any(|event| {\n                                let event_path_canonical = event.path.canonicalize().ok();\n                                match (&canonical_config_path, &event_path_canonical) {\n                                    (Some(conf_canon), Some(evt_canon)) => conf_canon == evt_canon,\n                                    _ => event.path == *current_config_path,\n                                }\n                            })\n                        } else {\n                            false\n                        };\n\n                        let mut config_reloaded = false;\n                        if config_changed {\n                            if !quiet && verbose > 0 {\n                                eprintln!(\n                                    \"{}\",\n                                    \"🔄 Config file changed. Reloading configuration...\".blue()\n                                );\n                            }\n                            match load_config_for_command(\n                                &project_root,\n                                &watch_args.project_config,\n                                None,\n                                Some(&watch_args),\n                                None,\n                            ) {\n                                Ok(reloaded_config) => {\n                                    config = Arc::new(reloaded_config);\n                                    config_reloaded = true;\n                                    if !quiet && verbose > 0 {\n                                        eprintln!(\"{}\", \"✅ Configuration reloaded.\".green());\n                                    }\n                                    if let Err(e) = setup_watches(\n                                        &project_root,\n                                        &config,\n                                        &mut debouncer,\n                                        &mut watched_paths,\n                                        quiet,\n                                        verbose,\n                                    ) {\n                                        if !quiet {\n                                            eprintln!(\n                                                \"{} {}\",\n                                                \"⚠️ Error setting up watches after config reload:\"\n                                                    .yellow(),\n                                                e\n                                            );\n                                        }\n                                    }\n                                }\n                                Err(e) => {\n                                    if !quiet {\n                                        eprintln!(\n                                            \"{} {:#}\\n\",\n                                            \"⚠️ Error reloading config:\".yellow(),\n                                            e\n                                        );\n                                    }\n                                }\n                            }\n                        }\n\n                        if !quiet && verbose > 0 && !config_reloaded {\n                            eprintln!(\"{}\", \"\\n🔄 Regenerating context...\".blue());\n                        } else if !quiet && verbose > 0 && config_reloaded {\n                            // Already printed message\n                        }\n\n                        let output_target_args = OutputTargetArgs {\n                            save: &watch_args.save,\n                            chunks: &None,\n                            stdout: watch_args.save.is_none(),\n                            format_output: &watch_args.format_output,\n                        };\n\n                        if let Err(e) = generate::trigger_generation(\n                            &project_root,\n                            &config,\n                            &output_target_args,\n                            quiet,\n                            verbose,\n                        ) {\n                            if !quiet {\n                                eprintln!(\"{} {:#}\\n\", \"⚠️ Error during regeneration:\".yellow(), e);\n                            }\n                        } else if !quiet && verbose > 0 {\n                            println!(\"{}\\n\", \"✅ Regeneration complete.\".green());\n                        }\n\n                        if !quiet && verbose > 0 && !watched_paths.is_empty() {\n                            println!(\"🔍 Watching {} files/paths...\", watched_paths.len());\n                        } else if !quiet && verbose > 0 && watched_paths.is_empty() {\n                            println!(\"🔍 No files currently being watched.\");\n                        }\n                    } else {\n                        log::trace!(\"Received empty debounced event list.\");\n                    }\n                }\n                Err(error) => {\n                    if !quiet {\n                        eprintln!(\"{} {:#}\\n\", \"⚠️ Watch error:\".yellow(), error);\n                    }\n                    log::error!(\"Notify error received: {:?}\", error);\n                }\n            },\n            Err(e) => {\n                eprintln!(\"{} {:#}\\n\", \"⛔ Watcher channel error:\".red(), e);\n                break Ok(());\n            }\n        }\n    }\n}\n"
    },
    {
      "path": "core/Cargo.toml",
      "content": "[package]\nname = \"xcontext-core\"\nversion = \"0.1.0\"\nedition = \"2024\"\ndescription = \"Core context generation logic for xcontext\"\nlicense = \"GPL-3.0-or-later\"\npublish = false\n\n[lib]\nname = \"xcontext_core\"\npath = \"core.rs\"\n\n[features]\ndefault = [\"serde_support\"]\nserde_support = [\"dep:serde\", \"dep:chrono\", \"dep:indexmap\", \"dep:byte-unit\", \"dep:quick-xml\"]\n\n[dependencies]\nlog = { workspace = true }\nserde = { workspace = true, optional = true }\nserde_json = { workspace = true }\nserde_yml = { workspace = true }\ntoml = { workspace = true }\nthiserror = { workspace = true }\nchrono = { workspace = true, optional = true }\nindexmap = { workspace = true, optional = true }\nbyte-unit = { workspace = true, optional = true }\nignore = { workspace = true }\nwalkdir = { workspace = true }\nglobset = { workspace = true }\nrust-embed = { workspace = true }\nquick-xml = { workspace = true, optional = true }\nsysinfo = { workspace = true }\ntiktoken-rs = { workspace = true }\npathdiff = { workspace = true }\nshellexpand = { workspace = true }\nonce_cell = { workspace = true }\ncolored = { workspace = true } # For internal logging/warnings if needed\nrayon = { workspace = true }\nparse_duration = { workspace = true } # Added from original src/config.rs\n"
    },
    {
      "path": "core/build.rs",
      "content": "fn main() {\n    println!(\"cargo:rerun-if-changed=data/prompts.yaml\");\n    println!(\"cargo:rerun-if-changed=data/builtin_ignores.yaml\");\n    println!(\"cargo:rerun-if-changed=data/ai_readme.yaml\");\n    println!(\"cargo:rerun-if-changed=data/rules/\");\n}\n"
    },
    {
      "path": "core/chunking.rs",
      "content": "use crate::error::{AppError, Result};\nuse crate::gather::FileInfo;\nuse crate::output_formats::{ChunkFile, ChunkInfo, FileContextInfo};\nuse byte_unit::Byte;\nuse log;\nuse std::convert::TryInto;\nuse std::path::Path;\nuse std::str::FromStr;\n\npub fn split_files_into_chunks(\n    source_files: Vec<FileInfo>,\n    chunk_size_str: &str,\n    project_root: &Path,\n) -> Result<Vec<ChunkFile>> {\n    let byte_value = Byte::from_str(chunk_size_str).map_err(|e| {\n        AppError::Chunking(format!(\n            \"Invalid chunk size format '{}': {}. Use KB, MB, etc.\",\n            chunk_size_str, e\n        ))\n    })?;\n    let target_chunk_size_bytes: u128 = byte_value.into();\n    let target_chunk_size_bytes_usize = target_chunk_size_bytes.try_into().map_err(|_| {\n        AppError::Chunking(\"Chunk size exceeds maximum usize value on this platform.\".to_string())\n    })?;\n\n    if target_chunk_size_bytes_usize == 0 {\n        return Err(AppError::Chunking(\n            \"Chunk size must be greater than 0 bytes\".to_string(),\n        ));\n    }\n\n    let mut chunks_data: Vec<Vec<FileContextInfo>> = Vec::new();\n    let mut current_chunk_files: Vec<FileContextInfo> = Vec::new();\n    let mut current_chunk_size: usize = 0;\n\n    let all_file_contexts: Vec<FileContextInfo> = source_files\n        .into_iter()\n        .map(|finfo| FileContextInfo {\n            path: pathdiff::diff_paths(&finfo.path, project_root)\n                .unwrap_or_else(|| finfo.path.clone())\n                .to_string_lossy()\n                .to_string(),\n            content: finfo.content,\n        })\n        .collect();\n\n    for file_context in all_file_contexts {\n        let file_size = file_context.content.len(); // Use content length for size\n\n        if file_size == 0 {\n            log::trace!(\"Skipping empty file: {}\", file_context.path);\n            continue; // Skip empty files\n        }\n\n        if file_size > target_chunk_size_bytes_usize {\n            log::trace!(\n                \"File {} ({}) exceeds chunk size ({}), putting in its own chunk.\",\n                file_context.path,\n                file_size,\n                target_chunk_size_bytes_usize\n            );\n            // If the current chunk isn't empty, push it first\n            if !current_chunk_files.is_empty() {\n                chunks_data.push(std::mem::take(&mut current_chunk_files));\n                current_chunk_size = 0; // Reset size for the next chunk\n            }\n            // Push the large file as its own chunk\n            chunks_data.push(vec![file_context]);\n            continue; // Move to the next file\n        }\n\n        // Check if adding the current file exceeds the chunk size\n        if !current_chunk_files.is_empty()\n            && (current_chunk_size.saturating_add(file_size)) > target_chunk_size_bytes_usize\n        {\n            // Current chunk is full, push it and start a new one\n            chunks_data.push(std::mem::take(&mut current_chunk_files));\n            current_chunk_files = vec![file_context]; // Start new chunk with current file\n            current_chunk_size = file_size;\n        } else {\n            // Add file to the current chunk\n            current_chunk_size = current_chunk_size.saturating_add(file_size);\n            current_chunk_files.push(file_context);\n        }\n    }\n\n    // Push the last chunk if it's not empty\n    if !current_chunk_files.is_empty() {\n        chunks_data.push(current_chunk_files);\n    }\n\n    let total_parts = chunks_data.len();\n    if total_parts == 0 {\n        log::debug!(\"No non-empty files to chunk.\");\n        return Ok(Vec::new()); // Return empty vec if no chunks were created\n    }\n\n    log::info!(\"Split content into {} chunks.\", total_parts);\n\n    let final_chunks: Vec<ChunkFile> = chunks_data\n        .into_iter()\n        .enumerate()\n        .map(|(i, chunk_files)| {\n            let chunk_num = i + 1;\n            let chunk_info = ChunkInfo {\n                current_part: chunk_num,\n                total_parts,\n            };\n            ChunkFile {\n                files: chunk_files,\n                chunk_info,\n            }\n        })\n        .collect();\n\n    Ok(final_chunks)\n}\n"
    },
    {
      "path": "core/config.rs",
      "content": "use crate::error::{AppError, Result};\nuse crate::rules::{self, mapping as rules_mapping};\nuse indexmap::IndexMap;\nuse log;\nuse parse_duration::parse;\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, HashSet};\nuse std::env;\nuse std::fs;\nuse std::path::{Path, PathBuf};\nuse std::time::Duration;\n\npub const DEFAULT_CONFIG_DIR: &str = \".xtools/xcontext\";\npub const DEFAULT_CONFIG_FILENAME: &str = \"xcontext.toml\";\npub const DEFAULT_CACHE_DIR: &str = \".xtools/xcontext/cache\";\npub const DEFAULT_WATCH_DELAY: &str = \"300ms\";\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]\n#[serde(deny_unknown_fields)]\npub struct Config {\n    #[serde(default)]\n    pub general: GeneralConfig,\n    #[serde(default)]\n    pub common_filters: CommonFiltersConfig,\n    #[serde(default)]\n    pub meta: MetaConfig,\n    #[serde(default)]\n    pub docs: DocsConfig,\n    #[serde(default)]\n    pub tree: TreeConfig,\n    #[serde(default)]\n    pub source: SourceConfig,\n    #[serde(default)]\n    pub rules: RulesConfig,\n    #[serde(default)]\n    pub prompts: PromptsConfig,\n    #[serde(default)]\n    pub output: OutputConfig,\n    #[serde(default)]\n    pub save: SaveConfig,\n    #[serde(default)]\n    pub watch: WatchConfig,\n}\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]\n#[serde(deny_unknown_fields)]\npub struct GeneralConfig {\n    #[serde(default)]\n    pub project_name: Option<String>,\n    #[serde(default = \"default_true\")]\n    pub use_gitignore: bool,\n    #[serde(default = \"default_true\")]\n    pub enable_builtin_ignore: bool,\n}\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq, Default)]\n#[serde(deny_unknown_fields)]\npub struct CommonFiltersConfig {\n    #[serde(default)]\n    pub include: Vec<String>,\n    #[serde(default)]\n    pub exclude: Vec<String>,\n}\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]\npub struct MetaConfig {\n    #[serde(default = \"default_true\")]\n    pub enabled: bool,\n    #[serde(flatten, default)]\n    pub custom_meta: HashMap<String, String>,\n}\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]\n#[serde(deny_unknown_fields)]\npub struct DocsConfig {\n    #[serde(default = \"default_true\")]\n    pub enabled: bool,\n    #[serde(default)]\n    pub use_gitignore: IgnoreSetting,\n    #[serde(default)]\n    pub include: Option<Vec<String>>,\n    #[serde(default)]\n    pub exclude: Option<Vec<String>>,\n}\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]\n#[serde(deny_unknown_fields)]\npub struct TreeConfig {\n    #[serde(default = \"default_true\")]\n    pub enabled: bool,\n    #[serde(default)]\n    pub use_gitignore: IgnoreSetting,\n    #[serde(default)]\n    pub include: Option<Vec<String>>,\n    #[serde(default)]\n    pub exclude: Option<Vec<String>>,\n}\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]\n#[serde(deny_unknown_fields)]\npub struct SourceConfig {\n    #[serde(default = \"default_true\")]\n    pub enabled: bool,\n    #[serde(default)]\n    pub use_gitignore: IgnoreSetting,\n    #[serde(default)]\n    pub include: Option<Vec<String>>,\n    #[serde(default)]\n    pub exclude: Option<Vec<String>>,\n}\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]\npub struct RulesConfig {\n    #[serde(default = \"default_true\")]\n    pub enabled: bool,\n    #[serde(default)]\n    pub include: Vec<String>,\n    #[serde(default)]\n    pub exclude: Vec<String>,\n    #[serde(default)]\n    pub import: Vec<PathBuf>,\n    #[serde(flatten)]\n    pub custom: IndexMap<String, Vec<String>>,\n}\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq, Default)]\npub struct PromptsConfig {\n    #[serde(default)]\n    pub import: Vec<PathBuf>, // Changed to PathBuf\n    #[serde(flatten, default)]\n    pub custom: HashMap<String, String>,\n}\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]\n#[serde(deny_unknown_fields)]\npub struct OutputConfig {\n    #[serde(default = \"default_format\")]\n    pub format: String,\n    #[serde(default = \"default_true\")]\n    pub json_minify: bool,\n    #[serde(default = \"default_false\")]\n    pub xml_pretty_print: bool, // Renamed for clarity, maps to !xml_minify\n    #[serde(default = \"default_true\")]\n    pub include_project_name: bool,\n    #[serde(default = \"default_true\")]\n    pub include_project_root: bool,\n    #[serde(default = \"default_true\")]\n    pub include_system_info: bool,\n    #[serde(default = \"default_true\")]\n    pub include_timestamp: bool,\n}\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]\n#[serde(deny_unknown_fields)]\npub struct SaveConfig {\n    #[serde(default = \"default_save_dir_config\")]\n    pub output_dir: PathBuf,\n    #[serde(default)]\n    pub filename_base: Option<String>,\n    #[serde(default)]\n    pub extension: Option<String>,\n}\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]\n#[serde(deny_unknown_fields)]\npub struct WatchConfig {\n    #[serde(default = \"default_watch_delay_string\")]\n    pub delay: String,\n}\n\n#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]\n#[serde(rename_all = \"snake_case\")]\npub enum IgnoreSetting {\n    Inherit,\n    True,\n    False,\n}\n\nimpl Default for IgnoreSetting {\n    fn default() -> Self {\n        IgnoreSetting::Inherit\n    }\n}\n\nfn default_true() -> bool {\n    true\n}\nfn default_false() -> bool {\n    false\n}\nfn default_format() -> String {\n    \"json\".to_string()\n}\nfn default_save_dir_config() -> PathBuf {\n    PathBuf::from(DEFAULT_CACHE_DIR)\n}\nfn default_watch_delay_string() -> String {\n    DEFAULT_WATCH_DELAY.to_string()\n}\n\nimpl Default for Config {\n    fn default() -> Self {\n        Self {\n            general: GeneralConfig::default(),\n            common_filters: CommonFiltersConfig::default(),\n            meta: MetaConfig::default(),\n            docs: DocsConfig::default(),\n            tree: TreeConfig::default(),\n            source: SourceConfig::default(),\n            rules: RulesConfig::default(),\n            prompts: PromptsConfig::default(),\n            output: OutputConfig::default(),\n            save: SaveConfig::default(),\n            watch: WatchConfig::default(),\n        }\n    }\n}\nimpl Default for GeneralConfig {\n    fn default() -> Self {\n        Self {\n            project_name: None,\n            use_gitignore: default_true(),\n            enable_builtin_ignore: default_true(),\n        }\n    }\n}\nimpl Default for MetaConfig {\n    fn default() -> Self {\n        let mut custom_meta = HashMap::new();\n        custom_meta.insert(\"author\".to_string(), \"json\".to_string());\n        Self {\n            enabled: default_true(),\n            custom_meta,\n        }\n    }\n}\nimpl Default for DocsConfig {\n    fn default() -> Self {\n        Self {\n            enabled: default_true(),\n            use_gitignore: IgnoreSetting::default(),\n            include: Some(Vec::new()),\n            exclude: Some(Vec::new()),\n        }\n    }\n}\nimpl Default for TreeConfig {\n    fn default() -> Self {\n        Self {\n            enabled: default_true(),\n            use_gitignore: IgnoreSetting::default(),\n            include: Some(Vec::new()),\n            exclude: Some(Vec::new()),\n        }\n    }\n}\nimpl Default for SourceConfig {\n    fn default() -> Self {\n        Self {\n            enabled: default_true(),\n            use_gitignore: IgnoreSetting::default(),\n            include: Some(Vec::new()),\n            exclude: Some(Vec::new()),\n        }\n    }\n}\nimpl Default for RulesConfig {\n    fn default() -> Self {\n        Self {\n            enabled: true,\n            include: Vec::new(),\n            exclude: Vec::new(),\n            import: Vec::new(),\n            custom: IndexMap::new(),\n        }\n    }\n}\n\nimpl Default for OutputConfig {\n    fn default() -> Self {\n        Self {\n            format: default_format(),\n            json_minify: default_true(),\n            xml_pretty_print: default_false(),\n            include_project_name: default_true(),\n            include_project_root: default_true(),\n            include_system_info: default_true(),\n            include_timestamp: default_true(),\n        }\n    }\n}\nimpl Default for SaveConfig {\n    fn default() -> Self {\n        Self {\n            output_dir: default_save_dir_config(),\n            filename_base: None,\n            extension: None,\n        }\n    }\n}\nimpl Default for WatchConfig {\n    fn default() -> Self {\n        Self {\n            delay: default_watch_delay_string(),\n        }\n    }\n}\n\nimpl Config {\n    pub fn get_effective_include<'a>(\n        &'a self,\n        section_include: &'a Option<Vec<String>>,\n    ) -> &'a Vec<String> {\n        section_include\n            .as_ref()\n            .unwrap_or(&self.common_filters.include)\n    }\n    pub fn get_effective_exclude<'a>(\n        &'a self,\n        section_exclude: &'a Option<Vec<String>>,\n    ) -> &'a Vec<String> {\n        section_exclude\n            .as_ref()\n            .unwrap_or(&self.common_filters.exclude)\n    }\n\n    pub fn determine_project_root(cli_project_root: Option<&PathBuf>) -> Result<PathBuf> {\n        let path_str_opt = cli_project_root\n            .map(|p| p.to_string_lossy().to_string())\n            .or_else(|| env::var(\"PROJECT_ROOT\").ok().filter(|s| !s.is_empty()));\n\n        let path_to_resolve = match path_str_opt {\n            Some(p_str) => PathBuf::from(shellexpand::tilde(&p_str).as_ref()),\n            None => env::current_dir().map_err(AppError::Io)?,\n        };\n\n        path_to_resolve.canonicalize().map_err(|e| {\n            AppError::Io(std::io::Error::new(\n                e.kind(),\n                format!(\n                    \"Failed to canonicalize project root '{}': {}\",\n                    path_to_resolve.display(),\n                    e\n                ),\n            ))\n        })\n    }\n\n    pub fn resolve_config_path(\n        project_root: &Path,\n        cli_config_file: Option<&String>,\n        cli_disable_config: bool,\n    ) -> Result<Option<PathBuf>> {\n        if cli_disable_config {\n            log::debug!(\"Config file loading disabled via CLI flag.\");\n            return Ok(None);\n        }\n\n        let path_to_check = match cli_config_file {\n            Some(p_str) => {\n                let expanded_path_cow = shellexpand::tilde(p_str);\n                let mut path = PathBuf::from(expanded_path_cow.as_ref());\n                let looks_like_path = path.is_absolute()\n                    || path.components().count() > 1\n                    || p_str.contains(['/', '\\\\']);\n\n                if looks_like_path {\n                    if !path.exists() && path.extension().is_none() {\n                        path.set_extension(\"toml\");\n                    }\n                    if !path.exists() {\n                        return Err(AppError::Config(format!(\n                            \"Specified config file not found at path: {}\",\n                            path.display()\n                        )));\n                    }\n                    log::debug!(\"Using specified config file path: {}\", path.display());\n                    Some(path)\n                } else {\n                    let filename = if path.extension().map_or(true, |e| e != \"toml\") {\n                        format!(\"{}.toml\", path.to_string_lossy())\n                    } else {\n                        path.to_string_lossy().to_string()\n                    };\n                    let full_path = project_root.join(DEFAULT_CONFIG_DIR).join(filename);\n                    if !full_path.exists() {\n                        return Err(AppError::Config(format!(\n                            \"Specified config file '{}' not found in default directory: {}\",\n                            path.display(),\n                            project_root.join(DEFAULT_CONFIG_DIR).display()\n                        )));\n                    }\n                    log::debug!(\n                        \"Using specified config filename in default directory: {}\",\n                        full_path.display()\n                    );\n                    Some(full_path)\n                }\n            }\n            None => {\n                let default_path = project_root\n                    .join(DEFAULT_CONFIG_DIR)\n                    .join(DEFAULT_CONFIG_FILENAME);\n                if default_path.exists() {\n                    log::debug!(\"Using default config file path: {}\", default_path.display());\n                    Some(default_path)\n                } else {\n                    log::debug!(\n                        \"No config file specified and default not found at: {}\",\n                        default_path.display()\n                    );\n                    None\n                }\n            }\n        };\n        Ok(path_to_check)\n    }\n\n    pub fn load_from_path(config_path: &Path) -> Result<Self> {\n        log::info!(\"Loading configuration from: {}\", config_path.display());\n        let toml_content = fs::read_to_string(config_path).map_err(|e| AppError::FileRead {\n            path: config_path.to_path_buf(),\n            source: e,\n        })?;\n        toml::from_str::<Config>(&toml_content).map_err(|e| {\n            AppError::TomlParse(format!(\n                \"Error parsing config file '{}': {}. Check TOML syntax and structure.\",\n                config_path.display(),\n                e\n            ))\n        })\n    }\n\n    pub fn get_watch_delay(&self) -> Result<Duration> {\n        parse(&self.watch.delay).map_err(|e| {\n            AppError::InvalidArgument(format!(\n                \"Invalid watch delay duration '{}': {}. Use format like '500ms', '2s'.\",\n                self.watch.delay, e\n            ))\n        })\n    }\n\n    pub fn get_effective_gitignore(&self, section_setting: &IgnoreSetting) -> bool {\n        match section_setting {\n            IgnoreSetting::True => true,\n            IgnoreSetting::False => false,\n            IgnoreSetting::Inherit => self.general.use_gitignore,\n        }\n    }\n\n    pub fn get_effective_builtin_ignore(&self) -> bool {\n        self.general.enable_builtin_ignore\n    }\n\n    pub fn is_docs_section_active(&self) -> bool {\n        self.docs.enabled\n    }\n\n    pub fn get_effective_project_name(&self, project_root: &Path) -> String {\n        self.general.project_name.clone().unwrap_or_else(|| {\n            project_root\n                .file_name()\n                .map(|n| n.to_string_lossy().to_string())\n                .unwrap_or_else(|| \"UnknownProject\".to_string())\n        })\n    }\n}\n\n#[derive(Debug, Default, Clone, Serialize)]\npub struct ResolvedRules {\n    pub rulesets: IndexMap<String, Vec<String>>,\n    pub origins: HashMap<String, String>,\n}\n\npub fn resolve_rules(\n    rules_config: &RulesConfig,\n    project_root: &Path,\n    project_characteristics: &HashSet<String>,\n) -> Result<ResolvedRules> {\n    let mut resolved = ResolvedRules::default();\n    if !rules_config.enabled {\n        log::debug!(\"Rules generation is disabled in configuration.\");\n        return Ok(resolved);\n    }\n    log::info!(\"Resolving rules...\");\n\n    let mut base_static_stems: HashSet<&str> = rules_mapping::get_default_rule_stems();\n    log::trace!(\"Default rule stems: {:?}\", base_static_stems);\n\n    for char in project_characteristics {\n        let char_lower = char.to_lowercase();\n        if let Some(stem) = rules_mapping::map_characteristic_to_rule_stem(&char_lower)\n            .or_else(|| rules_mapping::map_characteristic_to_rule_stem(char))\n        {\n            if base_static_stems.insert(stem) {\n                log::trace!(\n                    \"Dynamically detected rule stem '{}' from characteristic '{}'\",\n                    stem,\n                    char\n                );\n            }\n        }\n    }\n    log::debug!(\n        \"Base static rule stems (defaults + dynamic): {:?}\",\n        base_static_stems\n    );\n\n    let exclude_stems: HashSet<&str> = rules_config.exclude.iter().map(String::as_str).collect();\n    if !exclude_stems.is_empty() {\n        log::debug!(\"Applying rule exclusions: {:?}\", exclude_stems);\n    }\n\n    let mut effective_static_stems: HashSet<&str> = base_static_stems\n        .difference(&exclude_stems)\n        .copied()\n        .collect();\n    log::debug!(\n        \"Static stems after exclusions: {:?}\",\n        effective_static_stems\n    );\n\n    let include_stems: HashSet<&str> = rules_config.include.iter().map(String::as_str).collect();\n    if !include_stems.is_empty() {\n        log::debug!(\"Applying explicit rule inclusions: {:?}\", include_stems);\n    }\n    for stem in include_stems.iter() {\n        effective_static_stems.insert(stem);\n    }\n    log::debug!(\"Final static stems to load: {:?}\", effective_static_stems);\n\n    for stem in effective_static_stems.iter() {\n        match rules::get_static_rule_content(stem) {\n            Ok(content) => {\n                let key = format!(\"static:{}\", stem);\n                resolved.rulesets.insert(\n                    key.clone(),\n                    content\n                        .lines()\n                        .map(str::trim)\n                        .filter(|s| !s.is_empty())\n                        .map(String::from)\n                        .collect(),\n                );\n                let origin = match (\n                    rules_mapping::get_default_rule_stems().contains(stem),\n                    include_stems.contains(stem),\n                ) {\n                    (true, true) => \"default+include\",\n                    (true, false) => \"default\",\n                    (false, true) => \"include\",\n                    (false, false) => \"dynamic\",\n                };\n                resolved.origins.insert(key.clone(), origin.to_string());\n                log::trace!(\"Loaded static rule: {} (Origin: {})\", key, origin);\n            }\n            Err(e) => {\n                log::warn!(\"Skipping static rule stem '{}': {}\", stem, e);\n            }\n        }\n    }\n\n    if !rules_config.import.is_empty() {\n        log::debug!(\"Loading imported rules from: {:?}\", rules_config.import);\n    }\n    for import_path_rel in &rules_config.import {\n        let mut import_path = project_root.join(import_path_rel);\n        if !import_path.exists() {\n            let config_dir = project_root.join(DEFAULT_CONFIG_DIR);\n            import_path = config_dir.join(import_path_rel);\n            if import_path.exists() {\n                log::trace!(\n                    \"Found import {} relative to config dir\",\n                    import_path_rel.display()\n                );\n            } else {\n                log::warn!(\n                    \"Could not find imported rule file '{}' relative to project root or config dir. Skipping.\",\n                    import_path_rel.display()\n                );\n                continue;\n            }\n        }\n\n        let stem = import_path\n            .file_stem()\n            .and_then(|s| s.to_str())\n            .unwrap_or(\"imported_rule\");\n        let key = format!(\"imported:{}\", stem);\n        match fs::read_to_string(&import_path) {\n            Ok(content) => {\n                resolved.rulesets.insert(\n                    key.clone(),\n                    content\n                        .lines()\n                        .map(str::trim)\n                        .filter(|s| !s.is_empty())\n                        .map(String::from)\n                        .collect(),\n                );\n                resolved.origins.insert(key.clone(), \"import\".to_string());\n                log::trace!(\"Loaded imported rule: {}\", import_path.display());\n            }\n            Err(e) => {\n                log::warn!(\n                    \"Failed to read imported rule file '{}': {}\",\n                    import_path.display(),\n                    e\n                );\n            }\n        }\n    }\n\n    if !rules_config.custom.is_empty() {\n        log::debug!(\n            \"Loading custom rules defined in config: {:?}\",\n            rules_config.custom.keys()\n        );\n    }\n    for (name, rules_list) in &rules_config.custom {\n        if rules_list.is_empty() {\n            log::trace!(\"Skipping empty custom rule list: {}\", name);\n            continue;\n        }\n        let key = format!(\"custom:{}\", name);\n        resolved.rulesets.insert(\n            key.clone(),\n            rules_list\n                .iter()\n                .map(String::as_str)\n                .map(str::trim)\n                .filter(|s| !s.is_empty())\n                .map(String::from)\n                .collect(),\n        );\n        resolved.origins.insert(key.clone(), \"custom\".to_string());\n        log::trace!(\"Loaded custom rule set: {}\", name);\n    }\n    log::info!(\"Resolved {} rulesets.\", resolved.rulesets.len());\n    Ok(resolved)\n}\n\npub fn resolve_prompts(\n    prompts_config: &PromptsConfig,\n    project_root: &Path,\n) -> Result<HashMap<String, String>> {\n    let mut resolved = HashMap::new();\n    let predefined = crate::output_formats::get_predefined_prompts(); // Assuming get_predefined_prompts moved here\n    resolved.extend(\n        predefined\n            .iter()\n            .map(|(k, v)| (format!(\"static:{}\", k), v.clone())),\n    );\n\n    if !prompts_config.import.is_empty() {\n        log::debug!(\"Loading imported prompts from: {:?}\", prompts_config.import);\n    }\n    for import_path_rel in &prompts_config.import {\n        let mut import_path = project_root.join(import_path_rel);\n        if !import_path.exists() {\n            let config_dir = project_root.join(DEFAULT_CONFIG_DIR);\n            import_path = config_dir.join(import_path_rel);\n            if !import_path.exists() {\n                log::warn!(\n                    \"Could not find imported prompt file '{}' relative to project root or config dir. Skipping.\",\n                    import_path_rel.display()\n                );\n                continue;\n            }\n        }\n\n        let stem = import_path\n            .file_stem()\n            .and_then(|s| s.to_str())\n            .unwrap_or(\"imported_prompt\");\n        let key = format!(\"imported:{}\", stem);\n        match fs::read_to_string(&import_path) {\n            Ok(content) => {\n                if !content.trim().is_empty() {\n                    resolved.insert(key.clone(), content);\n                    log::trace!(\"Loaded imported prompt: {}\", import_path.display());\n                } else {\n                    log::trace!(\"Skipping empty imported prompt: {}\", import_path.display());\n                }\n            }\n            Err(e) => {\n                log::warn!(\n                    \"Failed to read imported prompt file '{}': {}\",\n                    import_path.display(),\n                    e\n                );\n            }\n        }\n    }\n\n    if !prompts_config.custom.is_empty() {\n        log::debug!(\n            \"Loading custom prompts defined in config: {:?}\",\n            prompts_config.custom.keys()\n        );\n    }\n    for (name, prompt_text) in &prompts_config.custom {\n        if !prompt_text.trim().is_empty() {\n            let key = format!(\"custom:{}\", name);\n            resolved.insert(key.clone(), prompt_text.clone());\n            log::trace!(\"Loaded custom prompt: {}\", name);\n        } else {\n            log::trace!(\"Skipping empty custom prompt: {}\", name);\n        }\n    }\n\n    log::info!(\"Resolved {} prompts.\", resolved.len());\n    Ok(resolved)\n}\n"
    },
    {
      "path": "core/context.rs",
      "content": "use crate::config::{self, Config, ResolvedRules};\nuse crate::error::Result;\nuse crate::gather::{self, TreeNode}; // Corrected: Use gather::TreeNode\n// Removed unused import: use crate::output_formats::AiReadmeText;\nuse crate::output_formats::{FileContextInfo, SourceRepresentation, get_ai_readme_text};\nuse crate::system::SystemInfo;\nuse chrono::{DateTime, Utc};\nuse indexmap::IndexMap;\nuse log;\n#[cfg(feature = \"serde_support\")]\nuse serde::Serialize;\nuse std::collections::{HashMap, HashSet};\nuse std::path::{Path, PathBuf};\n\n#[derive(Debug, Clone, Default)]\n#[cfg_attr(feature = \"serde_support\", derive(Serialize))]\n#[cfg_attr(feature = \"serde_support\", serde(rename_all = \"camelCase\"))] // Or snake_case\npub struct ProjectContext {\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\", rename = \"aiReadme\")\n    )]\n    pub ai_readme: Option<String>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    pub project_name: Option<String>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    pub project_root: Option<String>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    pub system_info: Option<SystemInfo>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    pub meta: Option<HashMap<String, String>>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    pub docs: Option<Vec<FileContextInfo>>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    pub tree: Option<Vec<TreeNode>>, // This uses gather::TreeNode now\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    pub source: Option<SourceRepresentation>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"IndexMap::is_empty\")\n    )]\n    pub rules: IndexMap<String, Vec<String>>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    pub prompts: Option<HashMap<String, String>>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    pub generation_timestamp: Option<DateTime<Utc>>,\n\n    // Internal data not serialized\n    #[cfg_attr(feature = \"serde_support\", serde(skip))]\n    pub resolved_rules_debug: Option<ResolvedRules>, // Keep for debug command\n}\n\nimpl ProjectContext {\n    pub fn build(\n        project_root_path: &Path,\n        config: &Config,\n        tree_structure: Option<Vec<TreeNode>>,\n        project_characteristics: &HashSet<String>,\n    ) -> Result<Self> {\n        log::debug!(\"Building project context skeleton...\");\n\n        let sys_info = if config.output.include_system_info {\n            log::trace!(\"Gathering system info...\");\n            Some(crate::system::gather_system_info()?)\n        } else {\n            log::trace!(\"System info collection disabled.\");\n            None\n        };\n\n        let meta_map = if config.meta.enabled && !config.meta.custom_meta.is_empty() {\n            log::trace!(\"Preparing metadata...\");\n            Some(config.meta.custom_meta.clone())\n        } else {\n            log::trace!(\"Metadata disabled or empty.\");\n            None\n        };\n\n        log::trace!(\"Resolving rules...\");\n        let resolved_rules =\n            config::resolve_rules(&config.rules, project_root_path, project_characteristics)?;\n        let resolved_rules_debug_info = resolved_rules.clone();\n        log::trace!(\"Rules resolved.\");\n\n        log::trace!(\"Resolving prompts...\");\n        let prompts_section = match config::resolve_prompts(&config.prompts, project_root_path) {\n            Ok(map) if !map.is_empty() => Some(map),\n            _ => None,\n        };\n        log::trace!(\"Prompts resolved.\");\n\n        let mut context = ProjectContext {\n            ai_readme: None, // Will be populated later\n            project_name: if config.output.include_project_name {\n                Some(config.get_effective_project_name(project_root_path))\n            } else {\n                None\n            },\n            project_root: if config.output.include_project_root {\n                Some(project_root_path.to_string_lossy().to_string())\n            } else {\n                None\n            },\n            system_info: sys_info,\n            meta: meta_map,\n            docs: None, // Populated by add_docs\n            tree: if config.tree.enabled {\n                tree_structure\n            } else {\n                None\n            },\n            source: None, // Populated by add_files or add_chunk_paths\n            rules: resolved_rules.rulesets,\n            prompts: prompts_section,\n            generation_timestamp: if config.output.include_timestamp {\n                Some(Utc::now())\n            } else {\n                None\n            },\n            resolved_rules_debug: Some(resolved_rules_debug_info),\n        };\n\n        context.populate_ai_readme(config); // Populate initial readme\n\n        log::debug!(\"Context skeleton built successfully.\");\n        Ok(context)\n    }\n\n    fn create_file_context_list(\n        files_info: Vec<gather::FileInfo>,\n        project_root: &Path,\n    ) -> Vec<FileContextInfo> {\n        files_info\n            .into_iter()\n            .map(|finfo| FileContextInfo {\n                path: pathdiff::diff_paths(&finfo.path, project_root)\n                    .unwrap_or_else(|| finfo.path.clone()) // Fallback to absolute if diff fails\n                    .to_string_lossy()\n                    .to_string(),\n                content: finfo.content,\n            })\n            .collect()\n    }\n\n    pub fn add_files(\n        mut self,\n        source_files_info: Vec<gather::FileInfo>,\n        project_root: &Path,\n        config: &Config, // Needed to repopulate readme\n    ) -> Self {\n        if config.source.enabled && !source_files_info.is_empty() {\n            log::debug!(\n                \"Adding {} source files inline to context.\",\n                source_files_info.len()\n            );\n            self.source = Some(SourceRepresentation {\n                files: Some(Self::create_file_context_list(\n                    source_files_info,\n                    project_root,\n                )),\n                chunks: None,\n            });\n        } else if config.source.enabled {\n            log::debug!(\"No source files provided or found to add inline.\");\n            self.source = None; // Explicitly set to None if enabled but no files\n        } else {\n            log::debug!(\"Source section disabled, not adding files.\");\n            self.source = None;\n        }\n        self.populate_ai_readme(config); // Repopulate after potentially changing source\n        self\n    }\n\n    pub fn add_docs(\n        mut self,\n        docs_files_info: Vec<gather::FileInfo>,\n        project_root: &Path,\n        config: &Config, // Needed to repopulate readme\n    ) -> Self {\n        if config.docs.enabled && !docs_files_info.is_empty() {\n            log::debug!(\n                \"Adding {} documentation files to context.\",\n                docs_files_info.len()\n            );\n            self.docs = Some(Self::create_file_context_list(\n                docs_files_info,\n                project_root,\n            ));\n        } else if config.docs.enabled {\n            log::debug!(\"No documentation files provided or found.\");\n            self.docs = None;\n        } else {\n            log::debug!(\"Docs section disabled, not adding files.\");\n            self.docs = None;\n        }\n        self.populate_ai_readme(config); // Repopulate after potentially changing docs\n        self\n    }\n\n    pub fn add_chunk_paths(\n        mut self,\n        chunk_paths: Vec<PathBuf>,\n        save_dir: &Path,\n        config: &Config, // Needed to repopulate readme\n    ) -> Self {\n        if config.source.enabled && !chunk_paths.is_empty() {\n            log::debug!(\n                \"Adding {} chunk file references to context.\",\n                chunk_paths.len()\n            );\n            let relative_chunk_paths = chunk_paths\n                .into_iter()\n                .map(|p| {\n                    // Try to make path relative to save_dir, fallback to original path string\n                    pathdiff::diff_paths(&p, save_dir)\n                        .map(|rel_p| rel_p.to_string_lossy().to_string())\n                        .unwrap_or_else(|| p.to_string_lossy().to_string())\n                })\n                .collect();\n            self.source = Some(SourceRepresentation {\n                files: None,\n                chunks: Some(relative_chunk_paths),\n            });\n        } else if config.source.enabled {\n            log::debug!(\"No chunk paths provided.\");\n            // If chunking was expected but produced no files, source might be empty\n            self.source = None;\n        } else {\n            log::debug!(\"Source section disabled, not adding chunk paths.\");\n            self.source = None;\n        }\n        self.populate_ai_readme(config); // Repopulate after potentially changing source representation\n        self\n    }\n\n    pub fn populate_ai_readme(&mut self, config: &Config) {\n        let readme_template = get_ai_readme_text();\n        let mut parts: Vec<&str> = Vec::new();\n        parts.push(&readme_template.intro);\n\n        let mut details: Vec<&str> = Vec::new();\n        if self.project_name.is_some() {\n            details.push(&readme_template.project_name_desc);\n        }\n        if self.project_root.is_some() {\n            details.push(&readme_template.project_root_desc);\n        }\n        if self.system_info.is_some() {\n            details.push(&readme_template.system_info_desc);\n        }\n        if self.meta.is_some() {\n            details.push(&readme_template.meta_desc);\n        }\n        if self.docs.is_some() {\n            details.push(&readme_template.docs_desc);\n        }\n        if self.tree.is_some() {\n            details.push(&readme_template.tree_desc);\n        }\n\n        if let Some(source_repr) = &self.source {\n            if source_repr.files.is_some() {\n                details.push(&readme_template.source_files_desc);\n            } else if source_repr.chunks.is_some() {\n                details.push(&readme_template.source_chunks_desc);\n            } else {\n                // Should not happen if source is Some, but handle defensively\n                details.push(&readme_template.source_missing_desc);\n            }\n        } else if config.source.enabled {\n            // Source enabled but no files/chunks added yet or found\n            details.push(&readme_template.source_missing_desc);\n        }\n        // No else needed if source is disabled\n\n        if !self.rules.is_empty() {\n            details.push(&readme_template.rules_desc);\n        } else if config.rules.enabled {\n            // Rules enabled but none resolved/found\n            details.push(&readme_template.rules_missing_desc);\n        }\n        // No else needed if rules are disabled\n\n        if self.generation_timestamp.is_some() {\n            details.push(&readme_template.timestamp_desc);\n        }\n\n        if !details.is_empty() {\n            parts.push(&readme_template.key_sections_header);\n            parts.extend(details);\n            self.ai_readme = Some(parts.join(\"\\\\n\"));\n        } else {\n            // Fallback if somehow no sections are included\n            self.ai_readme = Some(readme_template.intro.clone());\n        }\n        log::trace!(\"AI Readme populated.\");\n    }\n}\n"
    },
    {
      "path": "core/core.rs",
      "content": "pub mod chunking;\npub mod config;\npub mod context;\npub mod error;\npub mod gather;\npub mod output_formats;\npub mod rules;\npub mod system;\n\npub use config::{Config, MetaConfig, PromptsConfig, ResolvedRules, RulesConfig};\npub use context::ProjectContext;\npub use error::{AppError, Result};\npub use gather::{FileInfo, TreeNode, gather_files_and_tree}; // Ensure TreeNode is re-exported\npub use output_formats::{\n    AiReadmeText, BuiltinIgnores, ChunkFile, ChunkInfo, FileContextInfo, SourceRepresentation,\n    TextType, get_ai_readme_text, get_builtin_ignore_patterns, get_predefined_text,\n};\npub use rules::{detect_project_characteristics, get_static_rule_content};\npub use system::{SystemInfo, gather_system_info};\n"
    },
    {
      "path": "core/error.rs",
      "content": "use std::path::PathBuf;\nuse thiserror::Error;\n\npub type Result<T, E = AppError> = std::result::Result<T, E>;\n\n#[derive(Error, Debug)]\n#[non_exhaustive]\npub enum AppError {\n    #[error(\"Configuration Error: {0}\")]\n    Config(String),\n\n    #[error(\"TOML Parsing Error: {0}\")]\n    TomlParse(String),\n\n    #[error(\"TOML Serialization Error: {0}\")]\n    TomlSerialize(#[from] toml::ser::Error),\n\n    #[error(\"JSON Serialization Error: {0}\")]\n    JsonSerialize(#[from] serde_json::Error),\n\n    #[error(\"YAML Parsing/Serialization Error: {0}\")]\n    YamlError(#[from] serde_yml::Error),\n\n    #[error(\"XML Serialization/Deserialization Error: {0}\")]\n    XmlSerialize(String),\n\n    #[error(\"Filesystem Error: {0}\")]\n    Io(#[from] std::io::Error),\n\n    #[error(\"File Read Error: Path '{path}', Error: {source}\")]\n    FileRead {\n        path: PathBuf,\n        #[source]\n        source: std::io::Error,\n    },\n\n    #[error(\"File Write Error: Path '{path}', Error: {source}\")]\n    FileWrite {\n        path: PathBuf,\n        #[source]\n        source: std::io::Error,\n    },\n\n    #[error(\"Directory Creation Error: Path '{path}', Error: {source}\")]\n    DirCreation {\n        path: PathBuf,\n        #[source]\n        source: std::io::Error,\n    },\n\n    #[error(\"WalkDir Error: {0}\")]\n    WalkDir(String),\n\n    #[error(\"Ignore Error: {0}\")]\n    Ignore(#[from] ignore::Error),\n\n    #[error(\"Glob Pattern Error: {0}\")]\n    Glob(String),\n\n    #[error(\"Chunking Error: {0}\")]\n    Chunking(String),\n\n    #[error(\"System Info Error: {0}\")]\n    SystemInfo(String),\n\n    #[error(\"Invalid Argument: {0}\")]\n    InvalidArgument(String),\n\n    #[error(\"TikToken Error: {0}\")]\n    TikToken(String),\n\n    #[error(\"Rule Loading Error: {0}\")]\n    RuleLoading(String),\n\n    #[error(\"Data Loading Error: {0}\")]\n    DataLoading(String),\n\n    #[error(\"Duration Parsing Error: {0}\")]\n    DurationParse(String),\n}\n\n#[cfg(feature = \"serde_support\")]\nimpl From<quick_xml::se::SeError> for AppError {\n    fn from(err: quick_xml::se::SeError) -> Self {\n        AppError::XmlSerialize(err.to_string())\n    }\n}\n#[cfg(feature = \"serde_support\")]\nimpl From<quick_xml::DeError> for AppError {\n    fn from(err: quick_xml::DeError) -> Self {\n        AppError::XmlSerialize(err.to_string())\n    }\n}\n\nimpl From<globset::Error> for AppError {\n    fn from(err: globset::Error) -> Self {\n        AppError::Glob(format!(\"Globset error: {}\", err))\n    }\n}\n\nimpl From<walkdir::Error> for AppError {\n    fn from(err: walkdir::Error) -> Self {\n        AppError::WalkDir(err.to_string())\n    }\n}\n\nimpl From<std::str::Utf8Error> for AppError {\n    fn from(err: std::str::Utf8Error) -> Self {\n        AppError::DataLoading(format!(\"UTF-8 decoding error: {}\", err))\n    }\n}\n\nimpl From<parse_duration::parse::Error> for AppError {\n    fn from(err: parse_duration::parse::Error) -> Self {\n        AppError::DurationParse(err.to_string())\n    }\n}\n"
    },
    {
      "path": "core/gather.rs",
      "content": "use crate::config::Config;\nuse crate::error::{AppError, Result};\nuse crate::output_formats::get_builtin_ignore_patterns; // Keep this import\nuse globset::{Glob, GlobSet, GlobSetBuilder};\nuse ignore::{WalkBuilder, WalkState};\nuse log;\nuse rayon::prelude::*;\n#[cfg(feature = \"serde_support\")] // Corrected newline before this line\nuse serde::Serialize;\nuse std::fs;\nuse std::path::{Component, Path, PathBuf};\nuse std::sync::mpsc;\n\n#[derive(Debug, Clone)]\npub struct FileInfo {\n    pub path: PathBuf,\n    pub content: String,\n    pub size: usize,\n}\n\n// Corrected: Made TreeNode public and conditional compilation for Serialize\n#[derive(Debug, Clone)]\n#[cfg_attr(feature = \"serde_support\", derive(Serialize))]\npub struct TreeNode {\n    name: String,\n    #[cfg_attr(feature = \"serde_support\", serde(rename = \"type\"))]\n    node_type: String,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\") // Corrected syntax is fine here\n    )]\n    children: Option<Vec<TreeNode>>,\n}\n\npub fn gather_files_and_tree(\n    project_root: &Path,\n    config: &Config,\n    quiet: bool, // Keep quiet for conditional logging\n) -> Result<(Vec<FileInfo>, Vec<FileInfo>, Vec<(String, bool)>)> {\n    log::debug!(\"Starting file and tree gathering process...\");\n    let tree_include_patterns = config.get_effective_include(&config.tree.include);\n    let tree_exclude_patterns = config.get_effective_exclude(&config.tree.exclude);\n    let source_include_patterns = config.get_effective_include(&config.source.include);\n    let source_exclude_patterns = config.get_effective_exclude(&config.source.exclude);\n    let docs_include_patterns = config.get_effective_include(&config.docs.include);\n    let docs_exclude_patterns = config.get_effective_exclude(&config.docs.exclude);\n\n    log::trace!(\"Building glob sets for filtering...\");\n    let tree_include_set = build_glob_set_from_vec(tree_include_patterns)?;\n    let tree_exclude_set = build_glob_set_from_vec(tree_exclude_patterns)?;\n    let has_tree_includes = !tree_include_patterns.is_empty();\n\n    let source_include_set = build_glob_set_from_vec(source_include_patterns)?;\n    let source_exclude_set = build_glob_set_from_vec(source_exclude_patterns)?;\n    let has_source_includes = !source_include_patterns.is_empty();\n\n    let docs_active = config.is_docs_section_active();\n    let docs_include_set = if docs_active {\n        build_glob_set_from_vec(docs_include_patterns)?\n    } else {\n        GlobSet::empty()\n    };\n    let docs_exclude_set = if docs_active {\n        build_glob_set_from_vec(docs_exclude_patterns)?\n    } else {\n        GlobSet::empty()\n    };\n    let has_docs_includes = docs_active && !docs_include_patterns.is_empty();\n\n    let builtin_ignores = get_builtin_ignore_patterns();\n    let common_builtin_exclude_set = build_glob_set_from_vec(&builtin_ignores.common)?;\n    let tree_builtin_exclude_set = build_glob_set_from_vec(&builtin_ignores.tree)?;\n    let source_builtin_exclude_set = build_glob_set_from_vec(&builtin_ignores.source)?;\n    let docs_builtin_exclude_set = build_glob_set_from_vec(&builtin_ignores.docs)?;\n    let use_builtin_ignores = config.get_effective_builtin_ignore();\n    log::trace!(\"Glob sets built successfully.\");\n\n    let mut builder = WalkBuilder::new(project_root);\n    builder.threads(rayon::current_num_threads().min(12));\n    builder.hidden(false); // Consider making this configurable?\n\n    let use_global_gitignore = config.general.use_gitignore;\n    builder.ignore(use_global_gitignore);\n    builder.git_ignore(use_global_gitignore);\n    builder.git_exclude(use_global_gitignore);\n    builder.require_git(false);\n    log::debug!(\n        \"WalkBuilder configured (gitignore: {}, builtin: {})\",\n        use_global_gitignore,\n        use_builtin_ignores\n    );\n\n    let walker = builder.build_parallel();\n    let project_root_clone = project_root.to_path_buf();\n\n    #[derive(Debug)]\n    struct WalkedPathInfo {\n        path: PathBuf,\n        relative_path: PathBuf,\n        is_dir: bool,\n    }\n    let (tx_walked, rx_walked) = mpsc::channel::<WalkedPathInfo>();\n    let tx_for_closure = tx_walked.clone();\n\n    log::info!(\"Walking project directory: {}\", project_root.display());\n    walker.run(move || {\n        let tx_thread = tx_for_closure.clone();\n        let proj_root = project_root_clone.clone();\n\n        Box::new(move |entry_result| {\n            match entry_result {\n                Ok(entry) => {\n                    let path = entry.path();\n                    if entry.depth() == 0 {\n                        return WalkState::Continue;\n                    }\n                    // Skip cache directory explicitly if walkbuilder doesn't handle it\n                    if path.strip_prefix(&proj_root).map_or(false, |rel| {\n                        rel.starts_with(crate::config::DEFAULT_CACHE_DIR) // Use constant\n                    }) {\n                        log::trace!(\"Skipping cache directory: {}\", path.display());\n                        return WalkState::Skip;\n                    }\n\n                    if let Some(relative_path) = pathdiff::diff_paths(path, &proj_root) {\n                        let is_dir = entry.file_type().map_or(false, |ft| ft.is_dir());\n                        log::trace!(\"Walked path: {}\", relative_path.display());\n                        if tx_thread\n                            .send(WalkedPathInfo {\n                                path: path.to_path_buf(),\n                                relative_path,\n                                is_dir,\n                            })\n                            .is_err()\n                        {\n                            log::error!(\"Receiver dropped for walked paths, stopping walk early.\");\n                            return WalkState::Quit;\n                        }\n                    } else {\n                        log::warn!(\"Could not get relative path for: {}\", path.display());\n                    }\n                }\n                Err(e) => {\n                    log::warn!(\"Error walking directory: {}\", e);\n                }\n            }\n            WalkState::Continue\n        })\n    });\n    drop(tx_walked);\n\n    let walked_paths: Vec<WalkedPathInfo> = rx_walked.into_iter().collect();\n    log::info!(\n        \"Directory walk complete. Found {} potential paths.\",\n        walked_paths.len()\n    );\n\n    log::debug!(\"Filtering walked paths based on configuration...\");\n    let mut tree_candidates = Vec::<(String, bool)>::new();\n    let mut source_file_paths = Vec::<PathBuf>::new();\n    let mut docs_file_paths = Vec::<PathBuf>::new();\n    let mut file_read_errors = Vec::<AppError>::new(); // Collect errors\n\n    for walked_info in walked_paths {\n        let relative_path = &walked_info.relative_path;\n        let absolute_path = &walked_info.path;\n        let is_dir = walked_info.is_dir;\n\n        // Explicitly skip .git\n        if relative_path.components().next() == Some(std::path::Component::Normal(\".git\".as_ref()))\n        // Corrected: \".git\" is fine here\n        {\n            log::trace!(\n                \"Explicitly skipping path within .git: {}\",\n                relative_path.display()\n            );\n            continue;\n        }\n\n        let tree_git_ignore = config.get_effective_gitignore(&config.tree.use_gitignore);\n        let docs_git_ignore = config.get_effective_gitignore(&config.docs.use_gitignore);\n        let source_git_ignore = config.get_effective_gitignore(&config.source.use_gitignore);\n\n        let include_in_tree = config.tree.enabled\n            && should_include(\n                relative_path,\n                is_dir,\n                &tree_include_set,\n                has_tree_includes,\n                &tree_exclude_set,\n                tree_git_ignore,\n                project_root, // Pass project root if needed by gitignore logic internally\n                use_builtin_ignores,\n                &common_builtin_exclude_set,\n                &tree_builtin_exclude_set,\n            );\n\n        let include_in_docs = !is_dir\n            && docs_active\n            && should_include(\n                relative_path,\n                false, // is_dir is false for files\n                &docs_include_set,\n                has_docs_includes,\n                &docs_exclude_set,\n                docs_git_ignore,\n                project_root,\n                use_builtin_ignores,\n                &common_builtin_exclude_set,\n                &docs_builtin_exclude_set,\n            );\n\n        let include_in_source = !is_dir\n            && !include_in_docs // Don't include if it's already a doc file\n            && config.source.enabled\n            && should_include(\n                relative_path,\n                false, // is_dir is false for files\n                &source_include_set,\n                has_source_includes,\n                &source_exclude_set,\n                source_git_ignore,\n                project_root,\n                use_builtin_ignores,\n                &common_builtin_exclude_set,\n                &source_builtin_exclude_set,\n            );\n\n        if include_in_tree {\n            log::trace!(\"Including in tree: {}\", relative_path.display());\n            tree_candidates.push((relative_path.to_string_lossy().into_owned(), is_dir));\n        }\n\n        if include_in_docs {\n            log::trace!(\"Including in docs: {}\", relative_path.display());\n            docs_file_paths.push(absolute_path.clone());\n        } else if include_in_source {\n            log::trace!(\"Including in source: {}\", relative_path.display());\n            source_file_paths.push(absolute_path.clone());\n        } else if !is_dir && !include_in_tree {\n            // Only log file exclusions if not included in tree\n            log::trace!(\"Excluding file: {}\", relative_path.display());\n        }\n    }\n    log::debug!(\"Path filtering complete.\");\n\n    log::info!(\n        \"Reading content for {} source files and {} docs files...\",\n        source_file_paths.len(),\n        docs_file_paths.len()\n    );\n\n    let read_files = |paths: Vec<PathBuf>| -> (Vec<FileInfo>, Vec<AppError>) {\n        let results: Vec<_> = paths\n            .into_par_iter()\n            .map(|path| match fs::read(&path) {\n                Ok(bytes) => {\n                    let size = bytes.len();\n                    match String::from_utf8(bytes) {\n                        Ok(content) => Ok(FileInfo {\n                            path,\n                            content,\n                            size,\n                        }),\n                        Err(e) => {\n                            log::debug!(\"Skipping non-UTF-8 file: {} ({})\", path.display(), e);\n                            Err(AppError::DataLoading(format!(\n                                \"Skipped non-UTF-8 file: {}\",\n                                path.display()\n                            )))\n                        }\n                    }\n                }\n                Err(e) => Err(AppError::FileRead {\n                    path: path.clone(),\n                    source: e,\n                }),\n            })\n            .collect();\n\n        let mut files = Vec::new();\n        let mut errors = Vec::new();\n        for res in results {\n            match res {\n                Ok(info) => files.push(info),\n                Err(AppError::DataLoading(_)) => { /* Already logged, skip */ }\n                Err(e) => errors.push(e),\n            }\n        }\n        (files, errors)\n    };\n\n    let (mut final_source_files, source_errors) = read_files(source_file_paths);\n    let (mut final_docs_files, docs_errors) = read_files(docs_file_paths);\n    file_read_errors.extend(source_errors);\n    file_read_errors.extend(docs_errors);\n    log::info!(\"File reading complete.\");\n\n    // Sort results for deterministic output\n    final_source_files.par_sort_unstable_by(|a, b| a.path.cmp(&b.path));\n    final_docs_files.par_sort_unstable_by(|a, b| a.path.cmp(&b.path));\n    tree_candidates.par_sort_unstable_by(|a, b| a.0.cmp(&b.0));\n\n    // Report errors gathered during file reading if not quiet\n    if !file_read_errors.is_empty() && !quiet {\n        use colored::Colorize; // Only needed here\n        eprintln!(\n            // Corrected: Print a newline character here, not literal `\\n`\n            \"\\n{}\",\n            \"⚠️ Warning: Errors encountered during file reading:\".yellow()\n        );\n        for err in file_read_errors {\n            eprintln!(\" - {}\", err);\n        }\n        eprintln!(\"---\");\n    }\n\n    Ok((final_source_files, final_docs_files, tree_candidates))\n}\n\nfn build_glob_set_from_vec(patterns: &[String]) -> Result<GlobSet> {\n    let mut builder = GlobSetBuilder::new();\n    for pattern_str in patterns {\n        let mut processed_pattern = pattern_str.trim().to_string();\n        if processed_pattern.ends_with('/') && processed_pattern.len() > 1 {\n            processed_pattern.push_str(\"**\");\n        }\n        match Glob::new(&processed_pattern) {\n            Ok(glob) => {\n                log::trace!(\n                    \"Adding glob pattern: {} (processed as {})\",\n                    pattern_str,\n                    processed_pattern\n                );\n                builder.add(glob);\n            }\n            Err(e) => {\n                // Corrected: Use double quotes for format string\n                log::error!(\"Invalid glob pattern \\\"{}\\\": {}\", pattern_str, e);\n                return Err(AppError::Glob(format!(\n                    // Corrected: Use double quotes for format string\n                    \"Invalid glob pattern \\\"{}\\\" (processed as \\\"{}\\\"): {}\",\n                    pattern_str, processed_pattern, e\n                )));\n            }\n        }\n    }\n    builder.build().map_err(|e| {\n        log::error!(\"Error building glob set: {}\", e);\n        AppError::Glob(e.to_string())\n    })\n}\n\nfn should_include(\n    relative_path: &Path,\n    is_dir: bool,\n    include_set: &GlobSet,\n    has_includes: bool, // True if include patterns were provided\n    exclude_set: &GlobSet,\n    _use_gitignore: bool, // Handled by WalkBuilder, keep param for signature consistency?\n    _project_root: &Path, // Potentially needed if gitignore logic were here\n    use_builtin: bool,\n    common_builtin_exclude: &GlobSet,\n    section_builtin_exclude: &GlobSet,\n) -> bool {\n    // 1. Check Explicit Excludes\n    if exclude_set.is_match(relative_path)\n        || (is_dir && exclude_set.is_match(relative_path.join(\"dummy_file_for_dir_match\")))\n    {\n        log::trace!(\n            \"Path excluded by explicit exclude set: {}\",\n            relative_path.display()\n        );\n        return false;\n    }\n\n    // 2. Check Explicit Includes (if any were provided)\n    // Check both file and potential directory match for includes\n    let included_explicitly = !has_includes\n        || include_set.is_match(relative_path)\n        || (is_dir && include_set.is_match(relative_path.join(\"dummy_file_for_dir_match\")));\n\n    if !included_explicitly {\n        log::trace!(\n            \"Path not included by explicit include set: {}\",\n            relative_path.display()\n        );\n        return false;\n    }\n\n    // 3. Gitignore filtering is handled by the WalkBuilder itself\n\n    // 4. Check Built-in Ignores\n    if use_builtin {\n        if common_builtin_exclude.is_match(relative_path)\n            || (is_dir\n                && common_builtin_exclude.is_match(relative_path.join(\"dummy_file_for_dir_match\")))\n        {\n            log::trace!(\n                \"Path excluded by common built-in ignores: {}\",\n                relative_path.display()\n            );\n            return false;\n        }\n        if section_builtin_exclude.is_match(relative_path)\n            || (is_dir\n                && section_builtin_exclude.is_match(relative_path.join(\"dummy_file_for_dir_match\")))\n        {\n            log::trace!(\n                \"Path excluded by section built-in ignores: {}\",\n                relative_path.display()\n            );\n            return false;\n        }\n    }\n\n    // If not excluded by any rule, include it\n    log::trace!(\"Path included: {}\", relative_path.display());\n    true\n}\n\npub fn build_tree_from_paths(relative_path_types: &[(String, bool)]) -> Result<Vec<TreeNode>> {\n    log::debug!(\n        \"Building tree structure from {} paths...\",\n        relative_path_types.len()\n    );\n    let mut root_nodes: Vec<TreeNode> = Vec::new();\n\n    for (rel_path_str, is_dir) in relative_path_types {\n        let rel_path = PathBuf::from(rel_path_str);\n        let components: Vec<String> = rel_path\n            .components()\n            .filter_map(|c| match c {\n                Component::Normal(name) => Some(name.to_string_lossy().into_owned()),\n                _ => None,\n            })\n            .collect();\n\n        if !components.is_empty() {\n            if let Err(e) = insert_node(&mut root_nodes, &components, *is_dir) {\n                // Corrected: Use double quotes for format string\n                log::error!(\n                    \"Error inserting node into tree for path \\\"{}\\\": {}\",\n                    rel_path_str,\n                    e\n                );\n                // Optionally bubble up the error: return Err(e);\n            }\n        }\n    }\n\n    // Sort root nodes alphabetically\n    root_nodes.sort_by(|a, b| a.name.cmp(&b.name));\n    log::debug!(\"Tree structure built successfully.\");\n    Ok(root_nodes)\n}\n\nfn insert_node(\n    current_level_nodes: &mut Vec<TreeNode>,\n    components: &[String],\n    is_dir_at_end: bool, // True if the original path was a directory\n) -> Result<()> {\n    if components.is_empty() {\n        return Ok(());\n    }\n\n    let component_name = &components[0];\n    let remaining_components = &components[1..];\n    let is_last_component = remaining_components.is_empty();\n\n    match current_level_nodes.binary_search_by(|node| node.name.cmp(component_name)) {\n        Ok(index) => {\n            let existing_node = &mut current_level_nodes[index];\n\n            // If we find an existing node:\n            // - Ensure it's marked as a directory if we need to descend further\n            // - Ensure it's marked as a directory if the original path was a dir at this level\n            if !is_last_component {\n                if existing_node.node_type == \"file\" {\n                    // Conflict: Trying to add children to a file node\n                    return Err(AppError::Config(format!(\n                        \"Tree conflict: Trying to create children within file component {}\",\n                        component_name\n                    )));\n                }\n                // Ensure children structure exists if descending\n                if existing_node.children.is_none() {\n                    existing_node.children = Some(Vec::new());\n                }\n                insert_node(\n                    existing_node.children.as_mut().unwrap(),\n                    remaining_components,\n                    is_dir_at_end,\n                )?;\n                // Keep children sorted\n                existing_node\n                    .children\n                    .as_mut()\n                    .unwrap()\n                    .sort_by(|a, b| a.name.cmp(&b.name));\n            } else if is_dir_at_end && existing_node.node_type == \"file\" {\n                // Update node type if the full path indicates it's a directory\n                existing_node.node_type = \"directory\".to_string();\n                // If it's now a directory, ensure it can have children (even if empty for now)\n                if existing_node.children.is_none() {\n                    existing_node.children = Some(Vec::new());\n                }\n            }\n            // If is_last_component and !is_dir_at_end, no change needed if existing is file or dir.\n            // If is_last_component and is_dir_at_end and existing is already dir, no change needed.\n        }\n        Err(insertion_point) => {\n            // Node doesn't exist, create it\n            let node_type_str = if is_last_component {\n                if is_dir_at_end { \"directory\" } else { \"file\" }\n            } else {\n                // If not the last component, it must be an intermediate directory\n                \"directory\"\n            };\n\n            let mut new_node = TreeNode {\n                name: component_name.clone(),\n                node_type: node_type_str.to_string(),\n                children: if node_type_str == \"directory\" {\n                    Some(Vec::new())\n                } else {\n                    None\n                },\n            };\n\n            if !is_last_component {\n                // Must be a directory if not last, insert remaining components recursively\n                insert_node(\n                    new_node.children.as_mut().unwrap(), // Safe because type is \"directory\"\n                    remaining_components,\n                    is_dir_at_end,\n                )?;\n                // Keep children sorted\n                new_node\n                    .children\n                    .as_mut()\n                    .unwrap()\n                    .sort_by(|a, b| a.name.cmp(&b.name));\n            }\n\n            current_level_nodes.insert(insertion_point, new_node);\n        }\n    }\n    Ok(())\n}\n"
    },
    {
      "path": "core/output_formats.rs",
      "content": "use crate::error::{AppError, Result};\nuse once_cell::sync::Lazy;\n#[cfg(feature = \"serde_support\")]\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\n#[derive(Debug, Clone, PartialEq)]\n#[cfg_attr(feature = \"serde_support\", derive(Serialize, Deserialize))]\n#[cfg_attr(feature = \"serde_support\", serde(rename_all = \"camelCase\"))]\npub struct FileContextInfo {\n    pub path: String,\n    pub content: String,\n}\n\n#[derive(Debug, Clone, PartialEq, Default)]\n#[cfg_attr(feature = \"serde_support\", derive(Serialize))]\n#[cfg_attr(feature = \"serde_support\", serde(rename_all = \"camelCase\"))]\npub struct SourceRepresentation {\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    pub files: Option<Vec<FileContextInfo>>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    pub chunks: Option<Vec<String>>,\n}\n\n#[derive(Debug, Clone, PartialEq)]\n#[cfg_attr(feature = \"serde_support\", derive(Serialize))]\n#[cfg_attr(feature = \"serde_support\", serde(rename_all = \"camelCase\"))]\npub struct ChunkInfo {\n    pub current_part: usize,\n    pub total_parts: usize,\n}\n\n#[derive(Debug, Clone, PartialEq)]\n#[cfg_attr(feature = \"serde_support\", derive(Serialize))]\n#[cfg_attr(feature = \"serde_support\", serde(rename_all = \"camelCase\"))]\npub struct ChunkFile {\n    pub files: Vec<FileContextInfo>,\n    pub chunk_info: ChunkInfo,\n}\n\n#[derive(Debug)]\n#[cfg_attr(feature = \"serde_support\", derive(Deserialize))]\npub struct AiReadmeText {\n    pub intro: String,\n    pub key_sections_header: String,\n    pub project_name_desc: String,\n    pub project_root_desc: String,\n    pub system_info_desc: String,\n    pub meta_desc: String,\n    pub docs_desc: String,\n    pub tree_desc: String,\n    pub source_files_desc: String,\n    pub source_chunks_desc: String,\n    pub source_missing_desc: String,\n    pub rules_desc: String,\n    pub rules_missing_desc: String,\n    pub timestamp_desc: String,\n}\n#[derive(Debug, Default)]\n#[cfg_attr(feature = \"serde_support\", derive(Deserialize))]\npub struct BuiltinIgnores {\n    #[serde(default)]\n    pub common: Vec<String>,\n    #[serde(default)]\n    pub tree: Vec<String>,\n    #[serde(default)]\n    pub source: Vec<String>,\n    #[serde(default)]\n    pub docs: Vec<String>,\n}\n\nstatic PREDEFINED_PROMPTS: Lazy<HashMap<String, String>> = Lazy::new(|| {\n    // Corrected path: \"../data/\"\n    let yaml_content = include_str!(concat!(env!(\"CARGO_MANIFEST_DIR\"), \"/../data/prompts.yaml\"));\n    serde_yml::from_str(yaml_content).expect(\"Failed to parse embedded data/prompts.yaml\")\n});\nstatic AI_README_TEXT: Lazy<AiReadmeText> = Lazy::new(|| {\n    // Corrected path: \"../data/\"\n    let yaml_content = include_str!(concat!(\n        env!(\"CARGO_MANIFEST_DIR\"),\n        \"/../data/ai_readme.yaml\"\n    ));\n    serde_yml::from_str(yaml_content).expect(\"Failed to parse embedded data/ai_readme.yaml\")\n});\nstatic BUILTIN_IGNORE_PATTERNS: Lazy<BuiltinIgnores> = Lazy::new(|| {\n    // Corrected path: \"../data/\"\n    let yaml_content = include_str!(concat!(\n        env!(\"CARGO_MANIFEST_DIR\"),\n        \"/../data/builtin_ignores.yaml\"\n    ));\n    serde_yml::from_str(yaml_content).expect(\"Failed to parse embedded data/builtin_ignores.yaml\")\n});\n\npub fn get_predefined_prompts() -> &'static HashMap<String, String> {\n    &PREDEFINED_PROMPTS\n}\npub fn get_ai_readme_text() -> &'static AiReadmeText {\n    &AI_README_TEXT\n}\npub fn get_builtin_ignore_patterns() -> &'static BuiltinIgnores {\n    &BUILTIN_IGNORE_PATTERNS\n}\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum TextType {\n    Prompt,\n}\npub fn get_predefined_text(name: &str, text_type: TextType) -> Result<String> {\n    let map = match text_type {\n        TextType::Prompt => get_predefined_prompts(),\n    };\n    match map.get(name) {\n        Some(text) => Ok(text.clone()),\n        None => {\n            let type_str = match text_type {\n                TextType::Prompt => \"prompt\",\n            };\n            Err(AppError::Config(format!(\n                \"Predefined {} name \\\\\\\"{}\\\\\\\" specified in config not found.\",\n                type_str, name\n            )))\n        }\n    }\n}\n\n#[cfg(feature = \"serde_support\")]\npub fn serialize_context_to_json<T: Serialize>(\n    context: &T,\n    pretty: bool,\n) -> Result<String, AppError> {\n    if pretty {\n        serde_json::to_string_pretty(context).map_err(AppError::JsonSerialize)\n    } else {\n        serde_json::to_string(context).map_err(AppError::JsonSerialize)\n    }\n}\n\n#[cfg(feature = \"serde_support\")]\npub fn serialize_context_to_yaml<T: Serialize>(context: &T) -> Result<String, AppError> {\n    serde_yml::to_string(context).map_err(AppError::YamlError)\n}\n\n#[cfg(feature = \"serde_support\")]\npub fn serialize_context_to_xml<T: Serialize>(\n    context: &T,\n    root_name: &str,\n    _pretty: bool, // Mark pretty as unused for now\n) -> Result<String, AppError> {\n    // Use the simpler helper function which avoids manual Serializer creation\n    quick_xml::se::to_string_with_root(root_name, context)\n        .map_err(|e| AppError::XmlSerialize(e.to_string()))\n\n    // --- Keep the manual code commented out in case we need pretty printing later ---\n    /*\n    use quick_xml::se::Serializer;\n    use quick_xml::Writer;\n    use std::io::Cursor;\n\n    let mut buf = Vec::new();\n    // Create the writer wrapping the buffer\n    let mut writer = if pretty {\n        Writer::new_with_indent(Cursor::new(&mut buf), b' ', 4)\n    } else {\n        Writer::new(Cursor::new(&mut buf))\n    };\n\n    // This block is likely causing the trait bound issue\n    { // Scope might help, but maybe not needed with to_string_with_root\n        let mut ser = Serializer::with_root(&mut writer, Some(root_name))?;\n        context.serialize(ser)?;\n    } // End of scope for ser\n\n    // Retrieve the buffer content\n    // NOTE: This might need adjustment depending on how `writer`'s state is managed\n    // after `ser` is dropped or consumed by `serialize`.\n    // Let's assume `buf` directly holds the data for now if not using the helper.\n    // let final_buf = writer.into_inner().into_inner().to_owned();\n    String::from_utf8(buf).map_err(|e| AppError::XmlSerialize(e.to_string()))\n    */\n}\n"
    },
    {
      "path": "core/rules/mapping.rs",
      "content": "// src/rules/mapping.rs\nuse std::collections::HashSet;\n\n// Maps file extensions (lowercase) or exact filenames to rule stems\n// This function determines which static rule file (e.g., \"rust.org\")\n// might be relevant based on a file characteristic found in the project.\npub fn map_characteristic_to_rule_stem(characteristic: &str) -> Option<&'static str> {\n    match characteristic {\n        // File Extensions (matched lowercase)\n        \"rs\" => Some(\"rust\"),\n        \"rb\" => Some(\"ruby\"),\n        \"c\" | \"h\" => Some(\"c\"),\n        \"cpp\" | \"hpp\" => Some(\"cpp\"),\n        \"go\" => Some(\"go\"),\n        \"js\" | \"cjs\" | \"mjs\" | \"jsx\" => Some(\"javascript\"),\n        \"ts\" | \"tsx\" => Some(\"typescript\"),\n        \"php\" => Some(\"php\"),\n        \"org\" | \"md\" => Some(\"documentation\"), // Org/Markdown files trigger documentation rules\n        \"json\" | \"yaml\" | \"yml\" | \"toml\" | \"xml\" => Some(\"config_file\"), // Config files trigger config rules\n        \"rake\" => Some(\"rakefile\"), // Files with .rake extension\n\n        // Filenames (exact match - case sensitivity respected here)\n        \"Rakefile\" => Some(\"rakefile\"),\n        \"Gemfile\" => Some(\"ruby\"), // Gemfile also implies ruby rules\n        // Add more specific filename mappings here if needed (e.g., \"Cargo.toml\" -> \"rust\"?)\n        _ => None, // No known rule stem for this characteristic\n    }\n}\n\n// Defines the default set of rule stems that are always included\n// by default, unless explicitly excluded in the user's config.\npub fn get_default_rule_stems() -> HashSet<&'static str> {\n    // Use an array literal and convert to HashSet for clarity\n    [\"general\", \"guidelines\", \"documentation\"]\n        .iter()\n        .cloned()\n        .collect()\n}\n"
    },
    {
      "path": "core/rules.rs",
      "content": "use crate::error::{AppError as Error, Result};\nuse log;\nuse rust_embed::RustEmbed; // Added use statement\nuse std::collections::HashSet;\nuse std::path::Path;\nuse walkdir::WalkDir;\n\npub mod mapping; // Keep this declaration\n\n#[derive(RustEmbed)]\n#[folder = \"../data/rules/\"] // Corrected path relative to core crate root\n#[prefix = \"rules/\"] // Keep prefix for access path\nstruct StaticRuleAssets;\n\npub fn get_static_rule_content(rule_stem: &str) -> Result<String> {\n    let file_path = format!(\"rules/{}.org\", rule_stem);\n    log::trace!(\"Attempting to get embedded static rule: {}\", file_path);\n    // Now StaticRuleAssets::get should work because RustEmbed trait is in scope\n    let asset = StaticRuleAssets::get(&file_path).ok_or_else(|| {\n        Error::RuleLoading(format!(\n            \"Static rule file not found in embed: {}\",\n            file_path\n        ))\n    })?;\n    let content = std::str::from_utf8(asset.data.as_ref()).map_err(|e| {\n        Error::RuleLoading(format!(\"UTF-8 error in embedded rule {}: {}\", file_path, e))\n    })?;\n    Ok(content.to_string())\n}\n\npub fn detect_project_characteristics(project_root: &Path) -> Result<HashSet<String>> {\n    let mut characteristics = HashSet::new();\n    log::debug!(\n        \"Detecting project characteristics in: {}\",\n        project_root.display()\n    );\n    let walker = WalkDir::new(project_root).follow_links(false); //.max_depth(3); // Consider limiting depth\n\n    for entry_result in walker {\n        match entry_result {\n            Ok(entry) => {\n                if entry.file_type().is_file() {\n                    let path = entry.path();\n                    if let Some(filename) = path.file_name().and_then(|n| n.to_str()) {\n                        match filename {\n                            // Specific filenames implying characteristics\n                            \"Rakefile\" | \"Gemfile\" | \"Cargo.toml\" | \"package.json\"\n                            | \"composer.json\" | \"go.mod\" | \"Makefile\" => {\n                                log::trace!(\"Detected characteristic (filename): {}\", filename);\n                                characteristics.insert(filename.to_string());\n                            }\n                            _ => {}\n                        }\n                    }\n                    if let Some(extension) = path.extension().and_then(|ext| ext.to_str()) {\n                        let lower_ext = extension.to_lowercase();\n                        if characteristics.insert(lower_ext.clone()) {\n                            log::trace!(\"Detected characteristic (extension): {}\", lower_ext);\n                        }\n                        // Special handling if needed, e.g., .rake extension\n                        if extension == \"rake\" {\n                            if characteristics.insert(\"rake\".to_string()) {\n                                log::trace!(\n                                    \"Detected characteristic (extension): .rake maps to 'rake'\"\n                                );\n                            }\n                        }\n                    }\n                }\n            }\n            Err(e) => {\n                log::warn!(\n                    \"Error accessing path during characteristic detection: {} (at {})\",\n                    e,\n                    e.path()\n                        .map_or_else(|| \"unknown path\".into(), |p| p.display().to_string())\n                );\n                // Decide whether to continue or return error\n                // For characteristics detection, usually best to continue\n            }\n        }\n    }\n    log::debug!(\"Detected characteristics: {:?}\", characteristics);\n    Ok(characteristics)\n}\n\n// Removed the inline `pub mod mapping { ... }` block that started around line 84\n// The `pub mod mapping;` declaration at the top is sufficient.\n"
    },
    {
      "path": "core/system.rs",
      "content": "use crate::error::Result; // Removed AppError from here\n#[cfg(feature = \"serde_support\")]\nuse serde::{Deserialize, Serialize};\nuse std::env;\nuse sysinfo::System;\n\n#[derive(Debug, Clone, PartialEq, Default)]\n#[cfg_attr(feature = \"serde_support\", derive(Serialize, Deserialize))]\n#[cfg_attr(feature = \"serde_support\", serde(rename_all = \"camelCase\"))]\npub struct SystemInfo {\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    os_name: Option<String>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    os_version: Option<String>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    kernel_version: Option<String>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    hostname: Option<String>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    shell: Option<String>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    term: Option<String>,\n    #[cfg_attr(\n        feature = \"serde_support\",\n        serde(skip_serializing_if = \"Option::is_none\")\n    )]\n    error: Option<String>, // Keep for potential errors during collection\n}\n\npub fn gather_system_info() -> Result<SystemInfo> {\n    // Keep Result for consistency\n    let mut info = SystemInfo::default();\n    let mut sys = System::new_all();\n\n    // Removed catch_unwind\n    sys.refresh_all(); // Call directly\n\n    info.shell = env::var(\"SHELL\").ok();\n    info.term = env::var(\"TERM\").ok();\n\n    info.os_name = System::name();\n    info.os_version = System::os_version();\n    info.kernel_version = System::kernel_version();\n    info.hostname = System::host_name();\n\n    // Check if essential info is missing, potentially indicating an issue\n    if info.os_name.is_none() && info.hostname.is_none() {\n        info.error = Some(\"Failed to retrieve OS name and hostname.\".to_string());\n        // Return info with error message\n        return Ok(info);\n    }\n\n    Ok(info)\n}\n"
    },
    {
      "path": "data/ai_readme.yaml",
      "content": "intro: \"AI Readme: This JSON object provides context about a software project. Use this information to understand the project's structure, code, documentation, and guidelines.\"\nkey_sections_header: \"\\nKey Sections Explained:\"\nproject_name_desc: \"- 'project_name': Identifies the project.\"\nproject_root_desc: \"- 'project_root': The base directory path.\"\nsystem_info_desc: \"- 'system_info': Details about the environment where this context was generated.\"\nmeta_desc: \"- 'meta': Contains user-defined key-value pairs relevant to the project.\"\ndocs_desc: \"- 'docs': An array of documentation files. Each object has 'path' (relative to project root) and 'content'.\"\ntree_desc: \"- 'tree': Represents the project's directory structure hierarchically. Each node has 'name', 'type' ('file' or 'directory'), and optional 'children'.\"\nsource_files_desc: \"- 'source.files': An array of source code files. Each object has 'path' (relative) and 'content'. **Analyze this code carefully.**\"\nsource_chunks_desc: \"- 'source.chunks': An array of relative paths to chunk files (JSON). Load these files to get the complete source code context. The main context was too large.\"\nsource_missing_desc: \"- 'source': (Not included or empty) No source files matched the configuration filters.\"\nrules_desc: \"- 'rules': Contains directives (e.g., coding standards, user instructions) to be followed. Keys are rule set names (e.g., 'instructions', 'project_rules'). Values are arrays of rule strings. **Follow these directives strictly.**\"\nrules_missing_desc: \"- 'rules': (Not included or empty) No rules were defined or enabled.\"\ntimestamp_desc: \"- 'generation_timestamp': Indicates when this context was created (UTC).\"\n"
    },
    {
      "path": "data/builtin_ignores.yaml",
      "content": "common:\n  - \".gem/\"\n  - \".zed/\"\n  - \".idea/\"\n  - \".vscode/\"\n  - \".xtools/\"\n  - \"target/\"\n  - \".cache/\"\n  - \"node_modules/\"\n  - \"vendor/\"\n  - \"build/\"\n  - \"dist/\"\n  - \"__pycache__/\"\n  - \".direnv/\"\n  - \"*.pyc\"\n  - \".DS_Store\"\n  - \".envrc\"\n  - \"*.lock\"\n\ndocs: []\n\ntree:\n  - \".git/\"\n\nsource:\n  - \".rubocop.yml\"\n  - \".dir-locals.el\"\n  - \".projectile\"\n  - \"LICENSE\"\n"
    },
    {
      "path": "data/prompts.yaml",
      "content": "refactor: |\n  Refactor code to improve structure, clarity, and maintainability, adhering to language best practices and project rules. Optimize performance only if specified. Output code without comments, using professional documentation (rustdoc, rdoc, etc.) for public APIs/modules.\n\nexplain_code: |\n  Explain the purpose and functionality of the specified file (or main entry point if unspecified) in AI.org, focusing on high-level concepts and clarity. Adhere to project rules.\n\ngenerate_docs_for_function: |\n  Generate a Markdown description in AI.org for the specified function, detailing its purpose, parameters, return value, and side effects. Use professional documentation syntax (e.g., `///` for rustdoc) without adding comments to code. Adhere to project rules.\n\ngenerate_readme_section: |\n  Draft an Org-mode section for README.org on the requested feature (e.g., Installation, Usage, Configuration), explaining why the project exists, the problem it solves, or setup instructions. Keep it clear and concise, adhering to project rules.\n"
    },
    {
      "path": "data/rules/c.org",
      "content": "   - Use doxygen (`/** */`) for public APIs/modules.\n    - Pair `malloc`/`calloc`/`realloc` with `free`; check `NULL`.\n    - Use `#ifndef`/`#define`/`#endif` or `#pragma once`.\n    - Prefer standard library (`<stdio.h>`, `<stdlib.h>`, `<string.h>`).\n    - Use `strncpy`, `snprintf`, `memcpy_s` to prevent buffer overflows.\n    - Use forward declarations in headers.\n    - Use `const` for immutable data.\n    - Keep functions small.\n    - Follow K&R or Allman style.\n    - Check return values for I/O, memory, system calls.\n"
    },
    {
      "path": "data/rules/config_file.org",
      "content": "  - For JSON, YAML, TOML, and other config files, remove all comments unless explicitly specified.\n  - Follow project-specified formatting (e.g., 2-space indentation).\n  - Use meaningful keys and values, avoiding magic strings.\n"
    },
    {
      "path": "data/rules/cpp.org",
      "content": "    - Use doxygen (`/** */`) for public APIs/modules.\n    - Use RAII for resource management.\n    - Prefer smart pointers (`std::unique_ptr`, `std::shared_ptr`).\n    - Use `const` correctness; mark methods `const`.\n    - Use standard library (`<vector>`, `<string>`, `<algorithm>`, `<optional>`).\n    - Use C++11/14/17/20/23 features (`auto`, lambdas, concepts).\n    - Use `#pragma once` or header guards.\n    - Minimize headers with forward declarations.\n    - Follow Google C++ Style Guide or LLVM.\n    - Use exceptions for exceptional cases, error codes for expected issues.\n    - Use OOP (classes, namespaces).\n"
    },
    {
      "path": "data/rules/documentation.org",
      "content": "  - README.org: Why project exists, problem solved, setup instructions.\n  - SPEC.org: Technical specs, architecture.\n  - CLI.org: Command-line interface docs.\n  - GUI.org: Graphical user interface docs.\n  - AI.org: Changelogs, AI notes, complex decisions (e.g., architecture trade-offs).\n  - Use Mermaid inline code in Org files when needed (e.g., for diagrams).\n"
    },
    {
      "path": "data/rules/general.org",
      "content": "  - Write clear, concise, maintainable code.\n  - Avoid comments unless explaining why (e.g., external constraints) or weird behaviors (e.g., API workarounds). Always remove AI meta-comments (e.g., updated, removed, created).\n  - Use professional documentation (rustdoc, rdoc, doxygen, jsdoc, godoc, phpdoc) only for public APIs/modules.\n  - Use meaningful names for variables and functions.\n  - Avoid magic numbers/strings; use constants or configuration.\n  - Keep functions short, focused on one task (SRP).\n  - Follow project style guides and formatters (e.g., rustfmt, RuboCop, gofmt).\n  - Avoid tests unless required for public APIs exposed to external clients.\n  - Keep Org files (README.org, SPEC.org, CLI.org, GUI.org, AI.org) up-to-date with changes, especially after code generation.\n  - Follow KISS (keep simple), SRP (single responsibility), DRY (no duplication), YAGNI (no unneeded features), SoC (separate concerns), SOLID (OOP principles).\n  - Favor OOP and module systems.\n  - Add functionality to existing files unless a new logical component is needed.\n  - Prioritize correctness, clarity; optimize speed only if specified.\n  - Follow language error handling (e.g., Rust `Result`, Ruby bang methods).\n  - Use language-specific documentation syntax (e.g., `///` for rustdoc, `=begin`/`=end` for rdoc).\n  - Never suggest tests unless requested.\n"
    },
    {
      "path": "data/rules/go.org",
      "content": "    - Use godoc (`//`) for public APIs/modules.\n    - Follow `gofmt` or `goimports`.\n    - Check error returns; use `errors.Is`, `errors.As`.\n    - Use small interfaces for decoupling.\n    - Keep packages focused, lowercase, no cyclic dependencies.\n    - Use goroutines/channels for concurrency; use `sync.Mutex`, `sync.WaitGroup` if needed.\n    - Write simple, readable Go.\n    - Use short names locally, longer globally.\n"
    },
    {
      "path": "data/rules/guidelines.org",
      "content": " - Delete commented-out code (ai added comments and all explanation comments).\n  - During code outputting/generating max file limit range is 3-7 files (depending on file length) and once reached wait for me to type `CF` confirm to continue generating remaining files.\n  - Aim for files under 700 lines; split if handling multiple responsibilities.\n  - After generating each file; say `break!` and pause for 1-3 seconds and then continue.\n  - Don't generate partial files because i copy paste the code most of the times\n"
    },
    {
      "path": "data/rules/javascript.org",
      "content": "    - Use jsdoc (`/** */`) for public APIs/modules.\n    - Prefer vanilla JS; use Vite.js for builds.\n    - Follow common style guides (e.g., Airbnb, StandardJS).\n    - Use `const` for immutable, `let` for mutable variables.\n    - Avoid `var` unless needed for legacy code.\n    - Use arrow functions for callbacks.\n    - Prefer `async`/`await` over raw promises.\n"
    },
    {
      "path": "data/rules/php.org",
      "content": "    - Use phpdoc (`/** */`) for public APIs/modules.\n    - Follow PSR-12 standard.\n    - Use Composer for dependencies.\n    - Use OOP (classes, traits) for WordPress plugins.\n    - Use WP API for WordPress integrations.\n"
    },
    {
      "path": "data/rules/rakefile.org",
      "content": "  - Add `# frozen_string_literal: true` at top of all files.\n  - Avoid comments.\n  - Use short aliases for tasks.\n  - Provide descriptions for each task.\n  - Organize tasks into namespaces when needed.\n"
    },
    {
      "path": "data/rules/ruby.org",
      "content": "    - Use rdoc (`=begin`/`=end`) for public APIs/modules.\n    - Add `#!/usr/bin/env ruby` shebang at top of main files.\n    - Add `# frozen_string_literal: true` at top of all files.\n    - Use OOP, modules, mixins.\n    - Wrap execution in `def main; end; main` in main block.\n    - Use `each`, `map`, `select`, `reduce`, not loops.\n    - Use blocks, Procs, Lambdas where suitable.\n    - Follow Principle of Least Surprise.\n    - Use Bundler for gems (`Gemfile`, `Gemfile.lock`).\n    - Follow Ruby Style Guide (RuboCop).\n    - Use symbols for hash keys/options.\n    - Use `?` for predicates, `!` for mutations/errors.\n    - Prefer standard library; use gems as secondary option when necessary.\n"
    },
    {
      "path": "data/rules/rust.org",
      "content": "    - Use rustdoc (`///`) for public APIs/modules.\n    - Avoid `unwrap()`; use `?` for errors.\n    - Check bounds for indexing to avoid panics.\n    - Use flat module system with `src/some_module.rs`, avoiding `mod.rs`.\n    - Follow `rustfmt`, Rust API guidelines.\n    - Use traits for OOP/module behavior.\n    - Write idiomatic Rust (iterators, `Option`/`Result`, pattern matching).\n    - Prefer standard library; use crates as secondary option when necessary.\n    - Use `Result` for recoverable errors, `panic!` for unrecoverable.\n    - Keep functions small.\n    - Use project-specified YAML parser (e.g., serde_yml).\n"
    },
    {
      "path": "handling-large-codebases.org",
      "content": "#+TITLE: xcontext: Hybrid Client/Server Architecture for Large Codebase Context Management\n#+AUTHOR: json\n#+DATE: <2025-05-01 Thu>\n#+OPTIONS: toc:t num:t\n\n* Abstract\nInteracting with Large Language Models (LLMs) on large, real-world codebases presents a significant challenge due to the models' finite context windows. Standard codebases often exceed millions of tokens, making it impossible to provide complete context directly. The ~xcontext~ tool addresses this by implementing a decided *Hybrid Context Management* strategy, combining high-level code summaries (stored in SQLite) with precise, semantically relevant code snippets retrieved via Retrieval-Augmented Generation (RAG) using vector embeddings (stored in Qdrant). This report documents the chosen client/server architecture for ~xcontext~, outlining its components, specific technologies (Tree-sitter, Nomic/Google Embeddings, Qdrant, SQLite), data flows, and operational workflows, designed to provide scalable and comprehensive context to LLMs like Gemini.\n\n* The Challenge: LLM Context Limits vs. Large Codebases\nLLMs have token limits (e.g., 1-2M for Gemini Pro models) often insufficient for real-world codebases.\n\n*Example: Zed Editor Codebase*\n- Files: ~1,294 Rust files\n- Lines: ~675,366 total lines (~600k code lines)\n- Estimated Tokens: ~4-8 million+ tokens\n\nDirectly inputting such codebases is infeasible. ~xcontext~ employs a hybrid strategy.\n\n* The ~xcontext~ Solution: Hybrid Client/Server Architecture\n\nThe chosen strategy combines *LLM-generated Summaries* stored in SQLite (broad context) and *Retrieval-Augmented Generation (RAG)* using vector embeddings stored in Qdrant (specific context). This is implemented using a layered *Client/Server Architecture* within a Cargo Workspace.\n\n** Architectural Layers & Structure\n   - *Core Logic Library (`xcontext-core`):* Reusable Rust library containing the primary business logic, independent of server or UI concerns. Uses internal support modules/crates.\n   - *Server Process (`xcontext-server`):* Persistent background binary using `xcontext-core`. Manages state, DB connections, client communication, background tasks (`sync`, `mcp`). Built on an async runtime (e.g., Tokio).\n   - *Client Applications (`xcontext-cli`, `xcontext-gui`):* Lightweight frontends connecting to the `server` via a shared client library (`xcontext-client`) and protocol (e.g., gRPC).\n   - *Shared Client Library (`xcontext-client`):* Library crate containing shared logic for `cli` and `gui` to communicate with `server`.\n   - *Shared Utilities (`xcontext-common`):* Library crate for shared data structures (types, errors), constants, and basic utilities used across the workspace. Potentially includes shared algorithms and data structures unless complexity warrants dedicated crates (`xcontext-algorithms`, `xcontext-structures`).\n\n** High-Level Architecture Diagram (Overall Flow)\n   #+BEGIN_SRC mermaid\n     graph TD\n         subgraph \"User Interface Clients\"\n             CLI[\"xcontext-cli (Client Binary)\"]\n             GUI[\"(Future) xcontext-gui (Client Binary)\"]\n             ClientLib[\"xcontext-client (Shared Lib)\"]\n         end\n\n         subgraph \"Communication Protocol\"\n             direction LR\n             Proto[\"gRPC / JSON-RPC / etc.\"]\n         end\n\n         subgraph \"Server Process\"\n             Server[\"xcontext-server (Binary)\"]\n         end\n\n         subgraph \"Core Logic Library\"\n             Core[\"xcontext-core (Library)\"]\n         end\n\n         subgraph \"Background Tasks (Managed by Server)\"\n             Sync[\"sync watcher Logic\"]\n             MCP[\"mcp server Logic\"]\n         end\n\n         subgraph \"Data Stores\"\n             direction LR\n             VDB[(Qdrant Vector DB)]\n             KDB[(SQLite Summary KB)]\n         end\n\n         subgraph \"External Services\"\n             direction LR\n             EmbAPI{\"Embedding API (Nomic/Google)\"}\n             LLMAPI{\"LLM API (Gemini)\"}\n         end\n\n         subgraph \"Code Parsing\"\n            TS[\"tree-sitter\"]\n         end\n\n         CLI -- Uses --> ClientLib;\n         GUI -- Uses --> ClientLib;\n         ClientLib -- \"Request via Proto\" --> Server;\n         Server -- \"Response via Proto\" --> ClientLib;\n\n         Server -- Uses --> Core;\n         Server -- Manages --> Sync;\n         Server -- Manages --> MCP;\n\n         Core -- Contains/Uses --> Fetch(\"fetch logic\");\n         Core -- Contains/Uses --> Index(\"index logic\");\n         Core -- Contains/Uses --> Summaries(\"summaries logic\");\n         Core -- Contains/Uses --> Query(\"query logic\");\n         Core -- Contains/Uses --> LLM(\"llm logic\");\n         Core -- Contains/Uses --> Agent(\"agent logic\");\n         Core -- Contains/Uses --> Health(\"health logic\");\n         Core -- Uses --> Engine(\"engine module\");\n         Core -- Uses --> Auth(\"auth module\");\n         Core -- Uses --> Common(\"xcontext-common types/utils\");\n\n\n         Index -- Uses --> TS;\n         Index -- Uses --> EmbAPI;\n         Index -- \"Writes To\" --> VDB;\n         Summaries -- Uses --> LLMAPI;\n         Summaries -- \"Writes To\" --> KDB;\n         Query -- \"Reads From\" --> KDB;\n         Query -- \"Reads From\" --> VDB;\n         LLM -- Uses --> LLMAPI;\n         Agent -- Uses --> Engine;\n\n         Sync -- Triggers Updates In --> Index;\n         Sync -- Triggers Updates In --> Summaries;\n         MCP -- \"Handles Requests Using\" --> Query;\n\n         style Server fill:#f9f,stroke:#333,stroke-width:2px\n         style Core fill:#ccf,stroke:#333,stroke-width:2px\n\n   #+END_SRC\n\n* Core Components & Responsibilities\n\nThis section details the roles based on the decided names and structure, organized within a Cargo Workspace.\n\n** Workspace Crates & Component Diagram\n   #+BEGIN_SRC mermaid\n    graph TD\n        subgraph \"Workspace Binaries\"\n            CLI[\"xcontext-cli (bin)\"]\n            GUI[\"(Future) xcontext-gui (bin)\"]\n            Server[\"xcontext-server (bin)\"]\n        end\n\n        subgraph \"Core & Shared Libraries\"\n            Core[\"xcontext-core (lib)\"]\n            ClientLib[\"xcontext-client (lib)\"]\n            Common[\"xcontext-common (lib)\"]\n        end\n\n        subgraph \"Functional Logic Libs (used by Core/Server)\"\n            Fetch[\"fetch (module/lib)\"]\n            Index[\"index (module/lib)\"]\n            Summaries[\"summaries (module/lib)\"]\n            Query[\"query (module/lib)\"]\n            LLM[\"llm (module/lib)\"]\n            Agent[\"agent (module/lib)\"]\n            Health[\"health (module/lib)\"]\n            Sync[\"sync (module/lib)\"]\n            MCP[\"mcp (module/lib)\"]\n        end\n\n        subgraph \"Internal Support Libs (used by Core/Functional Libs)\"\n            Engine[\"engine (module/lib)\"]\n            Auth[\"auth (module/lib)\"]\n            Algo[\"(Future) xcontext-algorithms (lib)?\"]\n            DS[\"(Future) xcontext-structures (lib)?\"]\n        end\n\n        CLI --> ClientLib;\n        GUI --> ClientLib;\n        ClientLib --> Common;\n        ClientLib -- Talks via Protocol --> Server;\n\n        Server --> Core;\n        Server --> Sync;\n        Server --> MCP;\n        Server --> Common;\n\n        Core --> Fetch; Core --> Index; Core --> Summaries; Core --> Query;\n        Core --> LLM; Core --> Agent; Core --> Health;\n        Core --> Engine; Core --> Auth; Core --> Common;\n        Core --> Algo; Core --> DS;\n\n        Index --> Engine; Index --> Auth; Index --> Common;\n        Summaries --> Engine; Summaries --> Auth; Summaries --> LLM; Summaries --> Common;\n        Query --> Engine; Query --> Common;\n        Agent --> Engine; Agent --> Common;\n        Sync --> Index; Sync --> Summaries; Sync --> Common;\n        MCP --> Query; MCP --> Common;\n        Engine --> Common;\n        Auth --> Common;\n\n        Algo --> Common;\n        DS --> Common;\n\n\n        style CLI fill:#lightgreen,stroke:#333\n        style GUI fill:#lightgreen,stroke:#333\n        style Server fill:#f9f,stroke:#333,stroke-width:2px\n        style Core fill:#ccf,stroke:#333,stroke-width:2px\n        style ClientLib fill:#lightblue,stroke:#333\n        style Common fill:#yellow,stroke:#333\n\n   #+END_SRC\n\n** Component Descriptions\n   - *`xcontext-cli` (Binary):* Parses args, uses `xcontext-client` to talk to `server`, displays terminal results.\n   - *`xcontext-server` (Binary):* Persistent background server. Manages state, DB connections, runs `sync` & `mcp`, calls `core` logic via requests from `client`. Uses Tokio.\n   - *`xcontext-core` (Library):* Central logic library. Orchestrates operations, contains main business logic, uses internal/functional modules/crates.\n   - *`(Future) xcontext-gui` (Binary):* Graphical client using `xcontext-client`.\n   - *`xcontext-client` (Library):* Shared library for `cli` and `gui` client logic (communication with server).\n   - *`xcontext-common` (Library):* Holds shared types (structs, enums, errors), constants, basic utilities. *May initially contain shared algorithms/data structures.*\n   - *`Workspace` (Logic in `core`):* Gathers project files based on config/ignores. *(Note: This is the correct term for this function; \"Workspace\" in Cargo refers to the multi-crate project structure).*\n   - *`index` (Logic in `core`):* Builds/updates Qdrant vector index (uses `tree-sitter`, `auth`, `engine`).\n   - *`summaries` (Logic in `core`):* Builds/updates SQLite summary KB (uses `llm`, `auth`, `engine`).\n   - *`query` (Logic in `core`):* Retrieves hybrid context from SQLite+Qdrant (uses `engine`, `auth`). Includes context prioritization logic.\n   - *`llm` (Logic in `core`):* Interacts with primary LLM API (Gemini). Uses `auth`.\n   - *`agent` (Logic in `core`):* Executes actions based on `llm` output. Uses `engine`.\n   - *`health` (Logic in `core`):* Performs diagnostic checks.\n   - *`sync` (Logic used by `server`):* Background file watcher triggering `index`/`summaries` updates.\n   - *`mcp` (Logic used by `server`):* Implements MCP protocol server interface.\n   - *`engine` (Internal Module/Lib used by `core`):* Optimized I/O and DB access (Qdrant, SQLite).\n   - *`auth` (Internal Module/Lib used by `core`):* Secure credential handling (OS keyring/env vars).\n   - *`(Future) xcontext-algorithms`, `xcontext-structures` (Libraries):* Potential future crates if shared algorithms or data structures become complex enough to warrant separation from `xcontext-common`. Start by placing shared items in `common`.\n\n* Key Technologies (Chosen Stack)\n- *Code Parsing:* Tree-sitter (via `rust-tree-sitter`).\n- *Vector Embeddings:*\n  - *Local/Offline:* Ollama running *`nomic-embed-code`*.\n  - *Cloud/Online:* Google Vertex AI Text Embeddings API (*`text-embedding-004`*).\n  - *(Note: An index should be built & queried with a single, consistent embedding model).*\n- *Vector Database:* Qdrant (local).\n- *Summary Storage:* SQLite (local file).\n- *LLM APIs:* Google Gemini API.\n- *Async Runtime:* Tokio (for `xcontext-server`).\n- *Communication Protocol:* gRPC (via `tonic`) or similar.\n- *Project Structure:* Cargo Workspace.\n\n* Data Stores\n- *Vector Index (Qdrant):* Stores vectors + chunk metadata.\n- *Summary Knowledge Base (SQLite):* Stores summaries + associated metadata.\n\n* Workflows by Project Size\n\n(Workflows remain the same as described previously, differentiating small vs. large/giant projects and manual vs. agentic paths, using the defined components).\n\n** Parallel Indexing & Summarization Setup Diagram\n   #+BEGIN_SRC mermaid\n     graph TD\n         A[Codebase] --> B(xcontext-fetch);\n         B --> C(tree-sitter Chunking);\n         C --> D{Embedding Model};\n         D --> E(Qdrant Store);\n\n         B --> F(Batch Crates);\n         F --> G{LLM Summarize};\n         G --> H(SQLite Store);\n\n         subgraph RAG Indexing\n             C; D; E;\n         end\n\n         subgraph Summarization\n             F; G; H;\n         end\n\n         style RAG Indexing fill:#e6f7ff,stroke:#0066cc\n         style Summarization fill:#e6ffe6,stroke:#006600\n   #+END_SRC\n\n* Key Mechanism: Context Prioritization\nHandled within *`xcontext-core`*'s query/assembly logic before passing context to the `llm` component. Uses RAG scores, query analysis, etc., to select the best context subset within LLM token limits.\n\n* Implementation Considerations Summary\n(Async Server, Communication Protocol, Modularity/Traits in `core`, Config/Security via `auth`, Background Task Robustness, UX, Caching).\n\n* Conclusion\nThis documented Hybrid Client/Server architecture, utilizing the specified components and technologies (SQLite, Qdrant, Tree-sitter, Nomic/Google Embeddings, Gemini), provides a clear and robust roadmap for ~xcontext~. It addresses the challenge of large codebases by intelligently combining broad summaries and specific RAG results, managed within a maintainable and scalable Cargo workspace structure.\n"
    },
    {
      "path": "samples/xcontext_project.json",
      "content": ""
    }
  ],
  "tree": [
    {
      "name": ".config",
      "type": "directory",
      "children": []
    },
    {
      "name": ".dir-locals.el",
      "type": "file"
    },
    {
      "name": ".gitignore",
      "type": "file"
    },
    {
      "name": ".org",
      "type": "directory",
      "children": [
        {
          "name": "CLI.org",
          "type": "file"
        },
        {
          "name": "SPEC.org",
          "type": "file"
        }
      ]
    },
    {
      "name": "Cargo.toml",
      "type": "file"
    },
    {
      "name": "Gemfile",
      "type": "file"
    },
    {
      "name": "LICENSE",
      "type": "file"
    },
    {
      "name": "README.org",
      "type": "file"
    },
    {
      "name": "Rakefile",
      "type": "file"
    },
    {
      "name": "VISION.org",
      "type": "file"
    },
    {
      "name": "cli",
      "type": "directory",
      "children": [
        {
          "name": "Cargo.toml",
          "type": "file"
        },
        {
          "name": "cli.rs",
          "type": "file"
        },
        {
          "name": "cli_args.rs",
          "type": "file"
        },
        {
          "name": "commands",
          "type": "directory",
          "children": [
            {
              "name": "completion.rs",
              "type": "file"
            },
            {
              "name": "config.rs",
              "type": "file"
            },
            {
              "name": "debug.rs",
              "type": "file"
            },
            {
              "name": "generate.rs",
              "type": "file"
            },
            {
              "name": "metrics.rs",
              "type": "file"
            },
            {
              "name": "quick.rs",
              "type": "file"
            },
            {
              "name": "show.rs",
              "type": "file"
            }
          ]
        },
        {
          "name": "commands.rs",
          "type": "file"
        },
        {
          "name": "output.rs",
          "type": "file"
        },
        {
          "name": "watch.rs",
          "type": "file"
        }
      ]
    },
    {
      "name": "core",
      "type": "directory",
      "children": [
        {
          "name": "Cargo.toml",
          "type": "file"
        },
        {
          "name": "build.rs",
          "type": "file"
        },
        {
          "name": "chunking.rs",
          "type": "file"
        },
        {
          "name": "config.rs",
          "type": "file"
        },
        {
          "name": "context.rs",
          "type": "file"
        },
        {
          "name": "core.rs",
          "type": "file"
        },
        {
          "name": "error.rs",
          "type": "file"
        },
        {
          "name": "gather.rs",
          "type": "file"
        },
        {
          "name": "output_formats.rs",
          "type": "file"
        },
        {
          "name": "rules",
          "type": "directory",
          "children": [
            {
              "name": "mapping.rs",
              "type": "file"
            }
          ]
        },
        {
          "name": "rules.rs",
          "type": "file"
        },
        {
          "name": "system.rs",
          "type": "file"
        }
      ]
    },
    {
      "name": "data",
      "type": "directory",
      "children": [
        {
          "name": "ai_readme.yaml",
          "type": "file"
        },
        {
          "name": "builtin_ignores.yaml",
          "type": "file"
        },
        {
          "name": "prompts.yaml",
          "type": "file"
        },
        {
          "name": "rules",
          "type": "directory",
          "children": [
            {
              "name": "c.org",
              "type": "file"
            },
            {
              "name": "config_file.org",
              "type": "file"
            },
            {
              "name": "cpp.org",
              "type": "file"
            },
            {
              "name": "documentation.org",
              "type": "file"
            },
            {
              "name": "general.org",
              "type": "file"
            },
            {
              "name": "go.org",
              "type": "file"
            },
            {
              "name": "guidelines.org",
              "type": "file"
            },
            {
              "name": "javascript.org",
              "type": "file"
            },
            {
              "name": "php.org",
              "type": "file"
            },
            {
              "name": "rakefile.org",
              "type": "file"
            },
            {
              "name": "ruby.org",
              "type": "file"
            },
            {
              "name": "rust.org",
              "type": "file"
            }
          ]
        }
      ]
    },
    {
      "name": "handling-large-codebases.org",
      "type": "file"
    },
    {
      "name": "samples",
      "type": "directory",
      "children": [
        {
          "name": "xcontext_project.json",
          "type": "file"
        }
      ]
    }
  ],
  "rules": {
    "static:guidelines": [
      "- Delete commented-out code (ai added comments and all explanation comments).",
      "- During code outputting/generating max file limit range is 3-7 files (depending on file length) and once reached wait for me to type `CF` confirm to continue generating remaining files.",
      "- Aim for files under 700 lines; split if handling multiple responsibilities.",
      "- After generating each file; say `break!` and pause for 1-3 seconds and then continue.",
      "- Don't generate partial files because i copy paste the code most of the times"
    ],
    "static:rust": [
      "- Use rustdoc (`///`) for public APIs/modules.",
      "- Avoid `unwrap()`; use `?` for errors.",
      "- Check bounds for indexing to avoid panics.",
      "- Use flat module system with `src/some_module.rs`, avoiding `mod.rs`.",
      "- Follow `rustfmt`, Rust API guidelines.",
      "- Use traits for OOP/module behavior.",
      "- Write idiomatic Rust (iterators, `Option`/`Result`, pattern matching).",
      "- Prefer standard library; use crates as secondary option when necessary.",
      "- Use `Result` for recoverable errors, `panic!` for unrecoverable.",
      "- Keep functions small.",
      "- Use project-specified YAML parser (e.g., serde_yml)."
    ],
    "static:general": [
      "- Write clear, concise, maintainable code.",
      "- Avoid comments unless explaining why (e.g., external constraints) or weird behaviors (e.g., API workarounds). Always remove AI meta-comments (e.g., updated, removed, created).",
      "- Use professional documentation (rustdoc, rdoc, doxygen, jsdoc, godoc, phpdoc) only for public APIs/modules.",
      "- Use meaningful names for variables and functions.",
      "- Avoid magic numbers/strings; use constants or configuration.",
      "- Keep functions short, focused on one task (SRP).",
      "- Follow project style guides and formatters (e.g., rustfmt, RuboCop, gofmt).",
      "- Avoid tests unless required for public APIs exposed to external clients.",
      "- Keep Org files (README.org, SPEC.org, CLI.org, GUI.org, AI.org) up-to-date with changes, especially after code generation.",
      "- Follow KISS (keep simple), SRP (single responsibility), DRY (no duplication), YAGNI (no unneeded features), SoC (separate concerns), SOLID (OOP principles).",
      "- Favor OOP and module systems.",
      "- Add functionality to existing files unless a new logical component is needed.",
      "- Prioritize correctness, clarity; optimize speed only if specified.",
      "- Follow language error handling (e.g., Rust `Result`, Ruby bang methods).",
      "- Use language-specific documentation syntax (e.g., `///` for rustdoc, `=begin`/`=end` for rdoc).",
      "- Never suggest tests unless requested."
    ],
    "static:rakefile": [
      "- Add `# frozen_string_literal: true` at top of all files.",
      "- Avoid comments.",
      "- Use short aliases for tasks.",
      "- Provide descriptions for each task.",
      "- Organize tasks into namespaces when needed."
    ],
    "static:documentation": [
      "- README.org: Why project exists, problem solved, setup instructions.",
      "- SPEC.org: Technical specs, architecture.",
      "- CLI.org: Command-line interface docs.",
      "- GUI.org: Graphical user interface docs.",
      "- AI.org: Changelogs, AI notes, complex decisions (e.g., architecture trade-offs).",
      "- Use Mermaid inline code in Org files when needed (e.g., for diagrams)."
    ],
    "static:config_file": [
      "- For JSON, YAML, TOML, and other config files, remove all comments unless explicitly specified.",
      "- Follow project-specified formatting (e.g., 2-space indentation).",
      "- Use meaningful keys and values, avoiding magic strings."
    ],
    "static:ruby": [
      "- Use rdoc (`=begin`/`=end`) for public APIs/modules.",
      "- Add `#!/usr/bin/env ruby` shebang at top of main files.",
      "- Add `# frozen_string_literal: true` at top of all files.",
      "- Use OOP, modules, mixins.",
      "- Wrap execution in `def main; end; main` in main block.",
      "- Use `each`, `map`, `select`, `reduce`, not loops.",
      "- Use blocks, Procs, Lambdas where suitable.",
      "- Follow Principle of Least Surprise.",
      "- Use Bundler for gems (`Gemfile`, `Gemfile.lock`).",
      "- Follow Ruby Style Guide (RuboCop).",
      "- Use symbols for hash keys/options.",
      "- Use `?` for predicates, `!` for mutations/errors.",
      "- Prefer standard library; use gems as secondary option when necessary."
    ]
  },
  "prompts": {
    "static:generate_docs_for_function": "Generate a Markdown description in AI.org for the specified function, detailing its purpose, parameters, return value, and side effects. Use professional documentation syntax (e.g., `///` for rustdoc) without adding comments to code. Adhere to project rules.\n",
    "static:refactor": "Refactor code to improve structure, clarity, and maintainability, adhering to language best practices and project rules. Optimize performance only if specified. Output code without comments, using professional documentation (rustdoc, rdoc, etc.) for public APIs/modules.\n",
    "static:explain_code": "Explain the purpose and functionality of the specified file (or main entry point if unspecified) in AI.org, focusing on high-level concepts and clarity. Adhere to project rules.\n",
    "static:generate_readme_section": "Draft an Org-mode section for README.org on the requested feature (e.g., Installation, Usage, Configuration), explaining why the project exists, the problem it solves, or setup instructions. Keep it clear and concise, adhering to project rules.\n"
  },
  "generationTimestamp": "2025-05-16T21:14:52.705174440Z"
}\n